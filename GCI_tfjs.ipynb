{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCI_tfjs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usOBxGoNR0Yo",
        "colab_type": "text"
      },
      "source": [
        "# Google Code In: TensorFlow.js: Convert a model - Submission\n",
        "\n",
        "* Maanav Singh\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note:**\n",
        "\n",
        "This is a simple regression model that I created to predict Turkey Production Data in the U.S.; All of this is my work except the dataset which I found on Data.gov\n",
        "\n",
        "Skip down to the second to last code block to the javascript conversion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBxXNdvTpJrc",
        "colab_type": "code",
        "outputId": "73c5c37c-ba8c-42e0-98ab-01c6d294c1c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Created by Maanav Singh\n",
        "#CSV File can be found on pchsdatascience.com (I designed this website for a school club)\n",
        "\n",
        "#Data from Data.gov\n",
        "\n",
        "!pip install tensorflowjs\n",
        "try:\n",
        "  import tensorflowjs\n",
        "except:\n",
        "  print(\"tfjs already imported\")\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "import io\n",
        "import math\n",
        "import requests\n",
        "\n",
        "\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "\n",
        "#Uncomment if you want to manually load a csv\n",
        "'''\n",
        "print(\"Please Load Production CSV File\")\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv(io.BytesIO(uploaded[\"ProductionData.csv\"]))\n",
        "'''\n",
        "\n",
        "# Fetch Dataset from Our Website\n",
        "response = requests.get('https://pchsdatascience.com/wp-content/uploads/2019/12/ProductionData.csv')\n",
        "file_object = io.StringIO(response.content.decode('utf-8'))\n",
        "data = pd.read_csv(file_object)\n",
        "\n",
        "#Load CSV Data\n",
        "data.head()\n",
        "\n",
        "#Remove Header and Footer from file\n",
        "d1 = data.iloc[2:698]\n",
        "dates = d1.iloc[:,0]\n",
        "\n",
        "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
        "sess = tf.Session(config=config) \n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/73/f7ee6edced75b7dfe43916203f1b2e85dd14cba087a090e6372cbd82e462/tensorflowjs-1.4.0-py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow==1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (0.2.2)\n",
            "Collecting PyInquirer==1.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/4c/434b7c454010a284b49d6f1d446fe8dc5960415613d8c0225b9e2efb6724/PyInquirer-1.0.3.tar.gz\n",
            "Collecting tensorflow-hub==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/be/f18c352d84382d9c795a0f37eaf16d42ace7d161fbb0ad20bdcd5e550015/tensorflow_hub-0.5.0-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.4MB/s \n",
            "\u001b[?25hCollecting six==1.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py==2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.8.0)\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0->tensorflowjs) (1.11.2)\n",
            "Collecting prompt_toolkit==1.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/3d/b25d35a9f0d381dd1c02d8e04b37c353caaaff4bc32150328eeebe4931f5/prompt_toolkit-1.0.14-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 59.6MB/s \n",
            "\u001b[?25hCollecting Pygments>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/39/32da3184734730c0e4d3fa3b2b5872104668ad6dc1b5a73d8e477e5fe967/Pygments-2.5.2-py2.py3-none-any.whl (896kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 60.0MB/s \n",
            "\u001b[?25hCollecting regex>=2016.11.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/db/4b29a0adec5881542cd81cb5d1929b5c0787003c5740b3c921e627d9c2e5/regex-2019.12.9.tar.gz (669kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 56.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0->tensorflowjs) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->tensorflowjs) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt_toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.1.7)\n",
            "Building wheels for collected packages: PyInquirer, regex\n",
            "  Building wheel for PyInquirer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyInquirer: filename=PyInquirer-1.0.3-cp36-none-any.whl size=32853 sha256=412bf8e0b324bcdc07caf9821179241a9b9da276c6d3695fa84b944ffefc501b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6c/b1/3e4b0e8daf42a92883c7641c0ea8ffb62e0490ebed2faa55ad\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.12.9-cp36-cp36m-linux_x86_64.whl size=609166 sha256=e0bd32e753572fbda31dd8b210ec7898633e4735038a385516a5f160d7630dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/fb/b3/a89169557229468c49ca64f6839418f22461f6ee0a74f342b1\n",
            "Successfully built PyInquirer regex\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, prompt-toolkit, Pygments, regex, PyInquirer, numpy, tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: numpy 1.17.4\n",
            "    Uninstalling numpy-1.17.4:\n",
            "      Successfully uninstalled numpy-1.17.4\n",
            "  Found existing installation: tensorflow-hub 0.7.0\n",
            "    Uninstalling tensorflow-hub-0.7.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.7.0\n",
            "Successfully installed PyInquirer-1.0.3 Pygments-2.5.2 numpy-1.16.4 prompt-toolkit-1.0.14 regex-2019.12.9 six-1.11.0 tensorflow-hub-0.5.0 tensorflowjs-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "prompt_toolkit",
                  "pygments",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShtuW5_P1e_O",
        "colab_type": "code",
        "outputId": "3a38ba74-531f-4a44-c39f-fa7b662709ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "#Create Pandas Dataframe\n",
        "turkey = d1.iloc[:,13]\n",
        "pd.to_numeric(turkey)\n",
        "dataset = pd.DataFrame({'Date': dates, 'Weight': turkey})\n",
        "dataset = dataset.iloc[::-1]\n",
        "dataset.index = np.arange(0, len(dataset))\n",
        "dataset.index.name = \"Month number\"\n",
        "\n",
        "#view dataframe\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Weight</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Month number</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jan-1960</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Feb-1960</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mar-1960</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Apr-1960</td>\n",
              "      <td>16.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>May-1960</td>\n",
              "      <td>27.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>Aug-2017</td>\n",
              "      <td>543.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>Sep-2017</td>\n",
              "      <td>467.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>693</th>\n",
              "      <td>Oct-2017</td>\n",
              "      <td>554.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>Nov-2017</td>\n",
              "      <td>517.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>Dec-2017</td>\n",
              "      <td>461.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>696 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Date Weight\n",
              "Month number                 \n",
              "0             Jan-1960   22.1\n",
              "1             Feb-1960   14.0\n",
              "2             Mar-1960   13.4\n",
              "3             Apr-1960   16.2\n",
              "4             May-1960   27.1\n",
              "...                ...    ...\n",
              "691           Aug-2017  543.1\n",
              "692           Sep-2017  467.9\n",
              "693           Oct-2017  554.2\n",
              "694           Nov-2017  517.2\n",
              "695           Dec-2017  461.1\n",
              "\n",
              "[696 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eEDylljpP1l",
        "colab_type": "code",
        "outputId": "94f3e819-8bb1-4bbd-e852-192ce3936ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "#Create Array stroring Monthly Data from data frame\n",
        "arrdata = dataset.to_numpy()\n",
        "arrMonth = []\n",
        "arrDate = []\n",
        "arrWeight = []\n",
        "\n",
        "for i in range(len(arrdata)):\n",
        "    arrMonth.append(i)\n",
        "    arrDate.append(arrdata[i][0])\n",
        "    arrWeight.append(float(arrdata[i][1]))\n",
        "\n",
        "#plot monthly production data\n",
        "plt.scatter(arrMonth, arrWeight)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFUVRehBV7oqiKD3I/w+D2Mqhxdy2wAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj4kfV1E2hVO",
        "colab_type": "code",
        "outputId": "c74a7f3d-7bde-49af-98e5-c9fb989b1d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "arrWeightyr = []\n",
        "arrYear = []\n",
        "\n",
        "#Sum Monthly Data to create yearly data\n",
        "for i in range(58):\n",
        "    sum = 0\n",
        "    for j in range(0,11):\n",
        "        sum += float(arrWeight[12*i + j])\n",
        "    arrWeightyr.append(sum)\n",
        "    arrYear.append(i)\n",
        "   \n",
        "plt.scatter(arrYear, arrWeightyr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ff49419dcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQhz0zcwqxEHfzKxCHPTNzCrk/wPhBUYRgcC5ogAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBU_ChhGpUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "#Split data in half for training and validation\n",
        "\n",
        "#Use monthly data to create training data (Give model as much data as possible)\n",
        "for i in arrMonth:\n",
        "    if i % 2 == 0:\n",
        "        x_train.append(i)\n",
        "        y_train.append(arrWeight[i])\n",
        "    else:\n",
        "        x_val.append(i)\n",
        "        y_val.append(arrWeight[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkYvgGSX-ylP",
        "colab_type": "text"
      },
      "source": [
        "Important Note on Model:\n",
        "\n",
        "Since we are dealing with fairly large numbers and our loss function is Mean Squared Error our gradient and loss is going to be massively large, to reduce the size of these numbers we will use a process called batch normalization. This will scale all the neuron values down with respect to each other's magnitudes.\n",
        "\n",
        "tf.keras.layers.BatchNormalization(momentum=0.99, epsilon=0.001)\n",
        "\n",
        "Since it is possible for a previous neuron to be zero, we need to add a small number to prevent division by zero this is done though \"epsilon\" which is essentially just a small number we set.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhIfg_mGt93j",
        "colab_type": "code",
        "outputId": "7b56bb71-22e2-4804-de8d-04e7140dea48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Define Sequential Machine Learning Model\n",
        "\n",
        "\n",
        "#In order to save time training - we have already trained the model for for you and will just download the model from our servers\n",
        "#Set this option to false if you want to retrain live (might take a while)\n",
        "LoadPretrainedModel = False\n",
        "\n",
        "if LoadPretrainedModel == True:\n",
        "  model_data = requests.get('https://pchsdatascience.com/wp-content/uploads/2019/12/ProductionData.csv')\n",
        "  model_file = io.StringIO(response.content.decode('utf-8'))\n",
        "  \n",
        "else:\n",
        "  model = tf.keras.models.Sequential(\n",
        "      [tf.keras.layers.Dense(10, input_shape=[1], activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(momentum=0.99, epsilon=0.001),\n",
        "      tf.keras.layers.Dense(90, activation='relu'),\n",
        "      tf.keras.layers.Dense(1)\n",
        "      ])\n",
        "\n",
        "  #sgd = tf.optimizers.SGD(lr=.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  model.compile(optimizer='adam', loss='mean_squared_error', metrics=[\"accuracy\"])\n",
        "\n",
        "  #adjust number of epochs to suit processing power and required training\n",
        "  model.fit(x_train, y_train, epochs=3000, validation_data=(x_val,y_val))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 348 samples, validate on 348 samples\n",
            "Epoch 1/3000\n",
            "348/348 [==============================] - 3s 9ms/sample - loss: 122359.1263 - acc: 0.0000e+00 - val_loss: 121370.6912 - val_acc: 0.0000e+00\n",
            "Epoch 2/3000\n",
            "348/348 [==============================] - 0s 255us/sample - loss: 122063.1819 - acc: 0.0000e+00 - val_loss: 120691.3369 - val_acc: 0.0000e+00\n",
            "Epoch 3/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 121774.5248 - acc: 0.0000e+00 - val_loss: 120182.5874 - val_acc: 0.0000e+00\n",
            "Epoch 4/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 121483.1835 - acc: 0.0000e+00 - val_loss: 119760.3119 - val_acc: 0.0000e+00\n",
            "Epoch 5/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 121171.5891 - acc: 0.0000e+00 - val_loss: 119310.3574 - val_acc: 0.0000e+00\n",
            "Epoch 6/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 120813.7369 - acc: 0.0000e+00 - val_loss: 118836.6268 - val_acc: 0.0000e+00\n",
            "Epoch 7/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 120393.6017 - acc: 0.0000e+00 - val_loss: 118309.9292 - val_acc: 0.0000e+00\n",
            "Epoch 8/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 119896.8294 - acc: 0.0000e+00 - val_loss: 117704.8297 - val_acc: 0.0000e+00\n",
            "Epoch 9/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 119311.3931 - acc: 0.0000e+00 - val_loss: 117002.7424 - val_acc: 0.0000e+00\n",
            "Epoch 10/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 118627.4892 - acc: 0.0000e+00 - val_loss: 116222.6698 - val_acc: 0.0000e+00\n",
            "Epoch 11/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 117832.4847 - acc: 0.0000e+00 - val_loss: 115371.8541 - val_acc: 0.0000e+00\n",
            "Epoch 12/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 116915.2101 - acc: 0.0000e+00 - val_loss: 114389.0406 - val_acc: 0.0000e+00\n",
            "Epoch 13/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 115865.0849 - acc: 0.0000e+00 - val_loss: 113343.3306 - val_acc: 0.0000e+00\n",
            "Epoch 14/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 114672.1160 - acc: 0.0000e+00 - val_loss: 112178.9025 - val_acc: 0.0000e+00\n",
            "Epoch 15/3000\n",
            "348/348 [==============================] - 0s 253us/sample - loss: 113327.5226 - acc: 0.0000e+00 - val_loss: 110874.1457 - val_acc: 0.0000e+00\n",
            "Epoch 16/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 111823.6967 - acc: 0.0000e+00 - val_loss: 109497.7888 - val_acc: 0.0000e+00\n",
            "Epoch 17/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 110154.5883 - acc: 0.0000e+00 - val_loss: 107992.5875 - val_acc: 0.0000e+00\n",
            "Epoch 18/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 108315.4423 - acc: 0.0000e+00 - val_loss: 106485.6272 - val_acc: 0.0000e+00\n",
            "Epoch 19/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 106303.3285 - acc: 0.0000e+00 - val_loss: 104922.7819 - val_acc: 0.0000e+00\n",
            "Epoch 20/3000\n",
            "348/348 [==============================] - 0s 270us/sample - loss: 104116.7557 - acc: 0.0000e+00 - val_loss: 103225.9675 - val_acc: 0.0000e+00\n",
            "Epoch 21/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 101755.7751 - acc: 0.0000e+00 - val_loss: 101653.4519 - val_acc: 0.0000e+00\n",
            "Epoch 22/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 99222.1474 - acc: 0.0000e+00 - val_loss: 100270.9557 - val_acc: 0.0000e+00\n",
            "Epoch 23/3000\n",
            "348/348 [==============================] - 0s 257us/sample - loss: 96519.4583 - acc: 0.0000e+00 - val_loss: 99198.0025 - val_acc: 0.0000e+00\n",
            "Epoch 24/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 93652.4952 - acc: 0.0000e+00 - val_loss: 98600.7026 - val_acc: 0.0000e+00\n",
            "Epoch 25/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 90627.3901 - acc: 0.0000e+00 - val_loss: 95450.0938 - val_acc: 0.0000e+00\n",
            "Epoch 26/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 87460.0840 - acc: 0.0000e+00 - val_loss: 92758.4353 - val_acc: 0.0000e+00\n",
            "Epoch 27/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 84154.3670 - acc: 0.0000e+00 - val_loss: 90529.6596 - val_acc: 0.0000e+00\n",
            "Epoch 28/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 80725.7669 - acc: 0.0000e+00 - val_loss: 88574.9491 - val_acc: 0.0000e+00\n",
            "Epoch 29/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 77188.7733 - acc: 0.0000e+00 - val_loss: 87452.7218 - val_acc: 0.0000e+00\n",
            "Epoch 30/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 73561.7750 - acc: 0.0000e+00 - val_loss: 83944.6604 - val_acc: 0.0000e+00\n",
            "Epoch 31/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 69858.5735 - acc: 0.0000e+00 - val_loss: 79981.0618 - val_acc: 0.0000e+00\n",
            "Epoch 32/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 66101.8299 - acc: 0.0000e+00 - val_loss: 76368.4503 - val_acc: 0.0000e+00\n",
            "Epoch 33/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 62306.7171 - acc: 0.0000e+00 - val_loss: 72736.2853 - val_acc: 0.0000e+00\n",
            "Epoch 34/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 58492.7847 - acc: 0.0000e+00 - val_loss: 69182.7312 - val_acc: 0.0000e+00\n",
            "Epoch 35/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 54675.7821 - acc: 0.0000e+00 - val_loss: 65572.8010 - val_acc: 0.0000e+00\n",
            "Epoch 36/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 50870.4740 - acc: 0.0000e+00 - val_loss: 62058.2378 - val_acc: 0.0000e+00\n",
            "Epoch 37/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 47076.4288 - acc: 0.0000e+00 - val_loss: 58368.4958 - val_acc: 0.0000e+00\n",
            "Epoch 38/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 43292.2002 - acc: 0.0000e+00 - val_loss: 54326.3820 - val_acc: 0.0000e+00\n",
            "Epoch 39/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 39551.0828 - acc: 0.0000e+00 - val_loss: 50215.7983 - val_acc: 0.0000e+00\n",
            "Epoch 40/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 35891.5243 - acc: 0.0000e+00 - val_loss: 46331.2898 - val_acc: 0.0000e+00\n",
            "Epoch 41/3000\n",
            "348/348 [==============================] - 0s 177us/sample - loss: 32354.1916 - acc: 0.0000e+00 - val_loss: 42586.6185 - val_acc: 0.0000e+00\n",
            "Epoch 42/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 28942.3414 - acc: 0.0000e+00 - val_loss: 38615.6542 - val_acc: 0.0000e+00\n",
            "Epoch 43/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 25664.5930 - acc: 0.0000e+00 - val_loss: 34757.9202 - val_acc: 0.0000e+00\n",
            "Epoch 44/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 22590.8045 - acc: 0.0000e+00 - val_loss: 31095.9597 - val_acc: 0.0000e+00\n",
            "Epoch 45/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 19775.6897 - acc: 0.0000e+00 - val_loss: 27820.4836 - val_acc: 0.0000e+00\n",
            "Epoch 46/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 17210.9068 - acc: 0.0000e+00 - val_loss: 24840.6798 - val_acc: 0.0000e+00\n",
            "Epoch 47/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 14902.9946 - acc: 0.0000e+00 - val_loss: 22002.4816 - val_acc: 0.0000e+00\n",
            "Epoch 48/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 12906.6807 - acc: 0.0000e+00 - val_loss: 19381.5661 - val_acc: 0.0000e+00\n",
            "Epoch 49/3000\n",
            "348/348 [==============================] - 0s 238us/sample - loss: 11251.6493 - acc: 0.0000e+00 - val_loss: 17037.8497 - val_acc: 0.0000e+00\n",
            "Epoch 50/3000\n",
            "348/348 [==============================] - 0s 266us/sample - loss: 9922.1688 - acc: 0.0000e+00 - val_loss: 14846.3067 - val_acc: 0.0000e+00\n",
            "Epoch 51/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 8885.3374 - acc: 0.0000e+00 - val_loss: 12821.7228 - val_acc: 0.0000e+00\n",
            "Epoch 52/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 8093.4068 - acc: 0.0000e+00 - val_loss: 11094.7843 - val_acc: 0.0000e+00\n",
            "Epoch 53/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 7499.9452 - acc: 0.0000e+00 - val_loss: 9657.4900 - val_acc: 0.0000e+00\n",
            "Epoch 54/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 7065.7127 - acc: 0.0000e+00 - val_loss: 8534.9687 - val_acc: 0.0000e+00\n",
            "Epoch 55/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 6755.5593 - acc: 0.0000e+00 - val_loss: 7700.8808 - val_acc: 0.0000e+00\n",
            "Epoch 56/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 6531.6685 - acc: 0.0000e+00 - val_loss: 7093.7105 - val_acc: 0.0000e+00\n",
            "Epoch 57/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 6368.3748 - acc: 0.0000e+00 - val_loss: 6665.3998 - val_acc: 0.0000e+00\n",
            "Epoch 58/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 6247.9077 - acc: 0.0000e+00 - val_loss: 6371.5245 - val_acc: 0.0000e+00\n",
            "Epoch 59/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 6154.5718 - acc: 0.0000e+00 - val_loss: 6167.7397 - val_acc: 0.0000e+00\n",
            "Epoch 60/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 6078.7285 - acc: 0.0000e+00 - val_loss: 6029.0506 - val_acc: 0.0000e+00\n",
            "Epoch 61/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 6014.1498 - acc: 0.0000e+00 - val_loss: 5926.8677 - val_acc: 0.0000e+00\n",
            "Epoch 62/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 5956.9915 - acc: 0.0000e+00 - val_loss: 5859.0037 - val_acc: 0.0000e+00\n",
            "Epoch 63/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 5906.7099 - acc: 0.0000e+00 - val_loss: 5810.0267 - val_acc: 0.0000e+00\n",
            "Epoch 64/3000\n",
            "348/348 [==============================] - 0s 250us/sample - loss: 5863.0012 - acc: 0.0000e+00 - val_loss: 5774.8786 - val_acc: 0.0000e+00\n",
            "Epoch 65/3000\n",
            "348/348 [==============================] - 0s 240us/sample - loss: 5824.9154 - acc: 0.0000e+00 - val_loss: 5746.8720 - val_acc: 0.0000e+00\n",
            "Epoch 66/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 5790.4365 - acc: 0.0000e+00 - val_loss: 5728.6390 - val_acc: 0.0000e+00\n",
            "Epoch 67/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 5758.6800 - acc: 0.0000e+00 - val_loss: 5715.4226 - val_acc: 0.0000e+00\n",
            "Epoch 68/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 5729.3892 - acc: 0.0000e+00 - val_loss: 5707.9871 - val_acc: 0.0000e+00\n",
            "Epoch 69/3000\n",
            "348/348 [==============================] - 0s 252us/sample - loss: 5701.6592 - acc: 0.0000e+00 - val_loss: 5707.7381 - val_acc: 0.0000e+00\n",
            "Epoch 70/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5675.7290 - acc: 0.0000e+00 - val_loss: 5719.8173 - val_acc: 0.0000e+00\n",
            "Epoch 71/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 5651.4236 - acc: 0.0000e+00 - val_loss: 5746.4100 - val_acc: 0.0000e+00\n",
            "Epoch 72/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 5628.7500 - acc: 0.0000e+00 - val_loss: 5795.2196 - val_acc: 0.0000e+00\n",
            "Epoch 73/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 5608.3509 - acc: 0.0000e+00 - val_loss: 5881.1018 - val_acc: 0.0000e+00\n",
            "Epoch 74/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5591.2217 - acc: 0.0000e+00 - val_loss: 6028.0468 - val_acc: 0.0000e+00\n",
            "Epoch 75/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 5577.2278 - acc: 0.0000e+00 - val_loss: 6358.0919 - val_acc: 0.0000e+00\n",
            "Epoch 76/3000\n",
            "348/348 [==============================] - 0s 241us/sample - loss: 5565.4926 - acc: 0.0000e+00 - val_loss: 7180.1800 - val_acc: 0.0000e+00\n",
            "Epoch 77/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 5555.3591 - acc: 0.0000e+00 - val_loss: 7356.0363 - val_acc: 0.0000e+00\n",
            "Epoch 78/3000\n",
            "348/348 [==============================] - 0s 253us/sample - loss: 5546.1741 - acc: 0.0000e+00 - val_loss: 7646.1710 - val_acc: 0.0000e+00\n",
            "Epoch 79/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 5537.7963 - acc: 0.0000e+00 - val_loss: 7744.2243 - val_acc: 0.0000e+00\n",
            "Epoch 80/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 5529.9844 - acc: 0.0000e+00 - val_loss: 7420.4806 - val_acc: 0.0000e+00\n",
            "Epoch 81/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 5522.6020 - acc: 0.0000e+00 - val_loss: 7268.7332 - val_acc: 0.0000e+00\n",
            "Epoch 82/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 5515.7349 - acc: 0.0000e+00 - val_loss: 7100.2582 - val_acc: 0.0000e+00\n",
            "Epoch 83/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 5509.2131 - acc: 0.0000e+00 - val_loss: 7019.0720 - val_acc: 0.0000e+00\n",
            "Epoch 84/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 5502.8563 - acc: 0.0000e+00 - val_loss: 6987.0177 - val_acc: 0.0000e+00\n",
            "Epoch 85/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 5496.7061 - acc: 0.0000e+00 - val_loss: 6987.6656 - val_acc: 0.0000e+00\n",
            "Epoch 86/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 5490.8611 - acc: 0.0000e+00 - val_loss: 6540.7519 - val_acc: 0.0000e+00\n",
            "Epoch 87/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 5484.8950 - acc: 0.0000e+00 - val_loss: 6571.6085 - val_acc: 0.0000e+00\n",
            "Epoch 88/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 5478.8796 - acc: 0.0000e+00 - val_loss: 6767.2930 - val_acc: 0.0000e+00\n",
            "Epoch 89/3000\n",
            "348/348 [==============================] - 0s 260us/sample - loss: 5473.1488 - acc: 0.0000e+00 - val_loss: 6522.7066 - val_acc: 0.0000e+00\n",
            "Epoch 90/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 5467.2699 - acc: 0.0000e+00 - val_loss: 6854.9127 - val_acc: 0.0000e+00\n",
            "Epoch 91/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 5462.4664 - acc: 0.0000e+00 - val_loss: 5936.7049 - val_acc: 0.0000e+00\n",
            "Epoch 92/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 5456.2827 - acc: 0.0000e+00 - val_loss: 5814.0804 - val_acc: 0.0000e+00\n",
            "Epoch 93/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 5450.6422 - acc: 0.0000e+00 - val_loss: 5839.7216 - val_acc: 0.0000e+00\n",
            "Epoch 94/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 5444.8997 - acc: 0.0000e+00 - val_loss: 5954.0265 - val_acc: 0.0000e+00\n",
            "Epoch 95/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 5439.0704 - acc: 0.0000e+00 - val_loss: 6159.4678 - val_acc: 0.0000e+00\n",
            "Epoch 96/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 5433.0673 - acc: 0.0000e+00 - val_loss: 6638.6253 - val_acc: 0.0000e+00\n",
            "Epoch 97/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 5428.0204 - acc: 0.0000e+00 - val_loss: 6085.8742 - val_acc: 0.0000e+00\n",
            "Epoch 98/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 5421.9762 - acc: 0.0000e+00 - val_loss: 6321.2594 - val_acc: 0.0000e+00\n",
            "Epoch 99/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 5416.3373 - acc: 0.0000e+00 - val_loss: 6556.8078 - val_acc: 0.0000e+00\n",
            "Epoch 100/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 5411.7863 - acc: 0.0000e+00 - val_loss: 5977.5007 - val_acc: 0.0000e+00\n",
            "Epoch 101/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 5405.7747 - acc: 0.0000e+00 - val_loss: 6070.1124 - val_acc: 0.0000e+00\n",
            "Epoch 102/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 5400.1567 - acc: 0.0000e+00 - val_loss: 6292.2788 - val_acc: 0.0000e+00\n",
            "Epoch 103/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 5394.9377 - acc: 0.0000e+00 - val_loss: 6446.3259 - val_acc: 0.0000e+00\n",
            "Epoch 104/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5391.0258 - acc: 0.0000e+00 - val_loss: 5539.2734 - val_acc: 0.0000e+00\n",
            "Epoch 105/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 5385.5071 - acc: 0.0000e+00 - val_loss: 5492.8487 - val_acc: 0.0000e+00\n",
            "Epoch 106/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 5380.3726 - acc: 0.0000e+00 - val_loss: 5598.4238 - val_acc: 0.0000e+00\n",
            "Epoch 107/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 5374.9798 - acc: 0.0000e+00 - val_loss: 5896.0051 - val_acc: 0.0000e+00\n",
            "Epoch 108/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 5369.5143 - acc: 0.0000e+00 - val_loss: 6294.5216 - val_acc: 0.0000e+00\n",
            "Epoch 109/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 5364.5096 - acc: 0.0000e+00 - val_loss: 6499.0387 - val_acc: 0.0000e+00\n",
            "Epoch 110/3000\n",
            "348/348 [==============================] - 0s 242us/sample - loss: 5361.0050 - acc: 0.0000e+00 - val_loss: 5585.7289 - val_acc: 0.0000e+00\n",
            "Epoch 111/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 5355.3186 - acc: 0.0000e+00 - val_loss: 5708.3171 - val_acc: 0.0000e+00\n",
            "Epoch 112/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 5350.2141 - acc: 0.0000e+00 - val_loss: 6037.9113 - val_acc: 0.0000e+00\n",
            "Epoch 113/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 5345.7453 - acc: 0.0000e+00 - val_loss: 6879.7501 - val_acc: 0.0000e+00\n",
            "Epoch 114/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 5344.9153 - acc: 0.0000e+00 - val_loss: 5253.1662 - val_acc: 0.0000e+00\n",
            "Epoch 115/3000\n",
            "348/348 [==============================] - 0s 258us/sample - loss: 5337.8390 - acc: 0.0000e+00 - val_loss: 5280.4982 - val_acc: 0.0000e+00\n",
            "Epoch 116/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 5333.4469 - acc: 0.0000e+00 - val_loss: 5262.0532 - val_acc: 0.0000e+00\n",
            "Epoch 117/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5328.9320 - acc: 0.0000e+00 - val_loss: 5254.7699 - val_acc: 0.0000e+00\n",
            "Epoch 118/3000\n",
            "348/348 [==============================] - 0s 298us/sample - loss: 5324.3951 - acc: 0.0000e+00 - val_loss: 5294.4667 - val_acc: 0.0000e+00\n",
            "Epoch 119/3000\n",
            "348/348 [==============================] - 0s 252us/sample - loss: 5319.7851 - acc: 0.0000e+00 - val_loss: 5388.2554 - val_acc: 0.0000e+00\n",
            "Epoch 120/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 5315.2958 - acc: 0.0000e+00 - val_loss: 5534.8352 - val_acc: 0.0000e+00\n",
            "Epoch 121/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 5310.5606 - acc: 0.0000e+00 - val_loss: 5754.3228 - val_acc: 0.0000e+00\n",
            "Epoch 122/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 5305.8033 - acc: 0.0000e+00 - val_loss: 6098.3671 - val_acc: 0.0000e+00\n",
            "Epoch 123/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5300.6176 - acc: 0.0000e+00 - val_loss: 6795.3226 - val_acc: 0.0000e+00\n",
            "Epoch 124/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 5295.1229 - acc: 0.0000e+00 - val_loss: 7731.6457 - val_acc: 0.0000e+00\n",
            "Epoch 125/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 5289.7218 - acc: 0.0000e+00 - val_loss: 9178.2808 - val_acc: 0.0000e+00\n",
            "Epoch 126/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 5286.8859 - acc: 0.0000e+00 - val_loss: 7870.8139 - val_acc: 0.0000e+00\n",
            "Epoch 127/3000\n",
            "348/348 [==============================] - 0s 274us/sample - loss: 5280.5076 - acc: 0.0000e+00 - val_loss: 8658.6060 - val_acc: 0.0000e+00\n",
            "Epoch 128/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 5275.1824 - acc: 0.0000e+00 - val_loss: 10589.8964 - val_acc: 0.0000e+00\n",
            "Epoch 129/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 5272.0510 - acc: 0.0000e+00 - val_loss: 9406.7468 - val_acc: 0.0000e+00\n",
            "Epoch 130/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 5266.3794 - acc: 0.0000e+00 - val_loss: 9806.4852 - val_acc: 0.0000e+00\n",
            "Epoch 131/3000\n",
            "348/348 [==============================] - 0s 249us/sample - loss: 5261.7638 - acc: 0.0000e+00 - val_loss: 10586.0154 - val_acc: 0.0000e+00\n",
            "Epoch 132/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 5259.1535 - acc: 0.0000e+00 - val_loss: 8882.8428 - val_acc: 0.0000e+00\n",
            "Epoch 133/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 5253.9167 - acc: 0.0000e+00 - val_loss: 8843.3863 - val_acc: 0.0000e+00\n",
            "Epoch 134/3000\n",
            "348/348 [==============================] - 0s 247us/sample - loss: 5249.9012 - acc: 0.0000e+00 - val_loss: 9003.0697 - val_acc: 0.0000e+00\n",
            "Epoch 135/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 5246.1932 - acc: 0.0000e+00 - val_loss: 8371.9900 - val_acc: 0.0000e+00\n",
            "Epoch 136/3000\n",
            "348/348 [==============================] - 0s 245us/sample - loss: 5241.8539 - acc: 0.0000e+00 - val_loss: 8640.0272 - val_acc: 0.0000e+00\n",
            "Epoch 137/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 5238.2906 - acc: 0.0000e+00 - val_loss: 8413.0627 - val_acc: 0.0000e+00\n",
            "Epoch 138/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 5234.6510 - acc: 0.0000e+00 - val_loss: 7579.0268 - val_acc: 0.0000e+00\n",
            "Epoch 139/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 5229.7207 - acc: 0.0000e+00 - val_loss: 7813.0913 - val_acc: 0.0000e+00\n",
            "Epoch 140/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 5226.2592 - acc: 0.0000e+00 - val_loss: 7472.1141 - val_acc: 0.0000e+00\n",
            "Epoch 141/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 5222.0549 - acc: 0.0000e+00 - val_loss: 7544.1287 - val_acc: 0.0000e+00\n",
            "Epoch 142/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 5218.6562 - acc: 0.0000e+00 - val_loss: 7320.7164 - val_acc: 0.0000e+00\n",
            "Epoch 143/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 5214.6084 - acc: 0.0000e+00 - val_loss: 7135.1992 - val_acc: 0.0000e+00\n",
            "Epoch 144/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 5210.9885 - acc: 0.0000e+00 - val_loss: 6618.6671 - val_acc: 0.0000e+00\n",
            "Epoch 145/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 5206.1820 - acc: 0.0000e+00 - val_loss: 6966.9275 - val_acc: 0.0000e+00\n",
            "Epoch 146/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 5202.9059 - acc: 0.0000e+00 - val_loss: 7083.3840 - val_acc: 0.0000e+00\n",
            "Epoch 147/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 5199.0582 - acc: 0.0000e+00 - val_loss: 6851.4983 - val_acc: 0.0000e+00\n",
            "Epoch 148/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 5195.8036 - acc: 0.0000e+00 - val_loss: 6275.8541 - val_acc: 0.0000e+00\n",
            "Epoch 149/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 5190.6134 - acc: 0.0000e+00 - val_loss: 6952.7027 - val_acc: 0.0000e+00\n",
            "Epoch 150/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 5188.3727 - acc: 0.0000e+00 - val_loss: 7233.7251 - val_acc: 0.0000e+00\n",
            "Epoch 151/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 5188.0788 - acc: 0.0000e+00 - val_loss: 5272.6988 - val_acc: 0.0000e+00\n",
            "Epoch 152/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 5182.1152 - acc: 0.0000e+00 - val_loss: 5339.1927 - val_acc: 0.0000e+00\n",
            "Epoch 153/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 5179.4511 - acc: 0.0000e+00 - val_loss: 5159.1002 - val_acc: 0.0000e+00\n",
            "Epoch 154/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 5172.4638 - acc: 0.0000e+00 - val_loss: 5202.6730 - val_acc: 0.0000e+00\n",
            "Epoch 155/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 5165.1738 - acc: 0.0000e+00 - val_loss: 6015.5093 - val_acc: 0.0000e+00\n",
            "Epoch 156/3000\n",
            "348/348 [==============================] - 0s 252us/sample - loss: 5158.3499 - acc: 0.0000e+00 - val_loss: 6955.6055 - val_acc: 0.0000e+00\n",
            "Epoch 157/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5157.6331 - acc: 0.0000e+00 - val_loss: 5710.5002 - val_acc: 0.0000e+00\n",
            "Epoch 158/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 5151.4148 - acc: 0.0000e+00 - val_loss: 5843.7029 - val_acc: 0.0000e+00\n",
            "Epoch 159/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 5144.3509 - acc: 0.0000e+00 - val_loss: 6522.4681 - val_acc: 0.0000e+00\n",
            "Epoch 160/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 5138.9852 - acc: 0.0000e+00 - val_loss: 6776.2337 - val_acc: 0.0000e+00\n",
            "Epoch 161/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 5135.3381 - acc: 0.0000e+00 - val_loss: 6227.3413 - val_acc: 0.0000e+00\n",
            "Epoch 162/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 5130.9315 - acc: 0.0000e+00 - val_loss: 6021.9692 - val_acc: 0.0000e+00\n",
            "Epoch 163/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 5126.3782 - acc: 0.0000e+00 - val_loss: 6406.1436 - val_acc: 0.0000e+00\n",
            "Epoch 164/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 5127.8676 - acc: 0.0000e+00 - val_loss: 5343.1564 - val_acc: 0.0000e+00\n",
            "Epoch 165/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 5126.1350 - acc: 0.0000e+00 - val_loss: 5329.4699 - val_acc: 0.0000e+00\n",
            "Epoch 166/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 5122.5478 - acc: 0.0000e+00 - val_loss: 5175.7144 - val_acc: 0.0000e+00\n",
            "Epoch 167/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 5120.7417 - acc: 0.0000e+00 - val_loss: 5871.8394 - val_acc: 0.0000e+00\n",
            "Epoch 168/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 5118.2800 - acc: 0.0000e+00 - val_loss: 6545.7499 - val_acc: 0.0000e+00\n",
            "Epoch 169/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 5114.1243 - acc: 0.0000e+00 - val_loss: 5643.7895 - val_acc: 0.0000e+00\n",
            "Epoch 170/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 5110.7349 - acc: 0.0000e+00 - val_loss: 5461.1328 - val_acc: 0.0000e+00\n",
            "Epoch 171/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 5132.7490 - acc: 0.0000e+00 - val_loss: 5496.5494 - val_acc: 0.0000e+00\n",
            "Epoch 172/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 5533.6022 - acc: 0.0000e+00 - val_loss: 10177.1681 - val_acc: 0.0000e+00\n",
            "Epoch 173/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 6589.5343 - acc: 0.0000e+00 - val_loss: 10755.3929 - val_acc: 0.0000e+00\n",
            "Epoch 174/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 6195.9778 - acc: 0.0000e+00 - val_loss: 12411.1805 - val_acc: 0.0000e+00\n",
            "Epoch 175/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 5843.9838 - acc: 0.0000e+00 - val_loss: 8382.6901 - val_acc: 0.0000e+00\n",
            "Epoch 176/3000\n",
            "348/348 [==============================] - 0s 240us/sample - loss: 5585.3607 - acc: 0.0000e+00 - val_loss: 5597.9418 - val_acc: 0.0000e+00\n",
            "Epoch 177/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 5410.8065 - acc: 0.0000e+00 - val_loss: 5866.1169 - val_acc: 0.0000e+00\n",
            "Epoch 178/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 5295.8458 - acc: 0.0000e+00 - val_loss: 5512.3063 - val_acc: 0.0000e+00\n",
            "Epoch 179/3000\n",
            "348/348 [==============================] - 0s 254us/sample - loss: 5223.8221 - acc: 0.0000e+00 - val_loss: 5284.8877 - val_acc: 0.0000e+00\n",
            "Epoch 180/3000\n",
            "348/348 [==============================] - 0s 258us/sample - loss: 5179.6528 - acc: 0.0000e+00 - val_loss: 5187.2418 - val_acc: 0.0000e+00\n",
            "Epoch 181/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 5152.9602 - acc: 0.0000e+00 - val_loss: 5160.1346 - val_acc: 0.0000e+00\n",
            "Epoch 182/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 5136.4923 - acc: 0.0000e+00 - val_loss: 5160.9373 - val_acc: 0.0000e+00\n",
            "Epoch 183/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 5125.5776 - acc: 0.0000e+00 - val_loss: 5177.2918 - val_acc: 0.0000e+00\n",
            "Epoch 184/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 5117.7452 - acc: 0.0000e+00 - val_loss: 5198.3732 - val_acc: 0.0000e+00\n",
            "Epoch 185/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 5111.5464 - acc: 0.0000e+00 - val_loss: 5214.6146 - val_acc: 0.0000e+00\n",
            "Epoch 186/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 5106.3061 - acc: 0.0000e+00 - val_loss: 5231.2911 - val_acc: 0.0000e+00\n",
            "Epoch 187/3000\n",
            "348/348 [==============================] - 0s 187us/sample - loss: 5101.6143 - acc: 0.0000e+00 - val_loss: 5239.9649 - val_acc: 0.0000e+00\n",
            "Epoch 188/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 5097.2613 - acc: 0.0000e+00 - val_loss: 5242.1692 - val_acc: 0.0000e+00\n",
            "Epoch 189/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 5093.0043 - acc: 0.0000e+00 - val_loss: 5238.8735 - val_acc: 0.0000e+00\n",
            "Epoch 190/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 5088.9139 - acc: 0.0000e+00 - val_loss: 5239.2046 - val_acc: 0.0000e+00\n",
            "Epoch 191/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 5084.8965 - acc: 0.0000e+00 - val_loss: 5236.8209 - val_acc: 0.0000e+00\n",
            "Epoch 192/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 5080.9412 - acc: 0.0000e+00 - val_loss: 5233.7444 - val_acc: 0.0000e+00\n",
            "Epoch 193/3000\n",
            "348/348 [==============================] - 0s 256us/sample - loss: 5077.0569 - acc: 0.0000e+00 - val_loss: 5223.4202 - val_acc: 0.0000e+00\n",
            "Epoch 194/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 5073.2260 - acc: 0.0000e+00 - val_loss: 5218.1406 - val_acc: 0.0000e+00\n",
            "Epoch 195/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 5069.3932 - acc: 0.0000e+00 - val_loss: 5210.7480 - val_acc: 0.0000e+00\n",
            "Epoch 196/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 5065.5295 - acc: 0.0000e+00 - val_loss: 5205.7696 - val_acc: 0.0000e+00\n",
            "Epoch 197/3000\n",
            "348/348 [==============================] - 0s 262us/sample - loss: 5061.6764 - acc: 0.0000e+00 - val_loss: 5187.1557 - val_acc: 0.0000e+00\n",
            "Epoch 198/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 5057.8416 - acc: 0.0000e+00 - val_loss: 5172.4153 - val_acc: 0.0000e+00\n",
            "Epoch 199/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 5054.0088 - acc: 0.0000e+00 - val_loss: 5168.0912 - val_acc: 0.0000e+00\n",
            "Epoch 200/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 5050.1506 - acc: 0.0000e+00 - val_loss: 5149.2208 - val_acc: 0.0000e+00\n",
            "Epoch 201/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 5046.3127 - acc: 0.0000e+00 - val_loss: 5141.4398 - val_acc: 0.0000e+00\n",
            "Epoch 202/3000\n",
            "348/348 [==============================] - 0s 260us/sample - loss: 5042.4809 - acc: 0.0000e+00 - val_loss: 5131.7626 - val_acc: 0.0000e+00\n",
            "Epoch 203/3000\n",
            "348/348 [==============================] - 0s 251us/sample - loss: 5038.6671 - acc: 0.0000e+00 - val_loss: 5123.5434 - val_acc: 0.0000e+00\n",
            "Epoch 204/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 5034.8162 - acc: 0.0000e+00 - val_loss: 5117.5203 - val_acc: 0.0000e+00\n",
            "Epoch 205/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 5030.9880 - acc: 0.0000e+00 - val_loss: 5103.3647 - val_acc: 0.0000e+00\n",
            "Epoch 206/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 5027.1367 - acc: 0.0000e+00 - val_loss: 5091.9141 - val_acc: 0.0000e+00\n",
            "Epoch 207/3000\n",
            "348/348 [==============================] - 0s 187us/sample - loss: 5023.2674 - acc: 0.0000e+00 - val_loss: 5070.8764 - val_acc: 0.0000e+00\n",
            "Epoch 208/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 5019.3924 - acc: 0.0000e+00 - val_loss: 5047.7751 - val_acc: 0.0000e+00\n",
            "Epoch 209/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 5015.5183 - acc: 0.0000e+00 - val_loss: 5023.4810 - val_acc: 0.0000e+00\n",
            "Epoch 210/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 5011.7045 - acc: 0.0000e+00 - val_loss: 5009.8423 - val_acc: 0.0000e+00\n",
            "Epoch 211/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 5007.8704 - acc: 0.0000e+00 - val_loss: 4996.0480 - val_acc: 0.0000e+00\n",
            "Epoch 212/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 5004.0148 - acc: 0.0000e+00 - val_loss: 4989.1773 - val_acc: 0.0000e+00\n",
            "Epoch 213/3000\n",
            "348/348 [==============================] - 0s 247us/sample - loss: 5000.1983 - acc: 0.0000e+00 - val_loss: 4989.1395 - val_acc: 0.0000e+00\n",
            "Epoch 214/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 4996.3971 - acc: 0.0000e+00 - val_loss: 4982.8320 - val_acc: 0.0000e+00\n",
            "Epoch 215/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 4992.6438 - acc: 0.0000e+00 - val_loss: 4975.0051 - val_acc: 0.0000e+00\n",
            "Epoch 216/3000\n",
            "348/348 [==============================] - 0s 248us/sample - loss: 4988.8376 - acc: 0.0000e+00 - val_loss: 4960.8386 - val_acc: 0.0000e+00\n",
            "Epoch 217/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 4985.0457 - acc: 0.0000e+00 - val_loss: 4946.6369 - val_acc: 0.0000e+00\n",
            "Epoch 218/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 4981.2743 - acc: 0.0000e+00 - val_loss: 4936.1411 - val_acc: 0.0000e+00\n",
            "Epoch 219/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4977.5376 - acc: 0.0000e+00 - val_loss: 4932.1967 - val_acc: 0.0000e+00\n",
            "Epoch 220/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 4973.7826 - acc: 0.0000e+00 - val_loss: 4930.2790 - val_acc: 0.0000e+00\n",
            "Epoch 221/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 4970.0444 - acc: 0.0000e+00 - val_loss: 4927.4549 - val_acc: 0.0000e+00\n",
            "Epoch 222/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 4966.3042 - acc: 0.0000e+00 - val_loss: 4919.0268 - val_acc: 0.0000e+00\n",
            "Epoch 223/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 4962.5992 - acc: 0.0000e+00 - val_loss: 4909.3563 - val_acc: 0.0000e+00\n",
            "Epoch 224/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 4958.8983 - acc: 0.0000e+00 - val_loss: 4904.9604 - val_acc: 0.0000e+00\n",
            "Epoch 225/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4955.2078 - acc: 0.0000e+00 - val_loss: 4898.8596 - val_acc: 0.0000e+00\n",
            "Epoch 226/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 4951.5173 - acc: 0.0000e+00 - val_loss: 4893.9270 - val_acc: 0.0000e+00\n",
            "Epoch 227/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 4947.8475 - acc: 0.0000e+00 - val_loss: 4890.1562 - val_acc: 0.0000e+00\n",
            "Epoch 228/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4944.1868 - acc: 0.0000e+00 - val_loss: 4886.0345 - val_acc: 0.0000e+00\n",
            "Epoch 229/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4940.5193 - acc: 0.0000e+00 - val_loss: 4883.2680 - val_acc: 0.0000e+00\n",
            "Epoch 230/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4936.8463 - acc: 0.0000e+00 - val_loss: 4878.7191 - val_acc: 0.0000e+00\n",
            "Epoch 231/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 4933.1784 - acc: 0.0000e+00 - val_loss: 4872.5118 - val_acc: 0.0000e+00\n",
            "Epoch 232/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 4929.5215 - acc: 0.0000e+00 - val_loss: 4867.2819 - val_acc: 0.0000e+00\n",
            "Epoch 233/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 4925.8729 - acc: 0.0000e+00 - val_loss: 4864.6180 - val_acc: 0.0000e+00\n",
            "Epoch 234/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 4922.2467 - acc: 0.0000e+00 - val_loss: 4862.4547 - val_acc: 0.0000e+00\n",
            "Epoch 235/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 4918.6443 - acc: 0.0000e+00 - val_loss: 4860.1619 - val_acc: 0.0000e+00\n",
            "Epoch 236/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 4915.0413 - acc: 0.0000e+00 - val_loss: 4856.6981 - val_acc: 0.0000e+00\n",
            "Epoch 237/3000\n",
            "348/348 [==============================] - 0s 247us/sample - loss: 4911.4442 - acc: 0.0000e+00 - val_loss: 4853.3878 - val_acc: 0.0000e+00\n",
            "Epoch 238/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 4907.8450 - acc: 0.0000e+00 - val_loss: 4850.4053 - val_acc: 0.0000e+00\n",
            "Epoch 239/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 4904.2471 - acc: 0.0000e+00 - val_loss: 4846.4823 - val_acc: 0.0000e+00\n",
            "Epoch 240/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 4900.6668 - acc: 0.0000e+00 - val_loss: 4842.0892 - val_acc: 0.0000e+00\n",
            "Epoch 241/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 4897.0843 - acc: 0.0000e+00 - val_loss: 4838.1915 - val_acc: 0.0000e+00\n",
            "Epoch 242/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4893.4867 - acc: 0.0000e+00 - val_loss: 4833.6648 - val_acc: 0.0000e+00\n",
            "Epoch 243/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 4889.9241 - acc: 0.0000e+00 - val_loss: 4831.4361 - val_acc: 0.0000e+00\n",
            "Epoch 244/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 4886.3772 - acc: 0.0000e+00 - val_loss: 4829.4765 - val_acc: 0.0000e+00\n",
            "Epoch 245/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 4882.8988 - acc: 0.0000e+00 - val_loss: 4825.2475 - val_acc: 0.0000e+00\n",
            "Epoch 246/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4879.4070 - acc: 0.0000e+00 - val_loss: 4823.4676 - val_acc: 0.0000e+00\n",
            "Epoch 247/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 4875.9147 - acc: 0.0000e+00 - val_loss: 4822.6088 - val_acc: 0.0000e+00\n",
            "Epoch 248/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 4872.4098 - acc: 0.0000e+00 - val_loss: 4821.1459 - val_acc: 0.0000e+00\n",
            "Epoch 249/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4868.8940 - acc: 0.0000e+00 - val_loss: 4820.7207 - val_acc: 0.0000e+00\n",
            "Epoch 250/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 4865.3891 - acc: 0.0000e+00 - val_loss: 4821.1046 - val_acc: 0.0000e+00\n",
            "Epoch 251/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 4861.8732 - acc: 0.0000e+00 - val_loss: 4818.8650 - val_acc: 0.0000e+00\n",
            "Epoch 252/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 4858.3574 - acc: 0.0000e+00 - val_loss: 4817.2804 - val_acc: 0.0000e+00\n",
            "Epoch 253/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4854.8278 - acc: 0.0000e+00 - val_loss: 4813.8282 - val_acc: 0.0000e+00\n",
            "Epoch 254/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4851.3009 - acc: 0.0000e+00 - val_loss: 4813.1755 - val_acc: 0.0000e+00\n",
            "Epoch 255/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 4847.7566 - acc: 0.0000e+00 - val_loss: 4814.6427 - val_acc: 0.0000e+00\n",
            "Epoch 256/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4844.2300 - acc: 0.0000e+00 - val_loss: 4815.8186 - val_acc: 0.0000e+00\n",
            "Epoch 257/3000\n",
            "348/348 [==============================] - 0s 249us/sample - loss: 4840.7152 - acc: 0.0000e+00 - val_loss: 4814.4294 - val_acc: 0.0000e+00\n",
            "Epoch 258/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 4837.2023 - acc: 0.0000e+00 - val_loss: 4810.9402 - val_acc: 0.0000e+00\n",
            "Epoch 259/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4833.7023 - acc: 0.0000e+00 - val_loss: 4807.0253 - val_acc: 0.0000e+00\n",
            "Epoch 260/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 4830.1992 - acc: 0.0000e+00 - val_loss: 4805.2180 - val_acc: 0.0000e+00\n",
            "Epoch 261/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 4826.7060 - acc: 0.0000e+00 - val_loss: 4806.1356 - val_acc: 0.0000e+00\n",
            "Epoch 262/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4823.1630 - acc: 0.0000e+00 - val_loss: 4803.8672 - val_acc: 0.0000e+00\n",
            "Epoch 263/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 4819.5780 - acc: 0.0000e+00 - val_loss: 4804.7805 - val_acc: 0.0000e+00\n",
            "Epoch 264/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 4815.9123 - acc: 0.0000e+00 - val_loss: 4809.2024 - val_acc: 0.0000e+00\n",
            "Epoch 265/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4812.1984 - acc: 0.0000e+00 - val_loss: 4815.5878 - val_acc: 0.0000e+00\n",
            "Epoch 266/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 4808.4950 - acc: 0.0000e+00 - val_loss: 4816.0739 - val_acc: 0.0000e+00\n",
            "Epoch 267/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 4804.8219 - acc: 0.0000e+00 - val_loss: 4817.7887 - val_acc: 0.0000e+00\n",
            "Epoch 268/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 4801.0473 - acc: 0.0000e+00 - val_loss: 4816.6900 - val_acc: 0.0000e+00\n",
            "Epoch 269/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4797.1725 - acc: 0.0000e+00 - val_loss: 4819.9111 - val_acc: 0.0000e+00\n",
            "Epoch 270/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 4793.3171 - acc: 0.0000e+00 - val_loss: 4824.6300 - val_acc: 0.0000e+00\n",
            "Epoch 271/3000\n",
            "348/348 [==============================] - 0s 245us/sample - loss: 4789.3693 - acc: 0.0000e+00 - val_loss: 4829.2911 - val_acc: 0.0000e+00\n",
            "Epoch 272/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 4785.2597 - acc: 0.0000e+00 - val_loss: 4846.3954 - val_acc: 0.0000e+00\n",
            "Epoch 273/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 4781.0479 - acc: 0.0000e+00 - val_loss: 4862.6889 - val_acc: 0.0000e+00\n",
            "Epoch 274/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 4776.7558 - acc: 0.0000e+00 - val_loss: 4884.6785 - val_acc: 0.0000e+00\n",
            "Epoch 275/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 4772.3501 - acc: 0.0000e+00 - val_loss: 4910.2984 - val_acc: 0.0000e+00\n",
            "Epoch 276/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4767.6923 - acc: 0.0000e+00 - val_loss: 4945.0414 - val_acc: 0.0000e+00\n",
            "Epoch 277/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4762.6875 - acc: 0.0000e+00 - val_loss: 4990.9891 - val_acc: 0.0000e+00\n",
            "Epoch 278/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 4757.4718 - acc: 0.0000e+00 - val_loss: 5075.2473 - val_acc: 0.0000e+00\n",
            "Epoch 279/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 4751.2200 - acc: 0.0000e+00 - val_loss: 5243.3951 - val_acc: 0.0000e+00\n",
            "Epoch 280/3000\n",
            "348/348 [==============================] - 0s 240us/sample - loss: 4742.5871 - acc: 0.0000e+00 - val_loss: 5585.9740 - val_acc: 0.0000e+00\n",
            "Epoch 281/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 4731.1613 - acc: 0.0000e+00 - val_loss: 6740.7313 - val_acc: 0.0000e+00\n",
            "Epoch 282/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4640.2590 - acc: 0.0000e+00 - val_loss: 6048.6570 - val_acc: 0.0000e+00\n",
            "Epoch 283/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 4677.6724 - acc: 0.0000e+00 - val_loss: 7669.6731 - val_acc: 0.0000e+00\n",
            "Epoch 284/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 4855.2794 - acc: 0.0000e+00 - val_loss: 5059.5982 - val_acc: 0.0000e+00\n",
            "Epoch 285/3000\n",
            "348/348 [==============================] - 0s 240us/sample - loss: 4740.3215 - acc: 0.0000e+00 - val_loss: 5875.5005 - val_acc: 0.0000e+00\n",
            "Epoch 286/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 4737.6601 - acc: 0.0000e+00 - val_loss: 5841.9702 - val_acc: 0.0000e+00\n",
            "Epoch 287/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 4733.9644 - acc: 0.0000e+00 - val_loss: 5580.1169 - val_acc: 0.0000e+00\n",
            "Epoch 288/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 4729.8614 - acc: 0.0000e+00 - val_loss: 5347.1098 - val_acc: 0.0000e+00\n",
            "Epoch 289/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 4725.8969 - acc: 0.0000e+00 - val_loss: 5185.0695 - val_acc: 0.0000e+00\n",
            "Epoch 290/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4721.9639 - acc: 0.0000e+00 - val_loss: 5064.8923 - val_acc: 0.0000e+00\n",
            "Epoch 291/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4717.9033 - acc: 0.0000e+00 - val_loss: 4972.0106 - val_acc: 0.0000e+00\n",
            "Epoch 292/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4713.7619 - acc: 0.0000e+00 - val_loss: 4898.4520 - val_acc: 0.0000e+00\n",
            "Epoch 293/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 4709.4951 - acc: 0.0000e+00 - val_loss: 4835.0997 - val_acc: 0.0000e+00\n",
            "Epoch 294/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4705.1970 - acc: 0.0000e+00 - val_loss: 4787.3429 - val_acc: 0.0000e+00\n",
            "Epoch 295/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 4700.8809 - acc: 0.0000e+00 - val_loss: 4750.9055 - val_acc: 0.0000e+00\n",
            "Epoch 296/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4696.5199 - acc: 0.0000e+00 - val_loss: 4720.1292 - val_acc: 0.0000e+00\n",
            "Epoch 297/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 4692.1183 - acc: 0.0000e+00 - val_loss: 4698.5529 - val_acc: 0.0000e+00\n",
            "Epoch 298/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 4687.6106 - acc: 0.0000e+00 - val_loss: 4680.5276 - val_acc: 0.0000e+00\n",
            "Epoch 299/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 4683.0937 - acc: 0.0000e+00 - val_loss: 4665.8773 - val_acc: 0.0000e+00\n",
            "Epoch 300/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 4678.5293 - acc: 0.0000e+00 - val_loss: 4657.7404 - val_acc: 0.0000e+00\n",
            "Epoch 301/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 4673.9140 - acc: 0.0000e+00 - val_loss: 4652.2052 - val_acc: 0.0000e+00\n",
            "Epoch 302/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 4669.3401 - acc: 0.0000e+00 - val_loss: 4648.5033 - val_acc: 0.0000e+00\n",
            "Epoch 303/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 4664.8019 - acc: 0.0000e+00 - val_loss: 4643.8759 - val_acc: 0.0000e+00\n",
            "Epoch 304/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4660.2601 - acc: 0.0000e+00 - val_loss: 4638.9567 - val_acc: 0.0000e+00\n",
            "Epoch 305/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 4655.6608 - acc: 0.0000e+00 - val_loss: 4634.7373 - val_acc: 0.0000e+00\n",
            "Epoch 306/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 4651.0262 - acc: 0.0000e+00 - val_loss: 4630.6727 - val_acc: 0.0000e+00\n",
            "Epoch 307/3000\n",
            "348/348 [==============================] - 0s 254us/sample - loss: 4646.3035 - acc: 0.0000e+00 - val_loss: 4626.6462 - val_acc: 0.0000e+00\n",
            "Epoch 308/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 4641.4784 - acc: 0.0000e+00 - val_loss: 4623.2119 - val_acc: 0.0000e+00\n",
            "Epoch 309/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 4636.6135 - acc: 0.0000e+00 - val_loss: 4619.3824 - val_acc: 0.0000e+00\n",
            "Epoch 310/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 4631.7211 - acc: 0.0000e+00 - val_loss: 4615.4211 - val_acc: 0.0000e+00\n",
            "Epoch 311/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 4626.7456 - acc: 0.0000e+00 - val_loss: 4611.7186 - val_acc: 0.0000e+00\n",
            "Epoch 312/3000\n",
            "348/348 [==============================] - 0s 328us/sample - loss: 4621.6995 - acc: 0.0000e+00 - val_loss: 4607.7461 - val_acc: 0.0000e+00\n",
            "Epoch 313/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 4616.6233 - acc: 0.0000e+00 - val_loss: 4604.7808 - val_acc: 0.0000e+00\n",
            "Epoch 314/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 4611.5150 - acc: 0.0000e+00 - val_loss: 4602.3444 - val_acc: 0.0000e+00\n",
            "Epoch 315/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 4606.4467 - acc: 0.0000e+00 - val_loss: 4599.4023 - val_acc: 0.0000e+00\n",
            "Epoch 316/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4601.2625 - acc: 0.0000e+00 - val_loss: 4594.3408 - val_acc: 0.0000e+00\n",
            "Epoch 317/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 4595.8970 - acc: 0.0000e+00 - val_loss: 4589.6973 - val_acc: 0.0000e+00\n",
            "Epoch 318/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 4590.4506 - acc: 0.0000e+00 - val_loss: 4586.1645 - val_acc: 0.0000e+00\n",
            "Epoch 319/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 4584.8967 - acc: 0.0000e+00 - val_loss: 4581.8216 - val_acc: 0.0000e+00\n",
            "Epoch 320/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 4579.2806 - acc: 0.0000e+00 - val_loss: 4576.6859 - val_acc: 0.0000e+00\n",
            "Epoch 321/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4573.6268 - acc: 0.0000e+00 - val_loss: 4571.5772 - val_acc: 0.0000e+00\n",
            "Epoch 322/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4567.8351 - acc: 0.0000e+00 - val_loss: 4566.4292 - val_acc: 0.0000e+00\n",
            "Epoch 323/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 4561.8684 - acc: 0.0000e+00 - val_loss: 4562.0003 - val_acc: 0.0000e+00\n",
            "Epoch 324/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 4555.7506 - acc: 0.0000e+00 - val_loss: 4557.3666 - val_acc: 0.0000e+00\n",
            "Epoch 325/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 4549.4243 - acc: 0.0000e+00 - val_loss: 4552.1782 - val_acc: 0.0000e+00\n",
            "Epoch 326/3000\n",
            "348/348 [==============================] - 0s 274us/sample - loss: 4543.0223 - acc: 0.0000e+00 - val_loss: 4547.3903 - val_acc: 0.0000e+00\n",
            "Epoch 327/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4536.5876 - acc: 0.0000e+00 - val_loss: 4542.6349 - val_acc: 0.0000e+00\n",
            "Epoch 328/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 4530.0409 - acc: 0.0000e+00 - val_loss: 4537.8256 - val_acc: 0.0000e+00\n",
            "Epoch 329/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4523.3496 - acc: 0.0000e+00 - val_loss: 4533.0317 - val_acc: 0.0000e+00\n",
            "Epoch 330/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 4516.5562 - acc: 0.0000e+00 - val_loss: 4528.5881 - val_acc: 0.0000e+00\n",
            "Epoch 331/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4509.7097 - acc: 0.0000e+00 - val_loss: 4524.3434 - val_acc: 0.0000e+00\n",
            "Epoch 332/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 4502.8703 - acc: 0.0000e+00 - val_loss: 4519.4664 - val_acc: 0.0000e+00\n",
            "Epoch 333/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 4496.1798 - acc: 0.0000e+00 - val_loss: 4514.2723 - val_acc: 0.0000e+00\n",
            "Epoch 334/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 4489.5099 - acc: 0.0000e+00 - val_loss: 4509.1355 - val_acc: 0.0000e+00\n",
            "Epoch 335/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4482.7098 - acc: 0.0000e+00 - val_loss: 4504.0150 - val_acc: 0.0000e+00\n",
            "Epoch 336/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 4475.8799 - acc: 0.0000e+00 - val_loss: 4499.5527 - val_acc: 0.0000e+00\n",
            "Epoch 337/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4469.0260 - acc: 0.0000e+00 - val_loss: 4495.1057 - val_acc: 0.0000e+00\n",
            "Epoch 338/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4462.2447 - acc: 0.0000e+00 - val_loss: 4490.8528 - val_acc: 0.0000e+00\n",
            "Epoch 339/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 4455.4342 - acc: 0.0000e+00 - val_loss: 4486.3460 - val_acc: 0.0000e+00\n",
            "Epoch 340/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 4448.4096 - acc: 0.0000e+00 - val_loss: 4481.2307 - val_acc: 0.0000e+00\n",
            "Epoch 341/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4441.2474 - acc: 0.0000e+00 - val_loss: 4476.0285 - val_acc: 0.0000e+00\n",
            "Epoch 342/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4433.9391 - acc: 0.0000e+00 - val_loss: 4470.6551 - val_acc: 0.0000e+00\n",
            "Epoch 343/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 4426.4477 - acc: 0.0000e+00 - val_loss: 4466.1219 - val_acc: 0.0000e+00\n",
            "Epoch 344/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 4418.8720 - acc: 0.0000e+00 - val_loss: 4461.8977 - val_acc: 0.0000e+00\n",
            "Epoch 345/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 4411.2251 - acc: 0.0000e+00 - val_loss: 4457.7863 - val_acc: 0.0000e+00\n",
            "Epoch 346/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4403.5868 - acc: 0.0000e+00 - val_loss: 4452.6960 - val_acc: 0.0000e+00\n",
            "Epoch 347/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 4396.0586 - acc: 0.0000e+00 - val_loss: 4449.2109 - val_acc: 0.0000e+00\n",
            "Epoch 348/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 4388.6489 - acc: 0.0000e+00 - val_loss: 4448.2524 - val_acc: 0.0000e+00\n",
            "Epoch 349/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4381.3148 - acc: 0.0000e+00 - val_loss: 4446.0721 - val_acc: 0.0000e+00\n",
            "Epoch 350/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 4374.0365 - acc: 0.0000e+00 - val_loss: 4442.3535 - val_acc: 0.0000e+00\n",
            "Epoch 351/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 4366.8110 - acc: 0.0000e+00 - val_loss: 4434.4336 - val_acc: 0.0000e+00\n",
            "Epoch 352/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 4359.6070 - acc: 0.0000e+00 - val_loss: 4428.0035 - val_acc: 0.0000e+00\n",
            "Epoch 353/3000\n",
            "348/348 [==============================] - 0s 245us/sample - loss: 4352.2822 - acc: 0.0000e+00 - val_loss: 4420.8169 - val_acc: 0.0000e+00\n",
            "Epoch 354/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 4344.8266 - acc: 0.0000e+00 - val_loss: 4413.6893 - val_acc: 0.0000e+00\n",
            "Epoch 355/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 4337.1772 - acc: 0.0000e+00 - val_loss: 4407.4192 - val_acc: 0.0000e+00\n",
            "Epoch 356/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4329.4277 - acc: 0.0000e+00 - val_loss: 4403.4259 - val_acc: 0.0000e+00\n",
            "Epoch 357/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 4321.7247 - acc: 0.0000e+00 - val_loss: 4399.3225 - val_acc: 0.0000e+00\n",
            "Epoch 358/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 4314.0414 - acc: 0.0000e+00 - val_loss: 4395.3539 - val_acc: 0.0000e+00\n",
            "Epoch 359/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 4306.4267 - acc: 0.0000e+00 - val_loss: 4391.5301 - val_acc: 0.0000e+00\n",
            "Epoch 360/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4298.8787 - acc: 0.0000e+00 - val_loss: 4385.8424 - val_acc: 0.0000e+00\n",
            "Epoch 361/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4291.4323 - acc: 0.0000e+00 - val_loss: 4379.9981 - val_acc: 0.0000e+00\n",
            "Epoch 362/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 4284.1851 - acc: 0.0000e+00 - val_loss: 4372.4648 - val_acc: 0.0000e+00\n",
            "Epoch 363/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 4276.8838 - acc: 0.0000e+00 - val_loss: 4366.7918 - val_acc: 0.0000e+00\n",
            "Epoch 364/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 4269.5643 - acc: 0.0000e+00 - val_loss: 4363.2216 - val_acc: 0.0000e+00\n",
            "Epoch 365/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4262.1653 - acc: 0.0000e+00 - val_loss: 4356.3789 - val_acc: 0.0000e+00\n",
            "Epoch 366/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 4254.7359 - acc: 0.0000e+00 - val_loss: 4350.2888 - val_acc: 0.0000e+00\n",
            "Epoch 367/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 4247.4149 - acc: 0.0000e+00 - val_loss: 4345.7358 - val_acc: 0.0000e+00\n",
            "Epoch 368/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 4240.1838 - acc: 0.0000e+00 - val_loss: 4342.9571 - val_acc: 0.0000e+00\n",
            "Epoch 369/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4232.8989 - acc: 0.0000e+00 - val_loss: 4339.8081 - val_acc: 0.0000e+00\n",
            "Epoch 370/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4225.6989 - acc: 0.0000e+00 - val_loss: 4333.3696 - val_acc: 0.0000e+00\n",
            "Epoch 371/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4218.5663 - acc: 0.0000e+00 - val_loss: 4328.1893 - val_acc: 0.0000e+00\n",
            "Epoch 372/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 4211.4287 - acc: 0.0000e+00 - val_loss: 4322.8838 - val_acc: 0.0000e+00\n",
            "Epoch 373/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 4204.3954 - acc: 0.0000e+00 - val_loss: 4320.8170 - val_acc: 0.0000e+00\n",
            "Epoch 374/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 4197.4570 - acc: 0.0000e+00 - val_loss: 4315.4364 - val_acc: 0.0000e+00\n",
            "Epoch 375/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 4190.5719 - acc: 0.0000e+00 - val_loss: 4309.7233 - val_acc: 0.0000e+00\n",
            "Epoch 376/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 4183.7377 - acc: 0.0000e+00 - val_loss: 4303.6895 - val_acc: 0.0000e+00\n",
            "Epoch 377/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4176.9791 - acc: 0.0000e+00 - val_loss: 4299.8397 - val_acc: 0.0000e+00\n",
            "Epoch 378/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 4170.2407 - acc: 0.0000e+00 - val_loss: 4295.8103 - val_acc: 0.0000e+00\n",
            "Epoch 379/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 4163.6606 - acc: 0.0000e+00 - val_loss: 4292.7858 - val_acc: 0.0000e+00\n",
            "Epoch 380/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 4157.1835 - acc: 0.0000e+00 - val_loss: 4286.9682 - val_acc: 0.0000e+00\n",
            "Epoch 381/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 4150.8657 - acc: 0.0000e+00 - val_loss: 4281.1690 - val_acc: 0.0000e+00\n",
            "Epoch 382/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 4144.7821 - acc: 0.0000e+00 - val_loss: 4278.2550 - val_acc: 0.0000e+00\n",
            "Epoch 383/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 4138.8815 - acc: 0.0000e+00 - val_loss: 4274.1913 - val_acc: 0.0000e+00\n",
            "Epoch 384/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 4133.0494 - acc: 0.0000e+00 - val_loss: 4268.0390 - val_acc: 0.0000e+00\n",
            "Epoch 385/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 4127.2475 - acc: 0.0000e+00 - val_loss: 4263.8191 - val_acc: 0.0000e+00\n",
            "Epoch 386/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 4121.6039 - acc: 0.0000e+00 - val_loss: 4261.0429 - val_acc: 0.0000e+00\n",
            "Epoch 387/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 4116.0962 - acc: 0.0000e+00 - val_loss: 4259.8396 - val_acc: 0.0000e+00\n",
            "Epoch 388/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 4110.5762 - acc: 0.0000e+00 - val_loss: 4258.7155 - val_acc: 0.0000e+00\n",
            "Epoch 389/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4104.9933 - acc: 0.0000e+00 - val_loss: 4255.8040 - val_acc: 0.0000e+00\n",
            "Epoch 390/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 4099.4654 - acc: 0.0000e+00 - val_loss: 4254.9572 - val_acc: 0.0000e+00\n",
            "Epoch 391/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 4093.9176 - acc: 0.0000e+00 - val_loss: 4259.4933 - val_acc: 0.0000e+00\n",
            "Epoch 392/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 4088.3643 - acc: 0.0000e+00 - val_loss: 4262.3880 - val_acc: 0.0000e+00\n",
            "Epoch 393/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 4082.7815 - acc: 0.0000e+00 - val_loss: 4264.9330 - val_acc: 0.0000e+00\n",
            "Epoch 394/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 4077.3342 - acc: 0.0000e+00 - val_loss: 4265.9923 - val_acc: 0.0000e+00\n",
            "Epoch 395/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 4072.0930 - acc: 0.0000e+00 - val_loss: 4266.4489 - val_acc: 0.0000e+00\n",
            "Epoch 396/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 4066.9678 - acc: 0.0000e+00 - val_loss: 4273.3631 - val_acc: 0.0000e+00\n",
            "Epoch 397/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 4061.9041 - acc: 0.0000e+00 - val_loss: 4272.0872 - val_acc: 0.0000e+00\n",
            "Epoch 398/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 4056.8530 - acc: 0.0000e+00 - val_loss: 4274.6857 - val_acc: 0.0000e+00\n",
            "Epoch 399/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 4051.9997 - acc: 0.0000e+00 - val_loss: 4277.1693 - val_acc: 0.0000e+00\n",
            "Epoch 400/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 4047.1502 - acc: 0.0000e+00 - val_loss: 4277.3060 - val_acc: 0.0000e+00\n",
            "Epoch 401/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 4042.3826 - acc: 0.0000e+00 - val_loss: 4270.3509 - val_acc: 0.0000e+00\n",
            "Epoch 402/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4037.7364 - acc: 0.0000e+00 - val_loss: 4275.3813 - val_acc: 0.0000e+00\n",
            "Epoch 403/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 4032.8751 - acc: 0.0000e+00 - val_loss: 4282.9590 - val_acc: 0.0000e+00\n",
            "Epoch 404/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 4027.9141 - acc: 0.0000e+00 - val_loss: 4290.5835 - val_acc: 0.0000e+00\n",
            "Epoch 405/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 4023.0152 - acc: 0.0000e+00 - val_loss: 4302.7617 - val_acc: 0.0000e+00\n",
            "Epoch 406/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 4018.2257 - acc: 0.0000e+00 - val_loss: 4317.0443 - val_acc: 0.0000e+00\n",
            "Epoch 407/3000\n",
            "348/348 [==============================] - 0s 261us/sample - loss: 4013.2030 - acc: 0.0000e+00 - val_loss: 4339.6491 - val_acc: 0.0000e+00\n",
            "Epoch 408/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 4007.4683 - acc: 0.0000e+00 - val_loss: 4384.3569 - val_acc: 0.0000e+00\n",
            "Epoch 409/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 4001.1129 - acc: 0.0000e+00 - val_loss: 4458.2138 - val_acc: 0.0000e+00\n",
            "Epoch 410/3000\n",
            "348/348 [==============================] - 0s 261us/sample - loss: 3992.9223 - acc: 0.0000e+00 - val_loss: 4616.3316 - val_acc: 0.0000e+00\n",
            "Epoch 411/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3979.3231 - acc: 0.0000e+00 - val_loss: 4978.8543 - val_acc: 0.0000e+00\n",
            "Epoch 412/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3950.4298 - acc: 0.0000e+00 - val_loss: 5841.0458 - val_acc: 0.0000e+00\n",
            "Epoch 413/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3997.1796 - acc: 0.0000e+00 - val_loss: 4720.1971 - val_acc: 0.0000e+00\n",
            "Epoch 414/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3959.6357 - acc: 0.0000e+00 - val_loss: 4908.5982 - val_acc: 0.0000e+00\n",
            "Epoch 415/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3931.2472 - acc: 0.0000e+00 - val_loss: 5969.6189 - val_acc: 0.0000e+00\n",
            "Epoch 416/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 4032.5869 - acc: 0.0000e+00 - val_loss: 4154.2283 - val_acc: 0.0000e+00\n",
            "Epoch 417/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3977.0915 - acc: 0.0000e+00 - val_loss: 4164.0926 - val_acc: 0.0000e+00\n",
            "Epoch 418/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3972.4367 - acc: 0.0000e+00 - val_loss: 4146.7833 - val_acc: 0.0000e+00\n",
            "Epoch 419/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3966.1659 - acc: 0.0000e+00 - val_loss: 4143.4527 - val_acc: 0.0000e+00\n",
            "Epoch 420/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3957.2762 - acc: 0.0000e+00 - val_loss: 4216.6270 - val_acc: 0.0000e+00\n",
            "Epoch 421/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3941.7015 - acc: 0.0000e+00 - val_loss: 4522.9039 - val_acc: 0.0000e+00\n",
            "Epoch 422/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3912.9908 - acc: 0.0000e+00 - val_loss: 5369.2242 - val_acc: 0.0000e+00\n",
            "Epoch 423/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3976.0144 - acc: 0.0000e+00 - val_loss: 4215.1131 - val_acc: 0.0000e+00\n",
            "Epoch 424/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3935.1983 - acc: 0.0000e+00 - val_loss: 4254.2347 - val_acc: 0.0000e+00\n",
            "Epoch 425/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3914.9063 - acc: 0.0000e+00 - val_loss: 4717.5001 - val_acc: 0.0000e+00\n",
            "Epoch 426/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3914.2198 - acc: 0.0000e+00 - val_loss: 4969.7880 - val_acc: 0.0000e+00\n",
            "Epoch 427/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3955.1430 - acc: 0.0000e+00 - val_loss: 4173.8137 - val_acc: 0.0000e+00\n",
            "Epoch 428/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3915.3229 - acc: 0.0000e+00 - val_loss: 4319.1340 - val_acc: 0.0000e+00\n",
            "Epoch 429/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3892.1347 - acc: 0.0000e+00 - val_loss: 5173.7643 - val_acc: 0.0000e+00\n",
            "Epoch 430/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3982.0903 - acc: 0.0000e+00 - val_loss: 4136.4147 - val_acc: 0.0000e+00\n",
            "Epoch 431/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3924.7142 - acc: 0.0000e+00 - val_loss: 4126.9025 - val_acc: 0.0000e+00\n",
            "Epoch 432/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3909.9318 - acc: 0.0000e+00 - val_loss: 4163.9065 - val_acc: 0.0000e+00\n",
            "Epoch 433/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3884.0695 - acc: 0.0000e+00 - val_loss: 4889.3317 - val_acc: 0.0000e+00\n",
            "Epoch 434/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3950.5974 - acc: 0.0000e+00 - val_loss: 4093.2616 - val_acc: 0.0000e+00\n",
            "Epoch 435/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3904.1317 - acc: 0.0000e+00 - val_loss: 4110.3413 - val_acc: 0.0000e+00\n",
            "Epoch 436/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3882.4768 - acc: 0.0000e+00 - val_loss: 4476.7575 - val_acc: 0.0000e+00\n",
            "Epoch 437/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3910.3624 - acc: 0.0000e+00 - val_loss: 4162.7708 - val_acc: 0.0000e+00\n",
            "Epoch 438/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3886.4788 - acc: 0.0000e+00 - val_loss: 4323.5125 - val_acc: 0.0000e+00\n",
            "Epoch 439/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3887.7195 - acc: 0.0000e+00 - val_loss: 4413.6914 - val_acc: 0.0000e+00\n",
            "Epoch 440/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3905.8019 - acc: 0.0000e+00 - val_loss: 4116.6981 - val_acc: 0.0000e+00\n",
            "Epoch 441/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3880.3065 - acc: 0.0000e+00 - val_loss: 4257.1851 - val_acc: 0.0000e+00\n",
            "Epoch 442/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3882.5660 - acc: 0.0000e+00 - val_loss: 4356.4524 - val_acc: 0.0000e+00\n",
            "Epoch 443/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3906.6621 - acc: 0.0000e+00 - val_loss: 4067.7581 - val_acc: 0.0000e+00\n",
            "Epoch 444/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3876.2256 - acc: 0.0000e+00 - val_loss: 4155.3348 - val_acc: 0.0000e+00\n",
            "Epoch 445/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3867.2168 - acc: 0.0000e+00 - val_loss: 4798.1706 - val_acc: 0.0000e+00\n",
            "Epoch 446/3000\n",
            "348/348 [==============================] - 0s 253us/sample - loss: 3950.5464 - acc: 0.0000e+00 - val_loss: 4367.9770 - val_acc: 0.0000e+00\n",
            "Epoch 447/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3891.8627 - acc: 0.0000e+00 - val_loss: 4341.6564 - val_acc: 0.0000e+00\n",
            "Epoch 448/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3879.4890 - acc: 0.0000e+00 - val_loss: 4071.8400 - val_acc: 0.0000e+00\n",
            "Epoch 449/3000\n",
            "348/348 [==============================] - 0s 254us/sample - loss: 3860.1990 - acc: 0.0000e+00 - val_loss: 4281.9935 - val_acc: 0.0000e+00\n",
            "Epoch 450/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3875.2712 - acc: 0.0000e+00 - val_loss: 4267.0935 - val_acc: 0.0000e+00\n",
            "Epoch 451/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3876.0618 - acc: 0.0000e+00 - val_loss: 4206.9520 - val_acc: 0.0000e+00\n",
            "Epoch 452/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3869.7093 - acc: 0.0000e+00 - val_loss: 4233.4323 - val_acc: 0.0000e+00\n",
            "Epoch 453/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3874.8984 - acc: 0.0000e+00 - val_loss: 4137.1202 - val_acc: 0.0000e+00\n",
            "Epoch 454/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3861.5425 - acc: 0.0000e+00 - val_loss: 4239.4392 - val_acc: 0.0000e+00\n",
            "Epoch 455/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3881.1175 - acc: 0.0000e+00 - val_loss: 4049.7603 - val_acc: 0.0000e+00\n",
            "Epoch 456/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3858.3741 - acc: 0.0000e+00 - val_loss: 4104.6159 - val_acc: 0.0000e+00\n",
            "Epoch 457/3000\n",
            "348/348 [==============================] - 0s 190us/sample - loss: 3852.3541 - acc: 0.0000e+00 - val_loss: 4415.8219 - val_acc: 0.0000e+00\n",
            "Epoch 458/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3895.1041 - acc: 0.0000e+00 - val_loss: 4048.4163 - val_acc: 0.0000e+00\n",
            "Epoch 459/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3857.2251 - acc: 0.0000e+00 - val_loss: 4053.3100 - val_acc: 0.0000e+00\n",
            "Epoch 460/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3845.1941 - acc: 0.0000e+00 - val_loss: 4307.2196 - val_acc: 0.0000e+00\n",
            "Epoch 461/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3877.1059 - acc: 0.0000e+00 - val_loss: 4067.2209 - val_acc: 0.0000e+00\n",
            "Epoch 462/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3848.9999 - acc: 0.0000e+00 - val_loss: 4186.6512 - val_acc: 0.0000e+00\n",
            "Epoch 463/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3868.0881 - acc: 0.0000e+00 - val_loss: 4049.7250 - val_acc: 0.0000e+00\n",
            "Epoch 464/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3845.6276 - acc: 0.0000e+00 - val_loss: 4176.9179 - val_acc: 0.0000e+00\n",
            "Epoch 465/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3866.7398 - acc: 0.0000e+00 - val_loss: 4044.1684 - val_acc: 0.0000e+00\n",
            "Epoch 466/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3843.9074 - acc: 0.0000e+00 - val_loss: 4171.3763 - val_acc: 0.0000e+00\n",
            "Epoch 467/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3862.6619 - acc: 0.0000e+00 - val_loss: 4048.4211 - val_acc: 0.0000e+00\n",
            "Epoch 468/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3842.4759 - acc: 0.0000e+00 - val_loss: 4162.7073 - val_acc: 0.0000e+00\n",
            "Epoch 469/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3856.1532 - acc: 0.0000e+00 - val_loss: 4087.0007 - val_acc: 0.0000e+00\n",
            "Epoch 470/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3848.2561 - acc: 0.0000e+00 - val_loss: 4092.8416 - val_acc: 0.0000e+00\n",
            "Epoch 471/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3847.2937 - acc: 0.0000e+00 - val_loss: 4103.0618 - val_acc: 0.0000e+00\n",
            "Epoch 472/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3847.8085 - acc: 0.0000e+00 - val_loss: 4092.0638 - val_acc: 0.0000e+00\n",
            "Epoch 473/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3846.3200 - acc: 0.0000e+00 - val_loss: 4093.9393 - val_acc: 0.0000e+00\n",
            "Epoch 474/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 3845.6515 - acc: 0.0000e+00 - val_loss: 4089.0045 - val_acc: 0.0000e+00\n",
            "Epoch 475/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3844.9460 - acc: 0.0000e+00 - val_loss: 4083.4708 - val_acc: 0.0000e+00\n",
            "Epoch 476/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3843.9561 - acc: 0.0000e+00 - val_loss: 4074.0474 - val_acc: 0.0000e+00\n",
            "Epoch 477/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3843.1878 - acc: 0.0000e+00 - val_loss: 4070.1246 - val_acc: 0.0000e+00\n",
            "Epoch 478/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3842.3227 - acc: 0.0000e+00 - val_loss: 4070.2223 - val_acc: 0.0000e+00\n",
            "Epoch 479/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3841.4830 - acc: 0.0000e+00 - val_loss: 4069.8277 - val_acc: 0.0000e+00\n",
            "Epoch 480/3000\n",
            "348/348 [==============================] - 0s 190us/sample - loss: 3840.7278 - acc: 0.0000e+00 - val_loss: 4066.8694 - val_acc: 0.0000e+00\n",
            "Epoch 481/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3839.9627 - acc: 0.0000e+00 - val_loss: 4060.4863 - val_acc: 0.0000e+00\n",
            "Epoch 482/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3839.1215 - acc: 0.0000e+00 - val_loss: 4053.9636 - val_acc: 0.0000e+00\n",
            "Epoch 483/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 3838.2131 - acc: 0.0000e+00 - val_loss: 4052.7942 - val_acc: 0.0000e+00\n",
            "Epoch 484/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3837.3840 - acc: 0.0000e+00 - val_loss: 4054.7271 - val_acc: 0.0000e+00\n",
            "Epoch 485/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3836.6416 - acc: 0.0000e+00 - val_loss: 4054.9465 - val_acc: 0.0000e+00\n",
            "Epoch 486/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3836.0946 - acc: 0.0000e+00 - val_loss: 4052.7575 - val_acc: 0.0000e+00\n",
            "Epoch 487/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3835.5783 - acc: 0.0000e+00 - val_loss: 4050.6358 - val_acc: 0.0000e+00\n",
            "Epoch 488/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3834.8571 - acc: 0.0000e+00 - val_loss: 4048.1040 - val_acc: 0.0000e+00\n",
            "Epoch 489/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 3834.2601 - acc: 0.0000e+00 - val_loss: 4046.7264 - val_acc: 0.0000e+00\n",
            "Epoch 490/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3833.5528 - acc: 0.0000e+00 - val_loss: 4052.2442 - val_acc: 0.0000e+00\n",
            "Epoch 491/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3833.1523 - acc: 0.0000e+00 - val_loss: 4046.9302 - val_acc: 0.0000e+00\n",
            "Epoch 492/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3832.7037 - acc: 0.0000e+00 - val_loss: 4049.6934 - val_acc: 0.0000e+00\n",
            "Epoch 493/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3831.4400 - acc: 0.0000e+00 - val_loss: 4056.6401 - val_acc: 0.0000e+00\n",
            "Epoch 494/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 3831.6007 - acc: 0.0000e+00 - val_loss: 4052.7655 - val_acc: 0.0000e+00\n",
            "Epoch 495/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3830.9042 - acc: 0.0000e+00 - val_loss: 4046.2560 - val_acc: 0.0000e+00\n",
            "Epoch 496/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3829.7849 - acc: 0.0000e+00 - val_loss: 4045.8124 - val_acc: 0.0000e+00\n",
            "Epoch 497/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3830.1836 - acc: 0.0000e+00 - val_loss: 4043.4479 - val_acc: 0.0000e+00\n",
            "Epoch 498/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3829.2021 - acc: 0.0000e+00 - val_loss: 4042.3389 - val_acc: 0.0000e+00\n",
            "Epoch 499/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3827.1993 - acc: 0.0000e+00 - val_loss: 4053.9398 - val_acc: 0.0000e+00\n",
            "Epoch 500/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3829.3011 - acc: 0.0000e+00 - val_loss: 4046.2680 - val_acc: 0.0000e+00\n",
            "Epoch 501/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3827.7510 - acc: 0.0000e+00 - val_loss: 4044.8937 - val_acc: 0.0000e+00\n",
            "Epoch 502/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3825.6306 - acc: 0.0000e+00 - val_loss: 4058.0161 - val_acc: 0.0000e+00\n",
            "Epoch 503/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3827.9573 - acc: 0.0000e+00 - val_loss: 4042.2099 - val_acc: 0.0000e+00\n",
            "Epoch 504/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3825.1577 - acc: 0.0000e+00 - val_loss: 4046.8577 - val_acc: 0.0000e+00\n",
            "Epoch 505/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3826.6457 - acc: 0.0000e+00 - val_loss: 4032.7084 - val_acc: 0.0000e+00\n",
            "Epoch 506/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 3824.0743 - acc: 0.0000e+00 - val_loss: 4038.0065 - val_acc: 0.0000e+00\n",
            "Epoch 507/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3825.8387 - acc: 0.0000e+00 - val_loss: 4029.3430 - val_acc: 0.0000e+00\n",
            "Epoch 508/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3823.3220 - acc: 0.0000e+00 - val_loss: 4036.3131 - val_acc: 0.0000e+00\n",
            "Epoch 509/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3825.0026 - acc: 0.0000e+00 - val_loss: 4027.8117 - val_acc: 0.0000e+00\n",
            "Epoch 510/3000\n",
            "348/348 [==============================] - 0s 270us/sample - loss: 3822.5441 - acc: 0.0000e+00 - val_loss: 4033.3476 - val_acc: 0.0000e+00\n",
            "Epoch 511/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3823.7889 - acc: 0.0000e+00 - val_loss: 4025.7702 - val_acc: 0.0000e+00\n",
            "Epoch 512/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 3821.7712 - acc: 0.0000e+00 - val_loss: 4031.5089 - val_acc: 0.0000e+00\n",
            "Epoch 513/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3823.1801 - acc: 0.0000e+00 - val_loss: 4022.6086 - val_acc: 0.0000e+00\n",
            "Epoch 514/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 3820.7528 - acc: 0.0000e+00 - val_loss: 4030.2938 - val_acc: 0.0000e+00\n",
            "Epoch 515/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3822.1017 - acc: 0.0000e+00 - val_loss: 4027.6531 - val_acc: 0.0000e+00\n",
            "Epoch 516/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3820.1614 - acc: 0.0000e+00 - val_loss: 4039.5321 - val_acc: 0.0000e+00\n",
            "Epoch 517/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3820.1023 - acc: 0.0000e+00 - val_loss: 4038.5388 - val_acc: 0.0000e+00\n",
            "Epoch 518/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3820.9440 - acc: 0.0000e+00 - val_loss: 4027.3260 - val_acc: 0.0000e+00\n",
            "Epoch 519/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3819.0363 - acc: 0.0000e+00 - val_loss: 4033.0146 - val_acc: 0.0000e+00\n",
            "Epoch 520/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3818.4748 - acc: 0.0000e+00 - val_loss: 4041.5613 - val_acc: 0.0000e+00\n",
            "Epoch 521/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3820.0213 - acc: 0.0000e+00 - val_loss: 4028.3083 - val_acc: 0.0000e+00\n",
            "Epoch 522/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3817.3928 - acc: 0.0000e+00 - val_loss: 4035.6495 - val_acc: 0.0000e+00\n",
            "Epoch 523/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3817.3995 - acc: 0.0000e+00 - val_loss: 4040.0102 - val_acc: 0.0000e+00\n",
            "Epoch 524/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3817.3053 - acc: 0.0000e+00 - val_loss: 4037.1022 - val_acc: 0.0000e+00\n",
            "Epoch 525/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3817.2822 - acc: 0.0000e+00 - val_loss: 4030.8551 - val_acc: 0.0000e+00\n",
            "Epoch 526/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3816.6835 - acc: 0.0000e+00 - val_loss: 4028.9020 - val_acc: 0.0000e+00\n",
            "Epoch 527/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3816.1705 - acc: 0.0000e+00 - val_loss: 4025.7239 - val_acc: 0.0000e+00\n",
            "Epoch 528/3000\n",
            "348/348 [==============================] - 0s 242us/sample - loss: 3816.0229 - acc: 0.0000e+00 - val_loss: 4020.4471 - val_acc: 0.0000e+00\n",
            "Epoch 529/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3815.6345 - acc: 0.0000e+00 - val_loss: 4018.7604 - val_acc: 0.0000e+00\n",
            "Epoch 530/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3815.1490 - acc: 0.0000e+00 - val_loss: 4020.7258 - val_acc: 0.0000e+00\n",
            "Epoch 531/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3814.8957 - acc: 0.0000e+00 - val_loss: 4020.6023 - val_acc: 0.0000e+00\n",
            "Epoch 532/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3814.5971 - acc: 0.0000e+00 - val_loss: 4018.6599 - val_acc: 0.0000e+00\n",
            "Epoch 533/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3814.1118 - acc: 0.0000e+00 - val_loss: 4016.5149 - val_acc: 0.0000e+00\n",
            "Epoch 534/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3813.7401 - acc: 0.0000e+00 - val_loss: 4015.7695 - val_acc: 0.0000e+00\n",
            "Epoch 535/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3813.3938 - acc: 0.0000e+00 - val_loss: 4014.8229 - val_acc: 0.0000e+00\n",
            "Epoch 536/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3813.1718 - acc: 0.0000e+00 - val_loss: 4010.2731 - val_acc: 0.0000e+00\n",
            "Epoch 537/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3812.7791 - acc: 0.0000e+00 - val_loss: 4008.1629 - val_acc: 0.0000e+00\n",
            "Epoch 538/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3812.4503 - acc: 0.0000e+00 - val_loss: 4009.6021 - val_acc: 0.0000e+00\n",
            "Epoch 539/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3812.1426 - acc: 0.0000e+00 - val_loss: 4012.2976 - val_acc: 0.0000e+00\n",
            "Epoch 540/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3811.7829 - acc: 0.0000e+00 - val_loss: 4012.0028 - val_acc: 0.0000e+00\n",
            "Epoch 541/3000\n",
            "348/348 [==============================] - 0s 241us/sample - loss: 3811.5538 - acc: 0.0000e+00 - val_loss: 4011.1799 - val_acc: 0.0000e+00\n",
            "Epoch 542/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3811.2624 - acc: 0.0000e+00 - val_loss: 4010.3801 - val_acc: 0.0000e+00\n",
            "Epoch 543/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3810.7779 - acc: 0.0000e+00 - val_loss: 4015.7434 - val_acc: 0.0000e+00\n",
            "Epoch 544/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3810.8580 - acc: 0.0000e+00 - val_loss: 4017.1725 - val_acc: 0.0000e+00\n",
            "Epoch 545/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3810.6058 - acc: 0.0000e+00 - val_loss: 4015.1003 - val_acc: 0.0000e+00\n",
            "Epoch 546/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3809.8879 - acc: 0.0000e+00 - val_loss: 4017.7830 - val_acc: 0.0000e+00\n",
            "Epoch 547/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3809.7893 - acc: 0.0000e+00 - val_loss: 4017.6343 - val_acc: 0.0000e+00\n",
            "Epoch 548/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3809.4995 - acc: 0.0000e+00 - val_loss: 4014.0992 - val_acc: 0.0000e+00\n",
            "Epoch 549/3000\n",
            "348/348 [==============================] - 0s 250us/sample - loss: 3809.1848 - acc: 0.0000e+00 - val_loss: 4013.4138 - val_acc: 0.0000e+00\n",
            "Epoch 550/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3808.9192 - acc: 0.0000e+00 - val_loss: 4008.9610 - val_acc: 0.0000e+00\n",
            "Epoch 551/3000\n",
            "348/348 [==============================] - 0s 241us/sample - loss: 3808.6059 - acc: 0.0000e+00 - val_loss: 4007.7942 - val_acc: 0.0000e+00\n",
            "Epoch 552/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3808.3212 - acc: 0.0000e+00 - val_loss: 4007.3028 - val_acc: 0.0000e+00\n",
            "Epoch 553/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3807.9741 - acc: 0.0000e+00 - val_loss: 4010.4859 - val_acc: 0.0000e+00\n",
            "Epoch 554/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3807.8100 - acc: 0.0000e+00 - val_loss: 4006.9285 - val_acc: 0.0000e+00\n",
            "Epoch 555/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3807.6685 - acc: 0.0000e+00 - val_loss: 4002.0336 - val_acc: 0.0000e+00\n",
            "Epoch 556/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3807.3440 - acc: 0.0000e+00 - val_loss: 4001.6425 - val_acc: 0.0000e+00\n",
            "Epoch 557/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3807.1079 - acc: 0.0000e+00 - val_loss: 4001.5465 - val_acc: 0.0000e+00\n",
            "Epoch 558/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3806.9082 - acc: 0.0000e+00 - val_loss: 4001.5606 - val_acc: 0.0000e+00\n",
            "Epoch 559/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3806.5828 - acc: 0.0000e+00 - val_loss: 4000.8893 - val_acc: 0.0000e+00\n",
            "Epoch 560/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3805.2894 - acc: 0.0000e+00 - val_loss: 4011.6879 - val_acc: 0.0000e+00\n",
            "Epoch 561/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3806.6336 - acc: 0.0000e+00 - val_loss: 4009.2840 - val_acc: 0.0000e+00\n",
            "Epoch 562/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3806.5994 - acc: 0.0000e+00 - val_loss: 4002.4412 - val_acc: 0.0000e+00\n",
            "Epoch 563/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3804.6809 - acc: 0.0000e+00 - val_loss: 4012.5388 - val_acc: 0.0000e+00\n",
            "Epoch 564/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3805.9442 - acc: 0.0000e+00 - val_loss: 4009.1190 - val_acc: 0.0000e+00\n",
            "Epoch 565/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3804.7867 - acc: 0.0000e+00 - val_loss: 4015.9913 - val_acc: 0.0000e+00\n",
            "Epoch 566/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3805.9923 - acc: 0.0000e+00 - val_loss: 4008.7255 - val_acc: 0.0000e+00\n",
            "Epoch 567/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3804.3879 - acc: 0.0000e+00 - val_loss: 4016.0508 - val_acc: 0.0000e+00\n",
            "Epoch 568/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 3805.5334 - acc: 0.0000e+00 - val_loss: 4008.4265 - val_acc: 0.0000e+00\n",
            "Epoch 569/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3804.1138 - acc: 0.0000e+00 - val_loss: 4009.6587 - val_acc: 0.0000e+00\n",
            "Epoch 570/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3803.7511 - acc: 0.0000e+00 - val_loss: 4016.7480 - val_acc: 0.0000e+00\n",
            "Epoch 571/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3804.9553 - acc: 0.0000e+00 - val_loss: 4007.2655 - val_acc: 0.0000e+00\n",
            "Epoch 572/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3803.6131 - acc: 0.0000e+00 - val_loss: 4006.7130 - val_acc: 0.0000e+00\n",
            "Epoch 573/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3803.1926 - acc: 0.0000e+00 - val_loss: 4011.5653 - val_acc: 0.0000e+00\n",
            "Epoch 574/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3803.3784 - acc: 0.0000e+00 - val_loss: 4014.4271 - val_acc: 0.0000e+00\n",
            "Epoch 575/3000\n",
            "348/348 [==============================] - 0s 187us/sample - loss: 3803.2878 - acc: 0.0000e+00 - val_loss: 4022.1260 - val_acc: 0.0000e+00\n",
            "Epoch 576/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3804.5054 - acc: 0.0000e+00 - val_loss: 4005.1135 - val_acc: 0.0000e+00\n",
            "Epoch 577/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3802.4210 - acc: 0.0000e+00 - val_loss: 4007.0870 - val_acc: 0.0000e+00\n",
            "Epoch 578/3000\n",
            "348/348 [==============================] - 0s 181us/sample - loss: 3802.4311 - acc: 0.0000e+00 - val_loss: 4002.1857 - val_acc: 0.0000e+00\n",
            "Epoch 579/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3802.7394 - acc: 0.0000e+00 - val_loss: 3998.9628 - val_acc: 0.0000e+00\n",
            "Epoch 580/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3802.5679 - acc: 0.0000e+00 - val_loss: 3998.5631 - val_acc: 0.0000e+00\n",
            "Epoch 581/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3802.2348 - acc: 0.0000e+00 - val_loss: 4000.1990 - val_acc: 0.0000e+00\n",
            "Epoch 582/3000\n",
            "348/348 [==============================] - 0s 242us/sample - loss: 3802.0615 - acc: 0.0000e+00 - val_loss: 4001.8951 - val_acc: 0.0000e+00\n",
            "Epoch 583/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3801.8834 - acc: 0.0000e+00 - val_loss: 4001.0132 - val_acc: 0.0000e+00\n",
            "Epoch 584/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3801.1337 - acc: 0.0000e+00 - val_loss: 4006.2653 - val_acc: 0.0000e+00\n",
            "Epoch 585/3000\n",
            "348/348 [==============================] - 0s 275us/sample - loss: 3801.8307 - acc: 0.0000e+00 - val_loss: 4000.0068 - val_acc: 0.0000e+00\n",
            "Epoch 586/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3801.7661 - acc: 0.0000e+00 - val_loss: 3995.6692 - val_acc: 0.0000e+00\n",
            "Epoch 587/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3800.9032 - acc: 0.0000e+00 - val_loss: 3995.5701 - val_acc: 0.0000e+00\n",
            "Epoch 588/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3801.1973 - acc: 0.0000e+00 - val_loss: 3990.6257 - val_acc: 0.0000e+00\n",
            "Epoch 589/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 3800.8339 - acc: 0.0000e+00 - val_loss: 3989.0865 - val_acc: 0.0000e+00\n",
            "Epoch 590/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3800.6832 - acc: 0.0000e+00 - val_loss: 3989.1473 - val_acc: 0.0000e+00\n",
            "Epoch 591/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3800.9228 - acc: 0.0000e+00 - val_loss: 3991.6054 - val_acc: 0.0000e+00\n",
            "Epoch 592/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3800.3203 - acc: 0.0000e+00 - val_loss: 3995.5611 - val_acc: 0.0000e+00\n",
            "Epoch 593/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3800.6449 - acc: 0.0000e+00 - val_loss: 3992.9065 - val_acc: 0.0000e+00\n",
            "Epoch 594/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3800.0650 - acc: 0.0000e+00 - val_loss: 3994.5294 - val_acc: 0.0000e+00\n",
            "Epoch 595/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3800.2076 - acc: 0.0000e+00 - val_loss: 3992.9580 - val_acc: 0.0000e+00\n",
            "Epoch 596/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3799.6685 - acc: 0.0000e+00 - val_loss: 3996.7206 - val_acc: 0.0000e+00\n",
            "Epoch 597/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3800.1264 - acc: 0.0000e+00 - val_loss: 3996.0647 - val_acc: 0.0000e+00\n",
            "Epoch 598/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3799.4925 - acc: 0.0000e+00 - val_loss: 4001.6059 - val_acc: 0.0000e+00\n",
            "Epoch 599/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3799.8468 - acc: 0.0000e+00 - val_loss: 3999.6907 - val_acc: 0.0000e+00\n",
            "Epoch 600/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3799.1516 - acc: 0.0000e+00 - val_loss: 4000.6742 - val_acc: 0.0000e+00\n",
            "Epoch 601/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3799.5262 - acc: 0.0000e+00 - val_loss: 3997.4284 - val_acc: 0.0000e+00\n",
            "Epoch 602/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3798.8112 - acc: 0.0000e+00 - val_loss: 3999.8433 - val_acc: 0.0000e+00\n",
            "Epoch 603/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3799.2483 - acc: 0.0000e+00 - val_loss: 3992.5682 - val_acc: 0.0000e+00\n",
            "Epoch 604/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3798.7326 - acc: 0.0000e+00 - val_loss: 3992.8099 - val_acc: 0.0000e+00\n",
            "Epoch 605/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3798.4197 - acc: 0.0000e+00 - val_loss: 3997.4904 - val_acc: 0.0000e+00\n",
            "Epoch 606/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3798.8953 - acc: 0.0000e+00 - val_loss: 3994.1742 - val_acc: 0.0000e+00\n",
            "Epoch 607/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3798.3026 - acc: 0.0000e+00 - val_loss: 3995.8019 - val_acc: 0.0000e+00\n",
            "Epoch 608/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3798.1140 - acc: 0.0000e+00 - val_loss: 3999.0916 - val_acc: 0.0000e+00\n",
            "Epoch 609/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3798.6804 - acc: 0.0000e+00 - val_loss: 3994.3632 - val_acc: 0.0000e+00\n",
            "Epoch 610/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3797.6375 - acc: 0.0000e+00 - val_loss: 3997.4758 - val_acc: 0.0000e+00\n",
            "Epoch 611/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3797.7190 - acc: 0.0000e+00 - val_loss: 3994.5633 - val_acc: 0.0000e+00\n",
            "Epoch 612/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3798.1793 - acc: 0.0000e+00 - val_loss: 3990.8571 - val_acc: 0.0000e+00\n",
            "Epoch 613/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3797.1568 - acc: 0.0000e+00 - val_loss: 3994.0057 - val_acc: 0.0000e+00\n",
            "Epoch 614/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3797.2399 - acc: 0.0000e+00 - val_loss: 3992.4711 - val_acc: 0.0000e+00\n",
            "Epoch 615/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3797.3388 - acc: 0.0000e+00 - val_loss: 3988.5849 - val_acc: 0.0000e+00\n",
            "Epoch 616/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3797.1052 - acc: 0.0000e+00 - val_loss: 3991.7017 - val_acc: 0.0000e+00\n",
            "Epoch 617/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3796.7547 - acc: 0.0000e+00 - val_loss: 4001.1355 - val_acc: 0.0000e+00\n",
            "Epoch 618/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3796.5427 - acc: 0.0000e+00 - val_loss: 4013.3802 - val_acc: 0.0000e+00\n",
            "Epoch 619/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3796.5837 - acc: 0.0000e+00 - val_loss: 4030.0654 - val_acc: 0.0000e+00\n",
            "Epoch 620/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3796.4545 - acc: 0.0000e+00 - val_loss: 4045.1746 - val_acc: 0.0000e+00\n",
            "Epoch 621/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3795.9787 - acc: 0.0000e+00 - val_loss: 4061.6727 - val_acc: 0.0000e+00\n",
            "Epoch 622/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3796.2009 - acc: 0.0000e+00 - val_loss: 4073.0565 - val_acc: 0.0000e+00\n",
            "Epoch 623/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3796.0876 - acc: 0.0000e+00 - val_loss: 4089.6485 - val_acc: 0.0000e+00\n",
            "Epoch 624/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3795.6008 - acc: 0.0000e+00 - val_loss: 4117.7377 - val_acc: 0.0000e+00\n",
            "Epoch 625/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3795.7014 - acc: 0.0000e+00 - val_loss: 4138.2101 - val_acc: 0.0000e+00\n",
            "Epoch 626/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3795.4056 - acc: 0.0000e+00 - val_loss: 4145.3701 - val_acc: 0.0000e+00\n",
            "Epoch 627/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3795.4239 - acc: 0.0000e+00 - val_loss: 4157.3091 - val_acc: 0.0000e+00\n",
            "Epoch 628/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3795.0947 - acc: 0.0000e+00 - val_loss: 4160.9251 - val_acc: 0.0000e+00\n",
            "Epoch 629/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3795.1369 - acc: 0.0000e+00 - val_loss: 4164.0278 - val_acc: 0.0000e+00\n",
            "Epoch 630/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3794.7921 - acc: 0.0000e+00 - val_loss: 4168.3037 - val_acc: 0.0000e+00\n",
            "Epoch 631/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3794.6345 - acc: 0.0000e+00 - val_loss: 4179.6909 - val_acc: 0.0000e+00\n",
            "Epoch 632/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3794.8996 - acc: 0.0000e+00 - val_loss: 4179.0330 - val_acc: 0.0000e+00\n",
            "Epoch 633/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3794.3769 - acc: 0.0000e+00 - val_loss: 4175.6970 - val_acc: 0.0000e+00\n",
            "Epoch 634/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3794.1455 - acc: 0.0000e+00 - val_loss: 4158.6620 - val_acc: 0.0000e+00\n",
            "Epoch 635/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3793.2450 - acc: 0.0000e+00 - val_loss: 4160.3391 - val_acc: 0.0000e+00\n",
            "Epoch 636/3000\n",
            "348/348 [==============================] - 0s 255us/sample - loss: 3792.8829 - acc: 0.0000e+00 - val_loss: 4172.1602 - val_acc: 0.0000e+00\n",
            "Epoch 637/3000\n",
            "348/348 [==============================] - 0s 245us/sample - loss: 3792.8846 - acc: 0.0000e+00 - val_loss: 4178.6433 - val_acc: 0.0000e+00\n",
            "Epoch 638/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3792.7877 - acc: 0.0000e+00 - val_loss: 4178.4519 - val_acc: 0.0000e+00\n",
            "Epoch 639/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3792.5227 - acc: 0.0000e+00 - val_loss: 4188.0415 - val_acc: 0.0000e+00\n",
            "Epoch 640/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3792.3211 - acc: 0.0000e+00 - val_loss: 4203.6841 - val_acc: 0.0000e+00\n",
            "Epoch 641/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3792.2454 - acc: 0.0000e+00 - val_loss: 4215.0716 - val_acc: 0.0000e+00\n",
            "Epoch 642/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3791.9328 - acc: 0.0000e+00 - val_loss: 4224.0931 - val_acc: 0.0000e+00\n",
            "Epoch 643/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3791.9394 - acc: 0.0000e+00 - val_loss: 4222.5531 - val_acc: 0.0000e+00\n",
            "Epoch 644/3000\n",
            "348/348 [==============================] - 0s 184us/sample - loss: 3791.5686 - acc: 0.0000e+00 - val_loss: 4227.1639 - val_acc: 0.0000e+00\n",
            "Epoch 645/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3791.6354 - acc: 0.0000e+00 - val_loss: 4211.4565 - val_acc: 0.0000e+00\n",
            "Epoch 646/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3791.3330 - acc: 0.0000e+00 - val_loss: 4214.1960 - val_acc: 0.0000e+00\n",
            "Epoch 647/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3791.1456 - acc: 0.0000e+00 - val_loss: 4225.7602 - val_acc: 0.0000e+00\n",
            "Epoch 648/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3791.0597 - acc: 0.0000e+00 - val_loss: 4236.8317 - val_acc: 0.0000e+00\n",
            "Epoch 649/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3790.8933 - acc: 0.0000e+00 - val_loss: 4244.7373 - val_acc: 0.0000e+00\n",
            "Epoch 650/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3790.6643 - acc: 0.0000e+00 - val_loss: 4245.4012 - val_acc: 0.0000e+00\n",
            "Epoch 651/3000\n",
            "348/348 [==============================] - 0s 269us/sample - loss: 3790.6208 - acc: 0.0000e+00 - val_loss: 4241.0447 - val_acc: 0.0000e+00\n",
            "Epoch 652/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3790.4091 - acc: 0.0000e+00 - val_loss: 4232.5497 - val_acc: 0.0000e+00\n",
            "Epoch 653/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3790.2230 - acc: 0.0000e+00 - val_loss: 4223.2516 - val_acc: 0.0000e+00\n",
            "Epoch 654/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3790.1602 - acc: 0.0000e+00 - val_loss: 4226.2122 - val_acc: 0.0000e+00\n",
            "Epoch 655/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3789.8426 - acc: 0.0000e+00 - val_loss: 4226.5624 - val_acc: 0.0000e+00\n",
            "Epoch 656/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3789.8807 - acc: 0.0000e+00 - val_loss: 4221.6856 - val_acc: 0.0000e+00\n",
            "Epoch 657/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3789.5269 - acc: 0.0000e+00 - val_loss: 4236.8067 - val_acc: 0.0000e+00\n",
            "Epoch 658/3000\n",
            "348/348 [==============================] - 0s 187us/sample - loss: 3789.6263 - acc: 0.0000e+00 - val_loss: 4251.4790 - val_acc: 0.0000e+00\n",
            "Epoch 659/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3789.3159 - acc: 0.0000e+00 - val_loss: 4266.5130 - val_acc: 0.0000e+00\n",
            "Epoch 660/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3789.0893 - acc: 0.0000e+00 - val_loss: 4280.7934 - val_acc: 0.0000e+00\n",
            "Epoch 661/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3789.1913 - acc: 0.0000e+00 - val_loss: 4290.2415 - val_acc: 0.0000e+00\n",
            "Epoch 662/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3788.8649 - acc: 0.0000e+00 - val_loss: 4299.1201 - val_acc: 0.0000e+00\n",
            "Epoch 663/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3788.8838 - acc: 0.0000e+00 - val_loss: 4308.7223 - val_acc: 0.0000e+00\n",
            "Epoch 664/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3788.0331 - acc: 0.0000e+00 - val_loss: 4370.3413 - val_acc: 0.0000e+00\n",
            "Epoch 665/3000\n",
            "348/348 [==============================] - 0s 252us/sample - loss: 3788.7922 - acc: 0.0000e+00 - val_loss: 4411.7450 - val_acc: 0.0000e+00\n",
            "Epoch 666/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3788.7400 - acc: 0.0000e+00 - val_loss: 4410.1865 - val_acc: 0.0000e+00\n",
            "Epoch 667/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3787.6863 - acc: 0.0000e+00 - val_loss: 4442.9837 - val_acc: 0.0000e+00\n",
            "Epoch 668/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3788.3402 - acc: 0.0000e+00 - val_loss: 4448.5863 - val_acc: 0.0000e+00\n",
            "Epoch 669/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3788.3128 - acc: 0.0000e+00 - val_loss: 4434.0780 - val_acc: 0.0000e+00\n",
            "Epoch 670/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3787.3492 - acc: 0.0000e+00 - val_loss: 4467.8396 - val_acc: 0.0000e+00\n",
            "Epoch 671/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3787.8481 - acc: 0.0000e+00 - val_loss: 4458.9865 - val_acc: 0.0000e+00\n",
            "Epoch 672/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3787.4141 - acc: 0.0000e+00 - val_loss: 4483.7236 - val_acc: 0.0000e+00\n",
            "Epoch 673/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3787.7413 - acc: 0.0000e+00 - val_loss: 4489.2428 - val_acc: 0.0000e+00\n",
            "Epoch 674/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3787.1344 - acc: 0.0000e+00 - val_loss: 4539.0300 - val_acc: 0.0000e+00\n",
            "Epoch 675/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3787.5013 - acc: 0.0000e+00 - val_loss: 4539.1880 - val_acc: 0.0000e+00\n",
            "Epoch 676/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3786.8741 - acc: 0.0000e+00 - val_loss: 4582.2484 - val_acc: 0.0000e+00\n",
            "Epoch 677/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3787.2974 - acc: 0.0000e+00 - val_loss: 4608.1840 - val_acc: 0.0000e+00\n",
            "Epoch 678/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 3786.7102 - acc: 0.0000e+00 - val_loss: 4657.9668 - val_acc: 0.0000e+00\n",
            "Epoch 679/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 3786.9015 - acc: 0.0000e+00 - val_loss: 4721.1693 - val_acc: 0.0000e+00\n",
            "Epoch 680/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3786.7897 - acc: 0.0000e+00 - val_loss: 4862.6516 - val_acc: 0.0000e+00\n",
            "Epoch 681/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3786.0789 - acc: 0.0000e+00 - val_loss: 5092.3829 - val_acc: 0.0000e+00\n",
            "Epoch 682/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3786.5511 - acc: 0.0000e+00 - val_loss: 5348.8541 - val_acc: 0.0000e+00\n",
            "Epoch 683/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3786.5145 - acc: 0.0000e+00 - val_loss: 5372.2151 - val_acc: 0.0000e+00\n",
            "Epoch 684/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3785.7445 - acc: 0.0000e+00 - val_loss: 5389.4666 - val_acc: 0.0000e+00\n",
            "Epoch 685/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3786.0049 - acc: 0.0000e+00 - val_loss: 5689.7365 - val_acc: 0.0000e+00\n",
            "Epoch 686/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3785.3471 - acc: 0.0000e+00 - val_loss: 6388.9037 - val_acc: 0.0000e+00\n",
            "Epoch 687/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3786.1049 - acc: 0.0000e+00 - val_loss: 5599.8844 - val_acc: 0.0000e+00\n",
            "Epoch 688/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3785.1958 - acc: 0.0000e+00 - val_loss: 5283.1959 - val_acc: 0.0000e+00\n",
            "Epoch 689/3000\n",
            "348/348 [==============================] - 0s 187us/sample - loss: 3785.4574 - acc: 0.0000e+00 - val_loss: 5051.7642 - val_acc: 0.0000e+00\n",
            "Epoch 690/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3785.3575 - acc: 0.0000e+00 - val_loss: 5136.2899 - val_acc: 0.0000e+00\n",
            "Epoch 691/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 3784.6924 - acc: 0.0000e+00 - val_loss: 5108.3024 - val_acc: 0.0000e+00\n",
            "Epoch 692/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3785.1884 - acc: 0.0000e+00 - val_loss: 4912.0792 - val_acc: 0.0000e+00\n",
            "Epoch 693/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3784.9841 - acc: 0.0000e+00 - val_loss: 4734.3193 - val_acc: 0.0000e+00\n",
            "Epoch 694/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3784.2035 - acc: 0.0000e+00 - val_loss: 4607.2588 - val_acc: 0.0000e+00\n",
            "Epoch 695/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3784.6872 - acc: 0.0000e+00 - val_loss: 4457.8523 - val_acc: 0.0000e+00\n",
            "Epoch 696/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3784.1674 - acc: 0.0000e+00 - val_loss: 4356.0834 - val_acc: 0.0000e+00\n",
            "Epoch 697/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3784.5240 - acc: 0.0000e+00 - val_loss: 4383.9214 - val_acc: 0.0000e+00\n",
            "Epoch 698/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3783.9155 - acc: 0.0000e+00 - val_loss: 4367.7180 - val_acc: 0.0000e+00\n",
            "Epoch 699/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3784.2383 - acc: 0.0000e+00 - val_loss: 4284.6888 - val_acc: 0.0000e+00\n",
            "Epoch 700/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3783.6049 - acc: 0.0000e+00 - val_loss: 4234.5517 - val_acc: 0.0000e+00\n",
            "Epoch 701/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3784.0432 - acc: 0.0000e+00 - val_loss: 4286.4894 - val_acc: 0.0000e+00\n",
            "Epoch 702/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3783.3862 - acc: 0.0000e+00 - val_loss: 4263.1608 - val_acc: 0.0000e+00\n",
            "Epoch 703/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3783.8507 - acc: 0.0000e+00 - val_loss: 4186.8241 - val_acc: 0.0000e+00\n",
            "Epoch 704/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3783.1182 - acc: 0.0000e+00 - val_loss: 4129.0164 - val_acc: 0.0000e+00\n",
            "Epoch 705/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3783.1537 - acc: 0.0000e+00 - val_loss: 4178.0110 - val_acc: 0.0000e+00\n",
            "Epoch 706/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3783.1629 - acc: 0.0000e+00 - val_loss: 4145.8578 - val_acc: 0.0000e+00\n",
            "Epoch 707/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3782.9644 - acc: 0.0000e+00 - val_loss: 4087.1776 - val_acc: 0.0000e+00\n",
            "Epoch 708/3000\n",
            "348/348 [==============================] - 0s 244us/sample - loss: 3782.9910 - acc: 0.0000e+00 - val_loss: 4040.2330 - val_acc: 0.0000e+00\n",
            "Epoch 709/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3782.8156 - acc: 0.0000e+00 - val_loss: 4077.5098 - val_acc: 0.0000e+00\n",
            "Epoch 710/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3782.6843 - acc: 0.0000e+00 - val_loss: 4071.3882 - val_acc: 0.0000e+00\n",
            "Epoch 711/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3782.5637 - acc: 0.0000e+00 - val_loss: 4031.3436 - val_acc: 0.0000e+00\n",
            "Epoch 712/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3782.5116 - acc: 0.0000e+00 - val_loss: 4080.8595 - val_acc: 0.0000e+00\n",
            "Epoch 713/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3782.4780 - acc: 0.0000e+00 - val_loss: 4068.3765 - val_acc: 0.0000e+00\n",
            "Epoch 714/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3782.3204 - acc: 0.0000e+00 - val_loss: 4029.9586 - val_acc: 0.0000e+00\n",
            "Epoch 715/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3782.2920 - acc: 0.0000e+00 - val_loss: 3990.8451 - val_acc: 0.0000e+00\n",
            "Epoch 716/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3782.0006 - acc: 0.0000e+00 - val_loss: 4015.5342 - val_acc: 0.0000e+00\n",
            "Epoch 717/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3782.6964 - acc: 0.0000e+00 - val_loss: 3991.5747 - val_acc: 0.0000e+00\n",
            "Epoch 718/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3781.6776 - acc: 0.0000e+00 - val_loss: 3977.7518 - val_acc: 0.0000e+00\n",
            "Epoch 719/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3781.5775 - acc: 0.0000e+00 - val_loss: 3967.5847 - val_acc: 0.0000e+00\n",
            "Epoch 720/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3782.4073 - acc: 0.0000e+00 - val_loss: 3980.4658 - val_acc: 0.0000e+00\n",
            "Epoch 721/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3781.4342 - acc: 0.0000e+00 - val_loss: 3984.3803 - val_acc: 0.0000e+00\n",
            "Epoch 722/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3781.2152 - acc: 0.0000e+00 - val_loss: 3973.0815 - val_acc: 0.0000e+00\n",
            "Epoch 723/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3782.0546 - acc: 0.0000e+00 - val_loss: 3990.3208 - val_acc: 0.0000e+00\n",
            "Epoch 724/3000\n",
            "348/348 [==============================] - 0s 269us/sample - loss: 3781.1425 - acc: 0.0000e+00 - val_loss: 3992.6361 - val_acc: 0.0000e+00\n",
            "Epoch 725/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3780.9834 - acc: 0.0000e+00 - val_loss: 3976.4591 - val_acc: 0.0000e+00\n",
            "Epoch 726/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3781.8028 - acc: 0.0000e+00 - val_loss: 3961.3987 - val_acc: 0.0000e+00\n",
            "Epoch 727/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3780.8595 - acc: 0.0000e+00 - val_loss: 3976.7621 - val_acc: 0.0000e+00\n",
            "Epoch 728/3000\n",
            "348/348 [==============================] - 0s 244us/sample - loss: 3780.6924 - acc: 0.0000e+00 - val_loss: 3977.2674 - val_acc: 0.0000e+00\n",
            "Epoch 729/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3781.1167 - acc: 0.0000e+00 - val_loss: 3966.7418 - val_acc: 0.0000e+00\n",
            "Epoch 730/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3780.7143 - acc: 0.0000e+00 - val_loss: 3960.4473 - val_acc: 0.0000e+00\n",
            "Epoch 731/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3780.8628 - acc: 0.0000e+00 - val_loss: 3973.7825 - val_acc: 0.0000e+00\n",
            "Epoch 732/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3780.5434 - acc: 0.0000e+00 - val_loss: 3973.3834 - val_acc: 0.0000e+00\n",
            "Epoch 733/3000\n",
            "348/348 [==============================] - 0s 256us/sample - loss: 3780.7007 - acc: 0.0000e+00 - val_loss: 3965.8101 - val_acc: 0.0000e+00\n",
            "Epoch 734/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3780.3595 - acc: 0.0000e+00 - val_loss: 3994.3130 - val_acc: 0.0000e+00\n",
            "Epoch 735/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3780.9910 - acc: 0.0000e+00 - val_loss: 3985.2410 - val_acc: 0.0000e+00\n",
            "Epoch 736/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3779.9175 - acc: 0.0000e+00 - val_loss: 3977.1019 - val_acc: 0.0000e+00\n",
            "Epoch 737/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3779.8466 - acc: 0.0000e+00 - val_loss: 3967.8889 - val_acc: 0.0000e+00\n",
            "Epoch 738/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3780.7916 - acc: 0.0000e+00 - val_loss: 3984.3007 - val_acc: 0.0000e+00\n",
            "Epoch 739/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3779.7580 - acc: 0.0000e+00 - val_loss: 3986.3649 - val_acc: 0.0000e+00\n",
            "Epoch 740/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3779.5231 - acc: 0.0000e+00 - val_loss: 3974.3430 - val_acc: 0.0000e+00\n",
            "Epoch 741/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3780.1549 - acc: 0.0000e+00 - val_loss: 3960.5355 - val_acc: 0.0000e+00\n",
            "Epoch 742/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3779.5289 - acc: 0.0000e+00 - val_loss: 3978.1526 - val_acc: 0.0000e+00\n",
            "Epoch 743/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3779.3852 - acc: 0.0000e+00 - val_loss: 3977.6268 - val_acc: 0.0000e+00\n",
            "Epoch 744/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3779.5064 - acc: 0.0000e+00 - val_loss: 3966.3457 - val_acc: 0.0000e+00\n",
            "Epoch 745/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3779.4305 - acc: 0.0000e+00 - val_loss: 3989.5948 - val_acc: 0.0000e+00\n",
            "Epoch 746/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3779.3126 - acc: 0.0000e+00 - val_loss: 3986.2455 - val_acc: 0.0000e+00\n",
            "Epoch 747/3000\n",
            "348/348 [==============================] - 0s 255us/sample - loss: 3779.1763 - acc: 0.0000e+00 - val_loss: 3969.1586 - val_acc: 0.0000e+00\n",
            "Epoch 748/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3779.0389 - acc: 0.0000e+00 - val_loss: 3990.2267 - val_acc: 0.0000e+00\n",
            "Epoch 749/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3778.9583 - acc: 0.0000e+00 - val_loss: 3987.4994 - val_acc: 0.0000e+00\n",
            "Epoch 750/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3778.9280 - acc: 0.0000e+00 - val_loss: 3971.0182 - val_acc: 0.0000e+00\n",
            "Epoch 751/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3778.8549 - acc: 0.0000e+00 - val_loss: 3958.6118 - val_acc: 0.0000e+00\n",
            "Epoch 752/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3778.7162 - acc: 0.0000e+00 - val_loss: 3969.2722 - val_acc: 0.0000e+00\n",
            "Epoch 753/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3778.6098 - acc: 0.0000e+00 - val_loss: 3966.2378 - val_acc: 0.0000e+00\n",
            "Epoch 754/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3778.6300 - acc: 0.0000e+00 - val_loss: 3958.6843 - val_acc: 0.0000e+00\n",
            "Epoch 755/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3778.4936 - acc: 0.0000e+00 - val_loss: 3975.6887 - val_acc: 0.0000e+00\n",
            "Epoch 756/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3778.4076 - acc: 0.0000e+00 - val_loss: 3974.0253 - val_acc: 0.0000e+00\n",
            "Epoch 757/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3778.3068 - acc: 0.0000e+00 - val_loss: 3962.2205 - val_acc: 0.0000e+00\n",
            "Epoch 758/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3778.2006 - acc: 0.0000e+00 - val_loss: 3956.7403 - val_acc: 0.0000e+00\n",
            "Epoch 759/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3778.1600 - acc: 0.0000e+00 - val_loss: 3971.4114 - val_acc: 0.0000e+00\n",
            "Epoch 760/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3778.0304 - acc: 0.0000e+00 - val_loss: 3973.5384 - val_acc: 0.0000e+00\n",
            "Epoch 761/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3777.9230 - acc: 0.0000e+00 - val_loss: 3965.6763 - val_acc: 0.0000e+00\n",
            "Epoch 762/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3777.8770 - acc: 0.0000e+00 - val_loss: 3987.9110 - val_acc: 0.0000e+00\n",
            "Epoch 763/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3777.7976 - acc: 0.0000e+00 - val_loss: 3989.8298 - val_acc: 0.0000e+00\n",
            "Epoch 764/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3777.6628 - acc: 0.0000e+00 - val_loss: 3977.4454 - val_acc: 0.0000e+00\n",
            "Epoch 765/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3777.6160 - acc: 0.0000e+00 - val_loss: 4006.2516 - val_acc: 0.0000e+00\n",
            "Epoch 766/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3777.8830 - acc: 0.0000e+00 - val_loss: 4000.2754 - val_acc: 0.0000e+00\n",
            "Epoch 767/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3777.3019 - acc: 0.0000e+00 - val_loss: 3978.2469 - val_acc: 0.0000e+00\n",
            "Epoch 768/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3777.1730 - acc: 0.0000e+00 - val_loss: 3965.3665 - val_acc: 0.0000e+00\n",
            "Epoch 769/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3777.1170 - acc: 0.0000e+00 - val_loss: 3995.2867 - val_acc: 0.0000e+00\n",
            "Epoch 770/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3777.4592 - acc: 0.0000e+00 - val_loss: 3987.6939 - val_acc: 0.0000e+00\n",
            "Epoch 771/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3776.9194 - acc: 0.0000e+00 - val_loss: 3978.7638 - val_acc: 0.0000e+00\n",
            "Epoch 772/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 3777.1010 - acc: 0.0000e+00 - val_loss: 3965.2606 - val_acc: 0.0000e+00\n",
            "Epoch 773/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3776.6782 - acc: 0.0000e+00 - val_loss: 3992.4973 - val_acc: 0.0000e+00\n",
            "Epoch 774/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3777.0278 - acc: 0.0000e+00 - val_loss: 3988.4748 - val_acc: 0.0000e+00\n",
            "Epoch 775/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3776.4786 - acc: 0.0000e+00 - val_loss: 3977.3356 - val_acc: 0.0000e+00\n",
            "Epoch 776/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3776.8897 - acc: 0.0000e+00 - val_loss: 3964.0744 - val_acc: 0.0000e+00\n",
            "Epoch 777/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3776.2656 - acc: 0.0000e+00 - val_loss: 3992.8861 - val_acc: 0.0000e+00\n",
            "Epoch 778/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3776.6493 - acc: 0.0000e+00 - val_loss: 3983.9043 - val_acc: 0.0000e+00\n",
            "Epoch 779/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 3776.1463 - acc: 0.0000e+00 - val_loss: 3969.6202 - val_acc: 0.0000e+00\n",
            "Epoch 780/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3776.4658 - acc: 0.0000e+00 - val_loss: 3956.8712 - val_acc: 0.0000e+00\n",
            "Epoch 781/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3776.2288 - acc: 0.0000e+00 - val_loss: 3973.7255 - val_acc: 0.0000e+00\n",
            "Epoch 782/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3775.7856 - acc: 0.0000e+00 - val_loss: 3976.8138 - val_acc: 0.0000e+00\n",
            "Epoch 783/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3776.1998 - acc: 0.0000e+00 - val_loss: 3963.2603 - val_acc: 0.0000e+00\n",
            "Epoch 784/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3775.6693 - acc: 0.0000e+00 - val_loss: 3957.2472 - val_acc: 0.0000e+00\n",
            "Epoch 785/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3775.9170 - acc: 0.0000e+00 - val_loss: 3979.9833 - val_acc: 0.0000e+00\n",
            "Epoch 786/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3775.8389 - acc: 0.0000e+00 - val_loss: 3979.0719 - val_acc: 0.0000e+00\n",
            "Epoch 787/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3775.2635 - acc: 0.0000e+00 - val_loss: 3967.1214 - val_acc: 0.0000e+00\n",
            "Epoch 788/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 3775.6912 - acc: 0.0000e+00 - val_loss: 3954.6619 - val_acc: 0.0000e+00\n",
            "Epoch 789/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3775.4651 - acc: 0.0000e+00 - val_loss: 3950.4776 - val_acc: 0.0000e+00\n",
            "Epoch 790/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3774.9891 - acc: 0.0000e+00 - val_loss: 3961.8351 - val_acc: 0.0000e+00\n",
            "Epoch 791/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3775.5064 - acc: 0.0000e+00 - val_loss: 3960.0089 - val_acc: 0.0000e+00\n",
            "Epoch 792/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3775.2037 - acc: 0.0000e+00 - val_loss: 3954.2183 - val_acc: 0.0000e+00\n",
            "Epoch 793/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3774.6478 - acc: 0.0000e+00 - val_loss: 3950.7189 - val_acc: 0.0000e+00\n",
            "Epoch 794/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3775.0703 - acc: 0.0000e+00 - val_loss: 3964.2197 - val_acc: 0.0000e+00\n",
            "Epoch 795/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3774.9031 - acc: 0.0000e+00 - val_loss: 3964.0947 - val_acc: 0.0000e+00\n",
            "Epoch 796/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3774.3454 - acc: 0.0000e+00 - val_loss: 3958.5877 - val_acc: 0.0000e+00\n",
            "Epoch 797/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3774.8008 - acc: 0.0000e+00 - val_loss: 3951.6053 - val_acc: 0.0000e+00\n",
            "Epoch 798/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3774.5704 - acc: 0.0000e+00 - val_loss: 3970.6447 - val_acc: 0.0000e+00\n",
            "Epoch 799/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3774.0358 - acc: 0.0000e+00 - val_loss: 3975.7735 - val_acc: 0.0000e+00\n",
            "Epoch 800/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3774.4908 - acc: 0.0000e+00 - val_loss: 3963.1960 - val_acc: 0.0000e+00\n",
            "Epoch 801/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 3773.9666 - acc: 0.0000e+00 - val_loss: 3954.2961 - val_acc: 0.0000e+00\n",
            "Epoch 802/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3774.1804 - acc: 0.0000e+00 - val_loss: 3973.7986 - val_acc: 0.0000e+00\n",
            "Epoch 803/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3774.1222 - acc: 0.0000e+00 - val_loss: 3972.1776 - val_acc: 0.0000e+00\n",
            "Epoch 804/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3773.5849 - acc: 0.0000e+00 - val_loss: 3964.2477 - val_acc: 0.0000e+00\n",
            "Epoch 805/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3773.7881 - acc: 0.0000e+00 - val_loss: 3954.0939 - val_acc: 0.0000e+00\n",
            "Epoch 806/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3773.6772 - acc: 0.0000e+00 - val_loss: 3949.6450 - val_acc: 0.0000e+00\n",
            "Epoch 807/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3773.2799 - acc: 0.0000e+00 - val_loss: 3967.8013 - val_acc: 0.0000e+00\n",
            "Epoch 808/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3773.6054 - acc: 0.0000e+00 - val_loss: 3967.9323 - val_acc: 0.0000e+00\n",
            "Epoch 809/3000\n",
            "348/348 [==============================] - 0s 254us/sample - loss: 3773.4074 - acc: 0.0000e+00 - val_loss: 3960.6676 - val_acc: 0.0000e+00\n",
            "Epoch 810/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3772.9065 - acc: 0.0000e+00 - val_loss: 3957.1731 - val_acc: 0.0000e+00\n",
            "Epoch 811/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3773.3108 - acc: 0.0000e+00 - val_loss: 3978.2892 - val_acc: 0.0000e+00\n",
            "Epoch 812/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3772.8131 - acc: 0.0000e+00 - val_loss: 3976.4151 - val_acc: 0.0000e+00\n",
            "Epoch 813/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3773.0245 - acc: 0.0000e+00 - val_loss: 3964.0698 - val_acc: 0.0000e+00\n",
            "Epoch 814/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3772.5701 - acc: 0.0000e+00 - val_loss: 3958.6678 - val_acc: 0.0000e+00\n",
            "Epoch 815/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3772.9084 - acc: 0.0000e+00 - val_loss: 3949.1170 - val_acc: 0.0000e+00\n",
            "Epoch 816/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3772.6544 - acc: 0.0000e+00 - val_loss: 3961.6868 - val_acc: 0.0000e+00\n",
            "Epoch 817/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3772.3302 - acc: 0.0000e+00 - val_loss: 3967.1392 - val_acc: 0.0000e+00\n",
            "Epoch 818/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3772.5375 - acc: 0.0000e+00 - val_loss: 3957.4980 - val_acc: 0.0000e+00\n",
            "Epoch 819/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3772.0427 - acc: 0.0000e+00 - val_loss: 3953.6448 - val_acc: 0.0000e+00\n",
            "Epoch 820/3000\n",
            "348/348 [==============================] - 0s 186us/sample - loss: 3772.0090 - acc: 0.0000e+00 - val_loss: 3978.4419 - val_acc: 0.0000e+00\n",
            "Epoch 821/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3772.3407 - acc: 0.0000e+00 - val_loss: 3971.2197 - val_acc: 0.0000e+00\n",
            "Epoch 822/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3771.8305 - acc: 0.0000e+00 - val_loss: 3958.7714 - val_acc: 0.0000e+00\n",
            "Epoch 823/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3771.7127 - acc: 0.0000e+00 - val_loss: 3952.0632 - val_acc: 0.0000e+00\n",
            "Epoch 824/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3772.0653 - acc: 0.0000e+00 - val_loss: 3969.1509 - val_acc: 0.0000e+00\n",
            "Epoch 825/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3771.6159 - acc: 0.0000e+00 - val_loss: 3969.0500 - val_acc: 0.0000e+00\n",
            "Epoch 826/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3771.4547 - acc: 0.0000e+00 - val_loss: 3958.5768 - val_acc: 0.0000e+00\n",
            "Epoch 827/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3771.7002 - acc: 0.0000e+00 - val_loss: 3949.9203 - val_acc: 0.0000e+00\n",
            "Epoch 828/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3771.3276 - acc: 0.0000e+00 - val_loss: 3971.9504 - val_acc: 0.0000e+00\n",
            "Epoch 829/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 3771.2794 - acc: 0.0000e+00 - val_loss: 3976.1799 - val_acc: 0.0000e+00\n",
            "Epoch 830/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3771.4911 - acc: 0.0000e+00 - val_loss: 3962.2818 - val_acc: 0.0000e+00\n",
            "Epoch 831/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3771.0436 - acc: 0.0000e+00 - val_loss: 3952.9981 - val_acc: 0.0000e+00\n",
            "Epoch 832/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3770.9090 - acc: 0.0000e+00 - val_loss: 3973.1318 - val_acc: 0.0000e+00\n",
            "Epoch 833/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3771.2864 - acc: 0.0000e+00 - val_loss: 3967.5905 - val_acc: 0.0000e+00\n",
            "Epoch 834/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3770.8312 - acc: 0.0000e+00 - val_loss: 3958.6167 - val_acc: 0.0000e+00\n",
            "Epoch 835/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3770.9866 - acc: 0.0000e+00 - val_loss: 3948.2333 - val_acc: 0.0000e+00\n",
            "Epoch 836/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3770.6262 - acc: 0.0000e+00 - val_loss: 3969.3073 - val_acc: 0.0000e+00\n",
            "Epoch 837/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3770.8469 - acc: 0.0000e+00 - val_loss: 3971.2227 - val_acc: 0.0000e+00\n",
            "Epoch 838/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3770.4348 - acc: 0.0000e+00 - val_loss: 3961.5286 - val_acc: 0.0000e+00\n",
            "Epoch 839/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3770.6354 - acc: 0.0000e+00 - val_loss: 3950.9609 - val_acc: 0.0000e+00\n",
            "Epoch 840/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3770.1852 - acc: 0.0000e+00 - val_loss: 3974.4894 - val_acc: 0.0000e+00\n",
            "Epoch 841/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3770.4634 - acc: 0.0000e+00 - val_loss: 3968.1586 - val_acc: 0.0000e+00\n",
            "Epoch 842/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3770.0885 - acc: 0.0000e+00 - val_loss: 3958.4911 - val_acc: 0.0000e+00\n",
            "Epoch 843/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3770.3334 - acc: 0.0000e+00 - val_loss: 3950.0150 - val_acc: 0.0000e+00\n",
            "Epoch 844/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3769.8637 - acc: 0.0000e+00 - val_loss: 3973.0170 - val_acc: 0.0000e+00\n",
            "Epoch 845/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3770.1538 - acc: 0.0000e+00 - val_loss: 3972.3343 - val_acc: 0.0000e+00\n",
            "Epoch 846/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3769.9981 - acc: 0.0000e+00 - val_loss: 3959.4403 - val_acc: 0.0000e+00\n",
            "Epoch 847/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3769.5341 - acc: 0.0000e+00 - val_loss: 3951.1842 - val_acc: 0.0000e+00\n",
            "Epoch 848/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3769.8982 - acc: 0.0000e+00 - val_loss: 3973.7500 - val_acc: 0.0000e+00\n",
            "Epoch 849/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3769.4779 - acc: 0.0000e+00 - val_loss: 3979.4775 - val_acc: 0.0000e+00\n",
            "Epoch 850/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3769.7120 - acc: 0.0000e+00 - val_loss: 3968.6458 - val_acc: 0.0000e+00\n",
            "Epoch 851/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3769.2202 - acc: 0.0000e+00 - val_loss: 3963.4993 - val_acc: 0.0000e+00\n",
            "Epoch 852/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3769.5598 - acc: 0.0000e+00 - val_loss: 3988.9188 - val_acc: 0.0000e+00\n",
            "Epoch 853/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3769.1753 - acc: 0.0000e+00 - val_loss: 3984.7178 - val_acc: 0.0000e+00\n",
            "Epoch 854/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3769.3675 - acc: 0.0000e+00 - val_loss: 3964.1458 - val_acc: 0.0000e+00\n",
            "Epoch 855/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3768.9414 - acc: 0.0000e+00 - val_loss: 3951.5109 - val_acc: 0.0000e+00\n",
            "Epoch 856/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3769.1929 - acc: 0.0000e+00 - val_loss: 3943.7079 - val_acc: 0.0000e+00\n",
            "Epoch 857/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3768.7328 - acc: 0.0000e+00 - val_loss: 3958.4286 - val_acc: 0.0000e+00\n",
            "Epoch 858/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3769.0439 - acc: 0.0000e+00 - val_loss: 3958.4540 - val_acc: 0.0000e+00\n",
            "Epoch 859/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3768.6197 - acc: 0.0000e+00 - val_loss: 3954.1556 - val_acc: 0.0000e+00\n",
            "Epoch 860/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3768.8165 - acc: 0.0000e+00 - val_loss: 3945.5104 - val_acc: 0.0000e+00\n",
            "Epoch 861/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3768.4939 - acc: 0.0000e+00 - val_loss: 3959.6957 - val_acc: 0.0000e+00\n",
            "Epoch 862/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3768.3885 - acc: 0.0000e+00 - val_loss: 3959.8658 - val_acc: 0.0000e+00\n",
            "Epoch 863/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3768.3547 - acc: 0.0000e+00 - val_loss: 3950.6952 - val_acc: 0.0000e+00\n",
            "Epoch 864/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3768.2710 - acc: 0.0000e+00 - val_loss: 3945.1319 - val_acc: 0.0000e+00\n",
            "Epoch 865/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3768.4169 - acc: 0.0000e+00 - val_loss: 3957.6530 - val_acc: 0.0000e+00\n",
            "Epoch 866/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3767.9936 - acc: 0.0000e+00 - val_loss: 3955.5941 - val_acc: 0.0000e+00\n",
            "Epoch 867/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3768.1993 - acc: 0.0000e+00 - val_loss: 3947.2679 - val_acc: 0.0000e+00\n",
            "Epoch 868/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3767.8443 - acc: 0.0000e+00 - val_loss: 3943.9757 - val_acc: 0.0000e+00\n",
            "Epoch 869/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3768.0642 - acc: 0.0000e+00 - val_loss: 3959.9894 - val_acc: 0.0000e+00\n",
            "Epoch 870/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3767.7080 - acc: 0.0000e+00 - val_loss: 3962.6145 - val_acc: 0.0000e+00\n",
            "Epoch 871/3000\n",
            "348/348 [==============================] - 0s 284us/sample - loss: 3767.7398 - acc: 0.0000e+00 - val_loss: 3953.0708 - val_acc: 0.0000e+00\n",
            "Epoch 872/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3767.4879 - acc: 0.0000e+00 - val_loss: 3948.8159 - val_acc: 0.0000e+00\n",
            "Epoch 873/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3767.7043 - acc: 0.0000e+00 - val_loss: 3970.9087 - val_acc: 0.0000e+00\n",
            "Epoch 874/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3767.6103 - acc: 0.0000e+00 - val_loss: 3973.2668 - val_acc: 0.0000e+00\n",
            "Epoch 875/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3767.1522 - acc: 0.0000e+00 - val_loss: 3962.7614 - val_acc: 0.0000e+00\n",
            "Epoch 876/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3767.4569 - acc: 0.0000e+00 - val_loss: 3949.3996 - val_acc: 0.0000e+00\n",
            "Epoch 877/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3767.2763 - acc: 0.0000e+00 - val_loss: 3943.1279 - val_acc: 0.0000e+00\n",
            "Epoch 878/3000\n",
            "348/348 [==============================] - 0s 241us/sample - loss: 3767.1610 - acc: 0.0000e+00 - val_loss: 3961.3160 - val_acc: 0.0000e+00\n",
            "Epoch 879/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 3767.1613 - acc: 0.0000e+00 - val_loss: 3960.5075 - val_acc: 0.0000e+00\n",
            "Epoch 880/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 3766.7405 - acc: 0.0000e+00 - val_loss: 3954.6865 - val_acc: 0.0000e+00\n",
            "Epoch 881/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3767.0783 - acc: 0.0000e+00 - val_loss: 3944.7640 - val_acc: 0.0000e+00\n",
            "Epoch 882/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3766.9412 - acc: 0.0000e+00 - val_loss: 3964.0837 - val_acc: 0.0000e+00\n",
            "Epoch 883/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3766.5620 - acc: 0.0000e+00 - val_loss: 3972.0866 - val_acc: 0.0000e+00\n",
            "Epoch 884/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3766.7850 - acc: 0.0000e+00 - val_loss: 3964.3392 - val_acc: 0.0000e+00\n",
            "Epoch 885/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3766.7036 - acc: 0.0000e+00 - val_loss: 3953.2186 - val_acc: 0.0000e+00\n",
            "Epoch 886/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3766.4828 - acc: 0.0000e+00 - val_loss: 3973.5052 - val_acc: 0.0000e+00\n",
            "Epoch 887/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3766.4741 - acc: 0.0000e+00 - val_loss: 3967.1180 - val_acc: 0.0000e+00\n",
            "Epoch 888/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3766.2042 - acc: 0.0000e+00 - val_loss: 3953.1540 - val_acc: 0.0000e+00\n",
            "Epoch 889/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3766.3500 - acc: 0.0000e+00 - val_loss: 3942.1816 - val_acc: 0.0000e+00\n",
            "Epoch 890/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3766.2129 - acc: 0.0000e+00 - val_loss: 3956.4820 - val_acc: 0.0000e+00\n",
            "Epoch 891/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3766.1199 - acc: 0.0000e+00 - val_loss: 3952.5252 - val_acc: 0.0000e+00\n",
            "Epoch 892/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3766.1227 - acc: 0.0000e+00 - val_loss: 3945.6350 - val_acc: 0.0000e+00\n",
            "Epoch 893/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3766.0117 - acc: 0.0000e+00 - val_loss: 3940.1439 - val_acc: 0.0000e+00\n",
            "Epoch 894/3000\n",
            "348/348 [==============================] - 0s 248us/sample - loss: 3765.9380 - acc: 0.0000e+00 - val_loss: 3938.1824 - val_acc: 0.0000e+00\n",
            "Epoch 895/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3765.8294 - acc: 0.0000e+00 - val_loss: 3950.6011 - val_acc: 0.0000e+00\n",
            "Epoch 896/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3765.8114 - acc: 0.0000e+00 - val_loss: 3952.6670 - val_acc: 0.0000e+00\n",
            "Epoch 897/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3765.7449 - acc: 0.0000e+00 - val_loss: 3944.1223 - val_acc: 0.0000e+00\n",
            "Epoch 898/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3765.6446 - acc: 0.0000e+00 - val_loss: 3939.9718 - val_acc: 0.0000e+00\n",
            "Epoch 899/3000\n",
            "348/348 [==============================] - 0s 238us/sample - loss: 3765.5832 - acc: 0.0000e+00 - val_loss: 3952.1148 - val_acc: 0.0000e+00\n",
            "Epoch 900/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3765.5026 - acc: 0.0000e+00 - val_loss: 3951.6711 - val_acc: 0.0000e+00\n",
            "Epoch 901/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3765.5009 - acc: 0.0000e+00 - val_loss: 3945.1562 - val_acc: 0.0000e+00\n",
            "Epoch 902/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3765.3537 - acc: 0.0000e+00 - val_loss: 3940.4880 - val_acc: 0.0000e+00\n",
            "Epoch 903/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3765.2917 - acc: 0.0000e+00 - val_loss: 3938.1753 - val_acc: 0.0000e+00\n",
            "Epoch 904/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3765.2152 - acc: 0.0000e+00 - val_loss: 3949.0702 - val_acc: 0.0000e+00\n",
            "Epoch 905/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3765.1252 - acc: 0.0000e+00 - val_loss: 3948.4680 - val_acc: 0.0000e+00\n",
            "Epoch 906/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3765.1400 - acc: 0.0000e+00 - val_loss: 3943.0285 - val_acc: 0.0000e+00\n",
            "Epoch 907/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3764.9280 - acc: 0.0000e+00 - val_loss: 3938.9561 - val_acc: 0.0000e+00\n",
            "Epoch 908/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3764.9648 - acc: 0.0000e+00 - val_loss: 3952.2864 - val_acc: 0.0000e+00\n",
            "Epoch 909/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3764.9254 - acc: 0.0000e+00 - val_loss: 3953.0684 - val_acc: 0.0000e+00\n",
            "Epoch 910/3000\n",
            "348/348 [==============================] - 0s 190us/sample - loss: 3764.8215 - acc: 0.0000e+00 - val_loss: 3943.5588 - val_acc: 0.0000e+00\n",
            "Epoch 911/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3764.7940 - acc: 0.0000e+00 - val_loss: 3937.8072 - val_acc: 0.0000e+00\n",
            "Epoch 912/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3764.6003 - acc: 0.0000e+00 - val_loss: 3950.6733 - val_acc: 0.0000e+00\n",
            "Epoch 913/3000\n",
            "348/348 [==============================] - 0s 262us/sample - loss: 3764.6444 - acc: 0.0000e+00 - val_loss: 3952.9849 - val_acc: 0.0000e+00\n",
            "Epoch 914/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3764.5555 - acc: 0.0000e+00 - val_loss: 3947.2660 - val_acc: 0.0000e+00\n",
            "Epoch 915/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3764.4051 - acc: 0.0000e+00 - val_loss: 3942.8166 - val_acc: 0.0000e+00\n",
            "Epoch 916/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3764.2614 - acc: 0.0000e+00 - val_loss: 3966.9859 - val_acc: 0.0000e+00\n",
            "Epoch 917/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3764.2564 - acc: 0.0000e+00 - val_loss: 3967.5589 - val_acc: 0.0000e+00\n",
            "Epoch 918/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3764.2284 - acc: 0.0000e+00 - val_loss: 3956.5933 - val_acc: 0.0000e+00\n",
            "Epoch 919/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3764.0893 - acc: 0.0000e+00 - val_loss: 3948.1499 - val_acc: 0.0000e+00\n",
            "Epoch 920/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3764.1297 - acc: 0.0000e+00 - val_loss: 3970.7727 - val_acc: 0.0000e+00\n",
            "Epoch 921/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3764.0577 - acc: 0.0000e+00 - val_loss: 3981.5554 - val_acc: 0.0000e+00\n",
            "Epoch 922/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3763.8477 - acc: 0.0000e+00 - val_loss: 3984.8316 - val_acc: 0.0000e+00\n",
            "Epoch 923/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3763.8240 - acc: 0.0000e+00 - val_loss: 3983.4495 - val_acc: 0.0000e+00\n",
            "Epoch 924/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3763.9889 - acc: 0.0000e+00 - val_loss: 4048.8490 - val_acc: 0.0000e+00\n",
            "Epoch 925/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3763.3011 - acc: 0.0000e+00 - val_loss: 4082.6520 - val_acc: 0.0000e+00\n",
            "Epoch 926/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3763.8245 - acc: 0.0000e+00 - val_loss: 4077.7610 - val_acc: 0.0000e+00\n",
            "Epoch 927/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3763.4645 - acc: 0.0000e+00 - val_loss: 4078.2864 - val_acc: 0.0000e+00\n",
            "Epoch 928/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3763.2309 - acc: 0.0000e+00 - val_loss: 4172.8895 - val_acc: 0.0000e+00\n",
            "Epoch 929/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3763.3454 - acc: 0.0000e+00 - val_loss: 4192.8673 - val_acc: 0.0000e+00\n",
            "Epoch 930/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3763.1165 - acc: 0.0000e+00 - val_loss: 4184.3175 - val_acc: 0.0000e+00\n",
            "Epoch 931/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3763.0878 - acc: 0.0000e+00 - val_loss: 4163.8911 - val_acc: 0.0000e+00\n",
            "Epoch 932/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3762.8933 - acc: 0.0000e+00 - val_loss: 4275.3383 - val_acc: 0.0000e+00\n",
            "Epoch 933/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3762.8664 - acc: 0.0000e+00 - val_loss: 4292.8626 - val_acc: 0.0000e+00\n",
            "Epoch 934/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3762.7120 - acc: 0.0000e+00 - val_loss: 4267.9056 - val_acc: 0.0000e+00\n",
            "Epoch 935/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3762.5901 - acc: 0.0000e+00 - val_loss: 4236.5195 - val_acc: 0.0000e+00\n",
            "Epoch 936/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3762.4969 - acc: 0.0000e+00 - val_loss: 4346.0085 - val_acc: 0.0000e+00\n",
            "Epoch 937/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3762.4492 - acc: 0.0000e+00 - val_loss: 4357.3698 - val_acc: 0.0000e+00\n",
            "Epoch 938/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 3762.3931 - acc: 0.0000e+00 - val_loss: 4325.5527 - val_acc: 0.0000e+00\n",
            "Epoch 939/3000\n",
            "348/348 [==============================] - 0s 242us/sample - loss: 3762.2113 - acc: 0.0000e+00 - val_loss: 4283.4526 - val_acc: 0.0000e+00\n",
            "Epoch 940/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3762.1713 - acc: 0.0000e+00 - val_loss: 4395.7927 - val_acc: 0.0000e+00\n",
            "Epoch 941/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3762.1436 - acc: 0.0000e+00 - val_loss: 4409.9128 - val_acc: 0.0000e+00\n",
            "Epoch 942/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3761.9698 - acc: 0.0000e+00 - val_loss: 4378.0895 - val_acc: 0.0000e+00\n",
            "Epoch 943/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3761.9236 - acc: 0.0000e+00 - val_loss: 4348.5842 - val_acc: 0.0000e+00\n",
            "Epoch 944/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3761.7835 - acc: 0.0000e+00 - val_loss: 4325.6996 - val_acc: 0.0000e+00\n",
            "Epoch 945/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3761.8120 - acc: 0.0000e+00 - val_loss: 4451.8158 - val_acc: 0.0000e+00\n",
            "Epoch 946/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3761.6412 - acc: 0.0000e+00 - val_loss: 4468.0521 - val_acc: 0.0000e+00\n",
            "Epoch 947/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3761.5259 - acc: 0.0000e+00 - val_loss: 4435.8990 - val_acc: 0.0000e+00\n",
            "Epoch 948/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 3761.5306 - acc: 0.0000e+00 - val_loss: 4406.5548 - val_acc: 0.0000e+00\n",
            "Epoch 949/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3761.3710 - acc: 0.0000e+00 - val_loss: 4570.5328 - val_acc: 0.0000e+00\n",
            "Epoch 950/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3761.2635 - acc: 0.0000e+00 - val_loss: 4627.2509 - val_acc: 0.0000e+00\n",
            "Epoch 951/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3761.1942 - acc: 0.0000e+00 - val_loss: 4620.2328 - val_acc: 0.0000e+00\n",
            "Epoch 952/3000\n",
            "348/348 [==============================] - 0s 276us/sample - loss: 3761.0238 - acc: 0.0000e+00 - val_loss: 4617.1736 - val_acc: 0.0000e+00\n",
            "Epoch 953/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3761.0114 - acc: 0.0000e+00 - val_loss: 4833.0707 - val_acc: 0.0000e+00\n",
            "Epoch 954/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3760.8265 - acc: 0.0000e+00 - val_loss: 4922.4137 - val_acc: 0.0000e+00\n",
            "Epoch 955/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3760.7028 - acc: 0.0000e+00 - val_loss: 4846.7417 - val_acc: 0.0000e+00\n",
            "Epoch 956/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 3760.6867 - acc: 0.0000e+00 - val_loss: 4666.4303 - val_acc: 0.0000e+00\n",
            "Epoch 957/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3760.5391 - acc: 0.0000e+00 - val_loss: 4707.6838 - val_acc: 0.0000e+00\n",
            "Epoch 958/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3760.4082 - acc: 0.0000e+00 - val_loss: 4615.0801 - val_acc: 0.0000e+00\n",
            "Epoch 959/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3760.3735 - acc: 0.0000e+00 - val_loss: 4465.7333 - val_acc: 0.0000e+00\n",
            "Epoch 960/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3760.2376 - acc: 0.0000e+00 - val_loss: 4330.6035 - val_acc: 0.0000e+00\n",
            "Epoch 961/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3760.1153 - acc: 0.0000e+00 - val_loss: 4367.9155 - val_acc: 0.0000e+00\n",
            "Epoch 962/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3760.0324 - acc: 0.0000e+00 - val_loss: 4310.9155 - val_acc: 0.0000e+00\n",
            "Epoch 963/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3759.9701 - acc: 0.0000e+00 - val_loss: 4230.0541 - val_acc: 0.0000e+00\n",
            "Epoch 964/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3759.5238 - acc: 0.0000e+00 - val_loss: 4157.0878 - val_acc: 0.0000e+00\n",
            "Epoch 965/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3760.1383 - acc: 0.0000e+00 - val_loss: 4178.0039 - val_acc: 0.0000e+00\n",
            "Epoch 966/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3759.7616 - acc: 0.0000e+00 - val_loss: 4146.4419 - val_acc: 0.0000e+00\n",
            "Epoch 967/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3759.5479 - acc: 0.0000e+00 - val_loss: 4102.5876 - val_acc: 0.0000e+00\n",
            "Epoch 968/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3759.5725 - acc: 0.0000e+00 - val_loss: 4054.5563 - val_acc: 0.0000e+00\n",
            "Epoch 969/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3759.0944 - acc: 0.0000e+00 - val_loss: 4101.4311 - val_acc: 0.0000e+00\n",
            "Epoch 970/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3759.6733 - acc: 0.0000e+00 - val_loss: 4072.1038 - val_acc: 0.0000e+00\n",
            "Epoch 971/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3759.4702 - acc: 0.0000e+00 - val_loss: 4029.5922 - val_acc: 0.0000e+00\n",
            "Epoch 972/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3759.1013 - acc: 0.0000e+00 - val_loss: 4069.9346 - val_acc: 0.0000e+00\n",
            "Epoch 973/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3759.0933 - acc: 0.0000e+00 - val_loss: 4055.8088 - val_acc: 0.0000e+00\n",
            "Epoch 974/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3759.0282 - acc: 0.0000e+00 - val_loss: 4022.0429 - val_acc: 0.0000e+00\n",
            "Epoch 975/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3758.8851 - acc: 0.0000e+00 - val_loss: 3991.2550 - val_acc: 0.0000e+00\n",
            "Epoch 976/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3758.8713 - acc: 0.0000e+00 - val_loss: 4038.2651 - val_acc: 0.0000e+00\n",
            "Epoch 977/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3758.7582 - acc: 0.0000e+00 - val_loss: 4031.8663 - val_acc: 0.0000e+00\n",
            "Epoch 978/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3758.6453 - acc: 0.0000e+00 - val_loss: 4006.5387 - val_acc: 0.0000e+00\n",
            "Epoch 979/3000\n",
            "348/348 [==============================] - 0s 234us/sample - loss: 3758.5501 - acc: 0.0000e+00 - val_loss: 3987.3321 - val_acc: 0.0000e+00\n",
            "Epoch 980/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3758.5235 - acc: 0.0000e+00 - val_loss: 3967.0523 - val_acc: 0.0000e+00\n",
            "Epoch 981/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3758.4328 - acc: 0.0000e+00 - val_loss: 3998.4686 - val_acc: 0.0000e+00\n",
            "Epoch 982/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3758.2867 - acc: 0.0000e+00 - val_loss: 3995.2946 - val_acc: 0.0000e+00\n",
            "Epoch 983/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3758.1861 - acc: 0.0000e+00 - val_loss: 3974.2379 - val_acc: 0.0000e+00\n",
            "Epoch 984/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3758.1455 - acc: 0.0000e+00 - val_loss: 3953.3373 - val_acc: 0.0000e+00\n",
            "Epoch 985/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3758.1236 - acc: 0.0000e+00 - val_loss: 3979.7424 - val_acc: 0.0000e+00\n",
            "Epoch 986/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3757.9906 - acc: 0.0000e+00 - val_loss: 3977.4654 - val_acc: 0.0000e+00\n",
            "Epoch 987/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3757.8236 - acc: 0.0000e+00 - val_loss: 3966.4834 - val_acc: 0.0000e+00\n",
            "Epoch 988/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3757.8314 - acc: 0.0000e+00 - val_loss: 3949.2735 - val_acc: 0.0000e+00\n",
            "Epoch 989/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3757.7399 - acc: 0.0000e+00 - val_loss: 3942.1637 - val_acc: 0.0000e+00\n",
            "Epoch 990/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3757.6688 - acc: 0.0000e+00 - val_loss: 3976.3961 - val_acc: 0.0000e+00\n",
            "Epoch 991/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3757.6094 - acc: 0.0000e+00 - val_loss: 3974.4884 - val_acc: 0.0000e+00\n",
            "Epoch 992/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3757.4436 - acc: 0.0000e+00 - val_loss: 3958.8066 - val_acc: 0.0000e+00\n",
            "Epoch 993/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3757.4716 - acc: 0.0000e+00 - val_loss: 3940.6644 - val_acc: 0.0000e+00\n",
            "Epoch 994/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3757.2448 - acc: 0.0000e+00 - val_loss: 3934.3990 - val_acc: 0.0000e+00\n",
            "Epoch 995/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3757.2412 - acc: 0.0000e+00 - val_loss: 3957.0097 - val_acc: 0.0000e+00\n",
            "Epoch 996/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3757.2157 - acc: 0.0000e+00 - val_loss: 3954.1656 - val_acc: 0.0000e+00\n",
            "Epoch 997/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3757.0559 - acc: 0.0000e+00 - val_loss: 3940.9522 - val_acc: 0.0000e+00\n",
            "Epoch 998/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3756.9648 - acc: 0.0000e+00 - val_loss: 3931.4848 - val_acc: 0.0000e+00\n",
            "Epoch 999/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3756.8381 - acc: 0.0000e+00 - val_loss: 3928.5634 - val_acc: 0.0000e+00\n",
            "Epoch 1000/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3756.8066 - acc: 0.0000e+00 - val_loss: 3947.1609 - val_acc: 0.0000e+00\n",
            "Epoch 1001/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3756.7353 - acc: 0.0000e+00 - val_loss: 3946.0036 - val_acc: 0.0000e+00\n",
            "Epoch 1002/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3756.5613 - acc: 0.0000e+00 - val_loss: 3936.6144 - val_acc: 0.0000e+00\n",
            "Epoch 1003/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3756.5163 - acc: 0.0000e+00 - val_loss: 3930.1346 - val_acc: 0.0000e+00\n",
            "Epoch 1004/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3756.3455 - acc: 0.0000e+00 - val_loss: 3951.1835 - val_acc: 0.0000e+00\n",
            "Epoch 1005/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3756.3747 - acc: 0.0000e+00 - val_loss: 3948.9692 - val_acc: 0.0000e+00\n",
            "Epoch 1006/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3756.2947 - acc: 0.0000e+00 - val_loss: 3942.2690 - val_acc: 0.0000e+00\n",
            "Epoch 1007/3000\n",
            "348/348 [==============================] - 0s 249us/sample - loss: 3756.1665 - acc: 0.0000e+00 - val_loss: 3936.6600 - val_acc: 0.0000e+00\n",
            "Epoch 1008/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3756.0626 - acc: 0.0000e+00 - val_loss: 3963.5252 - val_acc: 0.0000e+00\n",
            "Epoch 1009/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3756.0570 - acc: 0.0000e+00 - val_loss: 3967.5730 - val_acc: 0.0000e+00\n",
            "Epoch 1010/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3755.8641 - acc: 0.0000e+00 - val_loss: 3956.3301 - val_acc: 0.0000e+00\n",
            "Epoch 1011/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3755.8597 - acc: 0.0000e+00 - val_loss: 3942.1238 - val_acc: 0.0000e+00\n",
            "Epoch 1012/3000\n",
            "348/348 [==============================] - 0s 184us/sample - loss: 3755.8033 - acc: 0.0000e+00 - val_loss: 3930.7935 - val_acc: 0.0000e+00\n",
            "Epoch 1013/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3755.6928 - acc: 0.0000e+00 - val_loss: 3953.8625 - val_acc: 0.0000e+00\n",
            "Epoch 1014/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3755.6271 - acc: 0.0000e+00 - val_loss: 3956.5902 - val_acc: 0.0000e+00\n",
            "Epoch 1015/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3755.5871 - acc: 0.0000e+00 - val_loss: 3943.8277 - val_acc: 0.0000e+00\n",
            "Epoch 1016/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3755.3970 - acc: 0.0000e+00 - val_loss: 3937.6374 - val_acc: 0.0000e+00\n",
            "Epoch 1017/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3755.3897 - acc: 0.0000e+00 - val_loss: 3931.5787 - val_acc: 0.0000e+00\n",
            "Epoch 1018/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3755.3079 - acc: 0.0000e+00 - val_loss: 3952.2633 - val_acc: 0.0000e+00\n",
            "Epoch 1019/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3755.2102 - acc: 0.0000e+00 - val_loss: 3957.3844 - val_acc: 0.0000e+00\n",
            "Epoch 1020/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3755.0955 - acc: 0.0000e+00 - val_loss: 3948.3732 - val_acc: 0.0000e+00\n",
            "Epoch 1021/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3755.0530 - acc: 0.0000e+00 - val_loss: 3936.0916 - val_acc: 0.0000e+00\n",
            "Epoch 1022/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3754.9098 - acc: 0.0000e+00 - val_loss: 3965.1224 - val_acc: 0.0000e+00\n",
            "Epoch 1023/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3754.9064 - acc: 0.0000e+00 - val_loss: 3966.3361 - val_acc: 0.0000e+00\n",
            "Epoch 1024/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3754.8228 - acc: 0.0000e+00 - val_loss: 3947.9507 - val_acc: 0.0000e+00\n",
            "Epoch 1025/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3754.6056 - acc: 0.0000e+00 - val_loss: 3938.8496 - val_acc: 0.0000e+00\n",
            "Epoch 1026/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3754.5688 - acc: 0.0000e+00 - val_loss: 3930.8651 - val_acc: 0.0000e+00\n",
            "Epoch 1027/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3754.6529 - acc: 0.0000e+00 - val_loss: 3949.6594 - val_acc: 0.0000e+00\n",
            "Epoch 1028/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3754.3059 - acc: 0.0000e+00 - val_loss: 3961.0737 - val_acc: 0.0000e+00\n",
            "Epoch 1029/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3754.3916 - acc: 0.0000e+00 - val_loss: 3955.8542 - val_acc: 0.0000e+00\n",
            "Epoch 1030/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3754.2210 - acc: 0.0000e+00 - val_loss: 3951.2438 - val_acc: 0.0000e+00\n",
            "Epoch 1031/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3754.1846 - acc: 0.0000e+00 - val_loss: 3943.9100 - val_acc: 0.0000e+00\n",
            "Epoch 1032/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3754.1334 - acc: 0.0000e+00 - val_loss: 3991.9630 - val_acc: 0.0000e+00\n",
            "Epoch 1033/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 3753.9373 - acc: 0.0000e+00 - val_loss: 4018.6986 - val_acc: 0.0000e+00\n",
            "Epoch 1034/3000\n",
            "348/348 [==============================] - 0s 260us/sample - loss: 3753.9445 - acc: 0.0000e+00 - val_loss: 4012.6673 - val_acc: 0.0000e+00\n",
            "Epoch 1035/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3753.7584 - acc: 0.0000e+00 - val_loss: 4015.3190 - val_acc: 0.0000e+00\n",
            "Epoch 1036/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3753.7940 - acc: 0.0000e+00 - val_loss: 4008.2335 - val_acc: 0.0000e+00\n",
            "Epoch 1037/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3753.5493 - acc: 0.0000e+00 - val_loss: 4083.9585 - val_acc: 0.0000e+00\n",
            "Epoch 1038/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3753.6334 - acc: 0.0000e+00 - val_loss: 4104.0672 - val_acc: 0.0000e+00\n",
            "Epoch 1039/3000\n",
            "348/348 [==============================] - 0s 253us/sample - loss: 3753.4647 - acc: 0.0000e+00 - val_loss: 4098.8194 - val_acc: 0.0000e+00\n",
            "Epoch 1040/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3753.4608 - acc: 0.0000e+00 - val_loss: 4069.4828 - val_acc: 0.0000e+00\n",
            "Epoch 1041/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3753.2165 - acc: 0.0000e+00 - val_loss: 4169.1732 - val_acc: 0.0000e+00\n",
            "Epoch 1042/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3753.3019 - acc: 0.0000e+00 - val_loss: 4178.5252 - val_acc: 0.0000e+00\n",
            "Epoch 1043/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 3753.0755 - acc: 0.0000e+00 - val_loss: 4154.8971 - val_acc: 0.0000e+00\n",
            "Epoch 1044/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3752.8670 - acc: 0.0000e+00 - val_loss: 4145.2843 - val_acc: 0.0000e+00\n",
            "Epoch 1045/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3753.0039 - acc: 0.0000e+00 - val_loss: 4117.0701 - val_acc: 0.0000e+00\n",
            "Epoch 1046/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3752.7875 - acc: 0.0000e+00 - val_loss: 4096.6560 - val_acc: 0.0000e+00\n",
            "Epoch 1047/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3752.8451 - acc: 0.0000e+00 - val_loss: 4187.8173 - val_acc: 0.0000e+00\n",
            "Epoch 1048/3000\n",
            "348/348 [==============================] - 0s 247us/sample - loss: 3752.6120 - acc: 0.0000e+00 - val_loss: 4220.0284 - val_acc: 0.0000e+00\n",
            "Epoch 1049/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3752.6056 - acc: 0.0000e+00 - val_loss: 4185.2806 - val_acc: 0.0000e+00\n",
            "Epoch 1050/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3752.4251 - acc: 0.0000e+00 - val_loss: 4176.3927 - val_acc: 0.0000e+00\n",
            "Epoch 1051/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3752.4882 - acc: 0.0000e+00 - val_loss: 4148.7787 - val_acc: 0.0000e+00\n",
            "Epoch 1052/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3752.2305 - acc: 0.0000e+00 - val_loss: 4255.2264 - val_acc: 0.0000e+00\n",
            "Epoch 1053/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3752.2732 - acc: 0.0000e+00 - val_loss: 4277.4443 - val_acc: 0.0000e+00\n",
            "Epoch 1054/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3752.2732 - acc: 0.0000e+00 - val_loss: 4241.5365 - val_acc: 0.0000e+00\n",
            "Epoch 1055/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3752.0804 - acc: 0.0000e+00 - val_loss: 4211.6710 - val_acc: 0.0000e+00\n",
            "Epoch 1056/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3751.9085 - acc: 0.0000e+00 - val_loss: 4185.3026 - val_acc: 0.0000e+00\n",
            "Epoch 1057/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3751.9485 - acc: 0.0000e+00 - val_loss: 4272.3951 - val_acc: 0.0000e+00\n",
            "Epoch 1058/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3751.7327 - acc: 0.0000e+00 - val_loss: 4311.5738 - val_acc: 0.0000e+00\n",
            "Epoch 1059/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3751.7979 - acc: 0.0000e+00 - val_loss: 4280.9134 - val_acc: 0.0000e+00\n",
            "Epoch 1060/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3751.6048 - acc: 0.0000e+00 - val_loss: 4268.3802 - val_acc: 0.0000e+00\n",
            "Epoch 1061/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3751.6380 - acc: 0.0000e+00 - val_loss: 4234.0724 - val_acc: 0.0000e+00\n",
            "Epoch 1062/3000\n",
            "348/348 [==============================] - 0s 245us/sample - loss: 3751.4758 - acc: 0.0000e+00 - val_loss: 4358.3762 - val_acc: 0.0000e+00\n",
            "Epoch 1063/3000\n",
            "348/348 [==============================] - 0s 239us/sample - loss: 3751.5372 - acc: 0.0000e+00 - val_loss: 4380.9816 - val_acc: 0.0000e+00\n",
            "Epoch 1064/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3751.2984 - acc: 0.0000e+00 - val_loss: 4373.9576 - val_acc: 0.0000e+00\n",
            "Epoch 1065/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3751.3584 - acc: 0.0000e+00 - val_loss: 4320.0515 - val_acc: 0.0000e+00\n",
            "Epoch 1066/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3751.1381 - acc: 0.0000e+00 - val_loss: 4312.9678 - val_acc: 0.0000e+00\n",
            "Epoch 1067/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3751.2088 - acc: 0.0000e+00 - val_loss: 4479.8842 - val_acc: 0.0000e+00\n",
            "Epoch 1068/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3750.9126 - acc: 0.0000e+00 - val_loss: 4553.7300 - val_acc: 0.0000e+00\n",
            "Epoch 1069/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3751.0058 - acc: 0.0000e+00 - val_loss: 4504.7430 - val_acc: 0.0000e+00\n",
            "Epoch 1070/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3750.7823 - acc: 0.0000e+00 - val_loss: 4482.3329 - val_acc: 0.0000e+00\n",
            "Epoch 1071/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3750.7139 - acc: 0.0000e+00 - val_loss: 4492.1856 - val_acc: 0.0000e+00\n",
            "Epoch 1072/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3750.8396 - acc: 0.0000e+00 - val_loss: 4467.5838 - val_acc: 0.0000e+00\n",
            "Epoch 1073/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3750.6397 - acc: 0.0000e+00 - val_loss: 4597.4247 - val_acc: 0.0000e+00\n",
            "Epoch 1074/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3750.4983 - acc: 0.0000e+00 - val_loss: 4516.5270 - val_acc: 0.0000e+00\n",
            "Epoch 1075/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3750.5093 - acc: 0.0000e+00 - val_loss: 4354.6413 - val_acc: 0.0000e+00\n",
            "Epoch 1076/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 3750.3736 - acc: 0.0000e+00 - val_loss: 4236.7988 - val_acc: 0.0000e+00\n",
            "Epoch 1077/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3750.3397 - acc: 0.0000e+00 - val_loss: 4168.7267 - val_acc: 0.0000e+00\n",
            "Epoch 1078/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3750.3305 - acc: 0.0000e+00 - val_loss: 4208.8071 - val_acc: 0.0000e+00\n",
            "Epoch 1079/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3750.1364 - acc: 0.0000e+00 - val_loss: 4204.2736 - val_acc: 0.0000e+00\n",
            "Epoch 1080/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 3750.0526 - acc: 0.0000e+00 - val_loss: 4158.0954 - val_acc: 0.0000e+00\n",
            "Epoch 1081/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3750.1075 - acc: 0.0000e+00 - val_loss: 4065.3795 - val_acc: 0.0000e+00\n",
            "Epoch 1082/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3749.8608 - acc: 0.0000e+00 - val_loss: 4129.3229 - val_acc: 0.0000e+00\n",
            "Epoch 1083/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3749.6817 - acc: 0.0000e+00 - val_loss: 4129.6166 - val_acc: 0.0000e+00\n",
            "Epoch 1084/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3749.9450 - acc: 0.0000e+00 - val_loss: 4049.6924 - val_acc: 0.0000e+00\n",
            "Epoch 1085/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3749.6793 - acc: 0.0000e+00 - val_loss: 4020.0439 - val_acc: 0.0000e+00\n",
            "Epoch 1086/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3749.6945 - acc: 0.0000e+00 - val_loss: 3984.3184 - val_acc: 0.0000e+00\n",
            "Epoch 1087/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3749.4734 - acc: 0.0000e+00 - val_loss: 4023.1455 - val_acc: 0.0000e+00\n",
            "Epoch 1088/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3749.5750 - acc: 0.0000e+00 - val_loss: 4014.1426 - val_acc: 0.0000e+00\n",
            "Epoch 1089/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3749.3242 - acc: 0.0000e+00 - val_loss: 3988.7855 - val_acc: 0.0000e+00\n",
            "Epoch 1090/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3749.3799 - acc: 0.0000e+00 - val_loss: 3966.1095 - val_acc: 0.0000e+00\n",
            "Epoch 1091/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3749.1887 - acc: 0.0000e+00 - val_loss: 3952.2507 - val_acc: 0.0000e+00\n",
            "Epoch 1092/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3749.1897 - acc: 0.0000e+00 - val_loss: 3983.9907 - val_acc: 0.0000e+00\n",
            "Epoch 1093/3000\n",
            "348/348 [==============================] - 0s 211us/sample - loss: 3749.1352 - acc: 0.0000e+00 - val_loss: 3986.7257 - val_acc: 0.0000e+00\n",
            "Epoch 1094/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3749.1142 - acc: 0.0000e+00 - val_loss: 3960.3782 - val_acc: 0.0000e+00\n",
            "Epoch 1095/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3748.8477 - acc: 0.0000e+00 - val_loss: 3943.8273 - val_acc: 0.0000e+00\n",
            "Epoch 1096/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3748.9768 - acc: 0.0000e+00 - val_loss: 3930.2485 - val_acc: 0.0000e+00\n",
            "Epoch 1097/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3748.7649 - acc: 0.0000e+00 - val_loss: 3963.0972 - val_acc: 0.0000e+00\n",
            "Epoch 1098/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3748.6467 - acc: 0.0000e+00 - val_loss: 3963.9786 - val_acc: 0.0000e+00\n",
            "Epoch 1099/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3748.5281 - acc: 0.0000e+00 - val_loss: 3950.9560 - val_acc: 0.0000e+00\n",
            "Epoch 1100/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3748.5208 - acc: 0.0000e+00 - val_loss: 3931.0799 - val_acc: 0.0000e+00\n",
            "Epoch 1101/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3748.3750 - acc: 0.0000e+00 - val_loss: 3926.3233 - val_acc: 0.0000e+00\n",
            "Epoch 1102/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3748.2841 - acc: 0.0000e+00 - val_loss: 3952.0863 - val_acc: 0.0000e+00\n",
            "Epoch 1103/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 3748.2249 - acc: 0.0000e+00 - val_loss: 3959.8180 - val_acc: 0.0000e+00\n",
            "Epoch 1104/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3748.1004 - acc: 0.0000e+00 - val_loss: 3944.9832 - val_acc: 0.0000e+00\n",
            "Epoch 1105/3000\n",
            "348/348 [==============================] - 0s 226us/sample - loss: 3747.9771 - acc: 0.0000e+00 - val_loss: 3933.0178 - val_acc: 0.0000e+00\n",
            "Epoch 1106/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 3748.0292 - acc: 0.0000e+00 - val_loss: 3924.2074 - val_acc: 0.0000e+00\n",
            "Epoch 1107/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3747.8029 - acc: 0.0000e+00 - val_loss: 3955.5803 - val_acc: 0.0000e+00\n",
            "Epoch 1108/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3747.8805 - acc: 0.0000e+00 - val_loss: 3958.4033 - val_acc: 0.0000e+00\n",
            "Epoch 1109/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3747.6253 - acc: 0.0000e+00 - val_loss: 3952.9651 - val_acc: 0.0000e+00\n",
            "Epoch 1110/3000\n",
            "348/348 [==============================] - 0s 224us/sample - loss: 3747.7269 - acc: 0.0000e+00 - val_loss: 3932.8960 - val_acc: 0.0000e+00\n",
            "Epoch 1111/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 3747.4897 - acc: 0.0000e+00 - val_loss: 3924.2214 - val_acc: 0.0000e+00\n",
            "Epoch 1112/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3747.5182 - acc: 0.0000e+00 - val_loss: 3918.3378 - val_acc: 0.0000e+00\n",
            "Epoch 1113/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3747.2820 - acc: 0.0000e+00 - val_loss: 3945.0357 - val_acc: 0.0000e+00\n",
            "Epoch 1114/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3747.4050 - acc: 0.0000e+00 - val_loss: 3948.8337 - val_acc: 0.0000e+00\n",
            "Epoch 1115/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3747.2211 - acc: 0.0000e+00 - val_loss: 3937.8730 - val_acc: 0.0000e+00\n",
            "Epoch 1116/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3747.2313 - acc: 0.0000e+00 - val_loss: 3928.7244 - val_acc: 0.0000e+00\n",
            "Epoch 1117/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3747.1213 - acc: 0.0000e+00 - val_loss: 3924.6627 - val_acc: 0.0000e+00\n",
            "Epoch 1118/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3746.8294 - acc: 0.0000e+00 - val_loss: 3921.1196 - val_acc: 0.0000e+00\n",
            "Epoch 1119/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3747.7153 - acc: 0.0000e+00 - val_loss: 3932.4912 - val_acc: 0.0000e+00\n",
            "Epoch 1120/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3746.5968 - acc: 0.0000e+00 - val_loss: 3951.8978 - val_acc: 0.0000e+00\n",
            "Epoch 1121/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3746.3235 - acc: 0.0000e+00 - val_loss: 3947.1431 - val_acc: 0.0000e+00\n",
            "Epoch 1122/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3746.8493 - acc: 0.0000e+00 - val_loss: 3924.7793 - val_acc: 0.0000e+00\n",
            "Epoch 1123/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3746.4738 - acc: 0.0000e+00 - val_loss: 3921.0895 - val_acc: 0.0000e+00\n",
            "Epoch 1124/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3746.5054 - acc: 0.0000e+00 - val_loss: 3918.5978 - val_acc: 0.0000e+00\n",
            "Epoch 1125/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3746.3671 - acc: 0.0000e+00 - val_loss: 3947.9608 - val_acc: 0.0000e+00\n",
            "Epoch 1126/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3746.4447 - acc: 0.0000e+00 - val_loss: 3944.8480 - val_acc: 0.0000e+00\n",
            "Epoch 1127/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3746.1577 - acc: 0.0000e+00 - val_loss: 3938.0903 - val_acc: 0.0000e+00\n",
            "Epoch 1128/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3746.8136 - acc: 0.0000e+00 - val_loss: 3922.7067 - val_acc: 0.0000e+00\n",
            "Epoch 1129/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3745.8279 - acc: 0.0000e+00 - val_loss: 3922.1156 - val_acc: 0.0000e+00\n",
            "Epoch 1130/3000\n",
            "348/348 [==============================] - 0s 250us/sample - loss: 3745.5360 - acc: 0.0000e+00 - val_loss: 3929.6537 - val_acc: 0.0000e+00\n",
            "Epoch 1131/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 3746.4433 - acc: 0.0000e+00 - val_loss: 3963.9185 - val_acc: 0.0000e+00\n",
            "Epoch 1132/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3747.1406 - acc: 0.0000e+00 - val_loss: 3955.2532 - val_acc: 0.0000e+00\n",
            "Epoch 1133/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3746.8154 - acc: 0.0000e+00 - val_loss: 3938.2727 - val_acc: 0.0000e+00\n",
            "Epoch 1134/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3746.7300 - acc: 0.0000e+00 - val_loss: 3923.2329 - val_acc: 0.0000e+00\n",
            "Epoch 1135/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3745.6128 - acc: 0.0000e+00 - val_loss: 3920.2015 - val_acc: 0.0000e+00\n",
            "Epoch 1136/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3745.7177 - acc: 0.0000e+00 - val_loss: 3913.8054 - val_acc: 0.0000e+00\n",
            "Epoch 1137/3000\n",
            "348/348 [==============================] - 0s 191us/sample - loss: 3745.0228 - acc: 0.0000e+00 - val_loss: 3944.3219 - val_acc: 0.0000e+00\n",
            "Epoch 1138/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3745.8466 - acc: 0.0000e+00 - val_loss: 3942.1055 - val_acc: 0.0000e+00\n",
            "Epoch 1139/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3744.8470 - acc: 0.0000e+00 - val_loss: 3940.3559 - val_acc: 0.0000e+00\n",
            "Epoch 1140/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3744.7639 - acc: 0.0000e+00 - val_loss: 3945.2669 - val_acc: 0.0000e+00\n",
            "Epoch 1141/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3745.4070 - acc: 0.0000e+00 - val_loss: 3932.5374 - val_acc: 0.0000e+00\n",
            "Epoch 1142/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3745.9737 - acc: 0.0000e+00 - val_loss: 3921.4637 - val_acc: 0.0000e+00\n",
            "Epoch 1143/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3745.8367 - acc: 0.0000e+00 - val_loss: 3915.6626 - val_acc: 0.0000e+00\n",
            "Epoch 1144/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3745.5916 - acc: 0.0000e+00 - val_loss: 3938.9199 - val_acc: 0.0000e+00\n",
            "Epoch 1145/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3745.4584 - acc: 0.0000e+00 - val_loss: 3938.1859 - val_acc: 0.0000e+00\n",
            "Epoch 1146/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3745.3480 - acc: 0.0000e+00 - val_loss: 3931.5565 - val_acc: 0.0000e+00\n",
            "Epoch 1147/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3745.3219 - acc: 0.0000e+00 - val_loss: 3925.4978 - val_acc: 0.0000e+00\n",
            "Epoch 1148/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3745.3136 - acc: 0.0000e+00 - val_loss: 3918.4248 - val_acc: 0.0000e+00\n",
            "Epoch 1149/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3745.2041 - acc: 0.0000e+00 - val_loss: 3944.9090 - val_acc: 0.0000e+00\n",
            "Epoch 1150/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3745.1255 - acc: 0.0000e+00 - val_loss: 3948.6414 - val_acc: 0.0000e+00\n",
            "Epoch 1151/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3745.0180 - acc: 0.0000e+00 - val_loss: 3943.9562 - val_acc: 0.0000e+00\n",
            "Epoch 1152/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3744.9500 - acc: 0.0000e+00 - val_loss: 3930.7373 - val_acc: 0.0000e+00\n",
            "Epoch 1153/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3744.7203 - acc: 0.0000e+00 - val_loss: 3919.9461 - val_acc: 0.0000e+00\n",
            "Epoch 1154/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3744.7232 - acc: 0.0000e+00 - val_loss: 3916.6325 - val_acc: 0.0000e+00\n",
            "Epoch 1155/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3744.7032 - acc: 0.0000e+00 - val_loss: 3947.9547 - val_acc: 0.0000e+00\n",
            "Epoch 1156/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3744.4683 - acc: 0.0000e+00 - val_loss: 3950.9320 - val_acc: 0.0000e+00\n",
            "Epoch 1157/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3744.5115 - acc: 0.0000e+00 - val_loss: 3941.4495 - val_acc: 0.0000e+00\n",
            "Epoch 1158/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3744.4835 - acc: 0.0000e+00 - val_loss: 3939.0772 - val_acc: 0.0000e+00\n",
            "Epoch 1159/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3744.8028 - acc: 0.0000e+00 - val_loss: 3932.9694 - val_acc: 0.0000e+00\n",
            "Epoch 1160/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3744.8794 - acc: 0.0000e+00 - val_loss: 3923.7562 - val_acc: 0.0000e+00\n",
            "Epoch 1161/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3744.6826 - acc: 0.0000e+00 - val_loss: 3956.8315 - val_acc: 0.0000e+00\n",
            "Epoch 1162/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3744.5389 - acc: 0.0000e+00 - val_loss: 3963.3223 - val_acc: 0.0000e+00\n",
            "Epoch 1163/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3744.4132 - acc: 0.0000e+00 - val_loss: 3959.9839 - val_acc: 0.0000e+00\n",
            "Epoch 1164/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3744.2787 - acc: 0.0000e+00 - val_loss: 3946.1255 - val_acc: 0.0000e+00\n",
            "Epoch 1165/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3744.3124 - acc: 0.0000e+00 - val_loss: 3930.1394 - val_acc: 0.0000e+00\n",
            "Epoch 1166/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3744.3850 - acc: 0.0000e+00 - val_loss: 3926.6494 - val_acc: 0.0000e+00\n",
            "Epoch 1167/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3744.3276 - acc: 0.0000e+00 - val_loss: 3936.5016 - val_acc: 0.0000e+00\n",
            "Epoch 1168/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3748.4041 - acc: 0.0000e+00 - val_loss: 3923.9917 - val_acc: 0.0000e+00\n",
            "Epoch 1169/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3743.0875 - acc: 0.0000e+00 - val_loss: 3936.6457 - val_acc: 0.0000e+00\n",
            "Epoch 1170/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3741.9181 - acc: 0.0000e+00 - val_loss: 3960.7936 - val_acc: 0.0000e+00\n",
            "Epoch 1171/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3743.3275 - acc: 0.0000e+00 - val_loss: 3954.0741 - val_acc: 0.0000e+00\n",
            "Epoch 1172/3000\n",
            "348/348 [==============================] - 0s 238us/sample - loss: 3744.6795 - acc: 0.0000e+00 - val_loss: 3926.8969 - val_acc: 0.0000e+00\n",
            "Epoch 1173/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3743.3734 - acc: 0.0000e+00 - val_loss: 3928.8105 - val_acc: 0.0000e+00\n",
            "Epoch 1174/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3744.7412 - acc: 0.0000e+00 - val_loss: 3927.3007 - val_acc: 0.0000e+00\n",
            "Epoch 1175/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3744.6635 - acc: 0.0000e+00 - val_loss: 3962.6928 - val_acc: 0.0000e+00\n",
            "Epoch 1176/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3744.0307 - acc: 0.0000e+00 - val_loss: 3983.4796 - val_acc: 0.0000e+00\n",
            "Epoch 1177/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3744.2775 - acc: 0.0000e+00 - val_loss: 3987.5218 - val_acc: 0.0000e+00\n",
            "Epoch 1178/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3745.8958 - acc: 0.0000e+00 - val_loss: 3948.1489 - val_acc: 0.0000e+00\n",
            "Epoch 1179/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3742.1189 - acc: 0.0000e+00 - val_loss: 3964.0552 - val_acc: 0.0000e+00\n",
            "Epoch 1180/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3746.5369 - acc: 0.0000e+00 - val_loss: 3918.8855 - val_acc: 0.0000e+00\n",
            "Epoch 1181/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3741.8552 - acc: 0.0000e+00 - val_loss: 3936.3201 - val_acc: 0.0000e+00\n",
            "Epoch 1182/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3745.6248 - acc: 0.0000e+00 - val_loss: 3915.5606 - val_acc: 0.0000e+00\n",
            "Epoch 1183/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3741.6289 - acc: 0.0000e+00 - val_loss: 3970.2927 - val_acc: 0.0000e+00\n",
            "Epoch 1184/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3745.5963 - acc: 0.0000e+00 - val_loss: 3939.5616 - val_acc: 0.0000e+00\n",
            "Epoch 1185/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3740.9631 - acc: 0.0000e+00 - val_loss: 3970.8176 - val_acc: 0.0000e+00\n",
            "Epoch 1186/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3745.1429 - acc: 0.0000e+00 - val_loss: 3935.1326 - val_acc: 0.0000e+00\n",
            "Epoch 1187/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3740.7645 - acc: 0.0000e+00 - val_loss: 3943.1282 - val_acc: 0.0000e+00\n",
            "Epoch 1188/3000\n",
            "348/348 [==============================] - 0s 193us/sample - loss: 3744.2989 - acc: 0.0000e+00 - val_loss: 3912.6415 - val_acc: 0.0000e+00\n",
            "Epoch 1189/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3741.2723 - acc: 0.0000e+00 - val_loss: 3916.9636 - val_acc: 0.0000e+00\n",
            "Epoch 1190/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3741.4097 - acc: 0.0000e+00 - val_loss: 3920.8967 - val_acc: 0.0000e+00\n",
            "Epoch 1191/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3742.1571 - acc: 0.0000e+00 - val_loss: 3952.4448 - val_acc: 0.0000e+00\n",
            "Epoch 1192/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3740.7721 - acc: 0.0000e+00 - val_loss: 3967.1975 - val_acc: 0.0000e+00\n",
            "Epoch 1193/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3741.6581 - acc: 0.0000e+00 - val_loss: 3954.7633 - val_acc: 0.0000e+00\n",
            "Epoch 1194/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3740.6132 - acc: 0.0000e+00 - val_loss: 3956.3667 - val_acc: 0.0000e+00\n",
            "Epoch 1195/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3741.0138 - acc: 0.0000e+00 - val_loss: 3943.9390 - val_acc: 0.0000e+00\n",
            "Epoch 1196/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3740.3403 - acc: 0.0000e+00 - val_loss: 3940.0791 - val_acc: 0.0000e+00\n",
            "Epoch 1197/3000\n",
            "348/348 [==============================] - 0s 221us/sample - loss: 3740.4694 - acc: 0.0000e+00 - val_loss: 3931.3971 - val_acc: 0.0000e+00\n",
            "Epoch 1198/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3740.2105 - acc: 0.0000e+00 - val_loss: 3926.7012 - val_acc: 0.0000e+00\n",
            "Epoch 1199/3000\n",
            "348/348 [==============================] - 0s 216us/sample - loss: 3739.8974 - acc: 0.0000e+00 - val_loss: 3922.4435 - val_acc: 0.0000e+00\n",
            "Epoch 1200/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3739.8420 - acc: 0.0000e+00 - val_loss: 3919.9263 - val_acc: 0.0000e+00\n",
            "Epoch 1201/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3739.6220 - acc: 0.0000e+00 - val_loss: 3921.9619 - val_acc: 0.0000e+00\n",
            "Epoch 1202/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3739.6038 - acc: 0.0000e+00 - val_loss: 3918.4440 - val_acc: 0.0000e+00\n",
            "Epoch 1203/3000\n",
            "348/348 [==============================] - 0s 250us/sample - loss: 3739.0304 - acc: 0.0000e+00 - val_loss: 3918.3834 - val_acc: 0.0000e+00\n",
            "Epoch 1204/3000\n",
            "348/348 [==============================] - 0s 237us/sample - loss: 3739.3556 - acc: 0.0000e+00 - val_loss: 3914.2440 - val_acc: 0.0000e+00\n",
            "Epoch 1205/3000\n",
            "348/348 [==============================] - 0s 256us/sample - loss: 3738.4533 - acc: 0.0000e+00 - val_loss: 3916.2457 - val_acc: 0.0000e+00\n",
            "Epoch 1206/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3738.8721 - acc: 0.0000e+00 - val_loss: 3915.9328 - val_acc: 0.0000e+00\n",
            "Epoch 1207/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3738.0874 - acc: 0.0000e+00 - val_loss: 3917.9364 - val_acc: 0.0000e+00\n",
            "Epoch 1208/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3738.4497 - acc: 0.0000e+00 - val_loss: 3912.2160 - val_acc: 0.0000e+00\n",
            "Epoch 1209/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3737.7510 - acc: 0.0000e+00 - val_loss: 3952.7944 - val_acc: 0.0000e+00\n",
            "Epoch 1210/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3738.4497 - acc: 0.0000e+00 - val_loss: 3965.4634 - val_acc: 0.0000e+00\n",
            "Epoch 1211/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3737.1691 - acc: 0.0000e+00 - val_loss: 3979.9278 - val_acc: 0.0000e+00\n",
            "Epoch 1212/3000\n",
            "348/348 [==============================] - 0s 298us/sample - loss: 3737.7142 - acc: 0.0000e+00 - val_loss: 3965.4936 - val_acc: 0.0000e+00\n",
            "Epoch 1213/3000\n",
            "348/348 [==============================] - 0s 230us/sample - loss: 3737.1559 - acc: 0.0000e+00 - val_loss: 3951.4377 - val_acc: 0.0000e+00\n",
            "Epoch 1214/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3737.2621 - acc: 0.0000e+00 - val_loss: 3939.3377 - val_acc: 0.0000e+00\n",
            "Epoch 1215/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3736.5695 - acc: 0.0000e+00 - val_loss: 3943.4765 - val_acc: 0.0000e+00\n",
            "Epoch 1216/3000\n",
            "348/348 [==============================] - 0s 220us/sample - loss: 3737.0201 - acc: 0.0000e+00 - val_loss: 3939.1078 - val_acc: 0.0000e+00\n",
            "Epoch 1217/3000\n",
            "348/348 [==============================] - 0s 258us/sample - loss: 3736.2312 - acc: 0.0000e+00 - val_loss: 3936.4213 - val_acc: 0.0000e+00\n",
            "Epoch 1218/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3736.5329 - acc: 0.0000e+00 - val_loss: 3928.7113 - val_acc: 0.0000e+00\n",
            "Epoch 1219/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3736.2223 - acc: 0.0000e+00 - val_loss: 3932.1610 - val_acc: 0.0000e+00\n",
            "Epoch 1220/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3736.6825 - acc: 0.0000e+00 - val_loss: 3914.1608 - val_acc: 0.0000e+00\n",
            "Epoch 1221/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 3735.2592 - acc: 0.0000e+00 - val_loss: 3921.5001 - val_acc: 0.0000e+00\n",
            "Epoch 1222/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3735.7676 - acc: 0.0000e+00 - val_loss: 3923.3991 - val_acc: 0.0000e+00\n",
            "Epoch 1223/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3736.0570 - acc: 0.0000e+00 - val_loss: 3910.2178 - val_acc: 0.0000e+00\n",
            "Epoch 1224/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3734.7354 - acc: 0.0000e+00 - val_loss: 3915.9792 - val_acc: 0.0000e+00\n",
            "Epoch 1225/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3734.8520 - acc: 0.0000e+00 - val_loss: 3923.1359 - val_acc: 0.0000e+00\n",
            "Epoch 1226/3000\n",
            "348/348 [==============================] - 0s 222us/sample - loss: 3735.0780 - acc: 0.0000e+00 - val_loss: 3913.7580 - val_acc: 0.0000e+00\n",
            "Epoch 1227/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3734.1632 - acc: 0.0000e+00 - val_loss: 3913.1712 - val_acc: 0.0000e+00\n",
            "Epoch 1228/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3734.2652 - acc: 0.0000e+00 - val_loss: 3909.4766 - val_acc: 0.0000e+00\n",
            "Epoch 1229/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3734.2186 - acc: 0.0000e+00 - val_loss: 3920.4064 - val_acc: 0.0000e+00\n",
            "Epoch 1230/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3735.0046 - acc: 0.0000e+00 - val_loss: 3916.2772 - val_acc: 0.0000e+00\n",
            "Epoch 1231/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3734.0653 - acc: 0.0000e+00 - val_loss: 3928.0284 - val_acc: 0.0000e+00\n",
            "Epoch 1232/3000\n",
            "348/348 [==============================] - 0s 244us/sample - loss: 3734.5396 - acc: 0.0000e+00 - val_loss: 3913.5958 - val_acc: 0.0000e+00\n",
            "Epoch 1233/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3733.7749 - acc: 0.0000e+00 - val_loss: 3914.4500 - val_acc: 0.0000e+00\n",
            "Epoch 1234/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3734.2415 - acc: 0.0000e+00 - val_loss: 3910.1087 - val_acc: 0.0000e+00\n",
            "Epoch 1235/3000\n",
            "348/348 [==============================] - 0s 178us/sample - loss: 3733.3654 - acc: 0.0000e+00 - val_loss: 3921.3051 - val_acc: 0.0000e+00\n",
            "Epoch 1236/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3734.0967 - acc: 0.0000e+00 - val_loss: 3916.4447 - val_acc: 0.0000e+00\n",
            "Epoch 1237/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3732.9926 - acc: 0.0000e+00 - val_loss: 3923.7031 - val_acc: 0.0000e+00\n",
            "Epoch 1238/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3733.6766 - acc: 0.0000e+00 - val_loss: 3914.3117 - val_acc: 0.0000e+00\n",
            "Epoch 1239/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3732.8761 - acc: 0.0000e+00 - val_loss: 3918.9888 - val_acc: 0.0000e+00\n",
            "Epoch 1240/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3733.3016 - acc: 0.0000e+00 - val_loss: 3911.3020 - val_acc: 0.0000e+00\n",
            "Epoch 1241/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3732.5384 - acc: 0.0000e+00 - val_loss: 3915.3745 - val_acc: 0.0000e+00\n",
            "Epoch 1242/3000\n",
            "348/348 [==============================] - 0s 197us/sample - loss: 3732.1644 - acc: 0.0000e+00 - val_loss: 3916.5508 - val_acc: 0.0000e+00\n",
            "Epoch 1243/3000\n",
            "348/348 [==============================] - 0s 184us/sample - loss: 3732.5363 - acc: 0.0000e+00 - val_loss: 3912.6909 - val_acc: 0.0000e+00\n",
            "Epoch 1244/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3731.9443 - acc: 0.0000e+00 - val_loss: 3917.2837 - val_acc: 0.0000e+00\n",
            "Epoch 1245/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3731.9203 - acc: 0.0000e+00 - val_loss: 3921.9459 - val_acc: 0.0000e+00\n",
            "Epoch 1246/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3731.9414 - acc: 0.0000e+00 - val_loss: 3923.6723 - val_acc: 0.0000e+00\n",
            "Epoch 1247/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3731.3805 - acc: 0.0000e+00 - val_loss: 3925.3928 - val_acc: 0.0000e+00\n",
            "Epoch 1248/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3731.8124 - acc: 0.0000e+00 - val_loss: 3920.0691 - val_acc: 0.0000e+00\n",
            "Epoch 1249/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3730.9405 - acc: 0.0000e+00 - val_loss: 3920.7414 - val_acc: 0.0000e+00\n",
            "Epoch 1250/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3731.5481 - acc: 0.0000e+00 - val_loss: 3914.5467 - val_acc: 0.0000e+00\n",
            "Epoch 1251/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3730.7523 - acc: 0.0000e+00 - val_loss: 3921.4338 - val_acc: 0.0000e+00\n",
            "Epoch 1252/3000\n",
            "348/348 [==============================] - 0s 215us/sample - loss: 3731.0827 - acc: 0.0000e+00 - val_loss: 3922.1514 - val_acc: 0.0000e+00\n",
            "Epoch 1253/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3730.5256 - acc: 0.0000e+00 - val_loss: 3930.9651 - val_acc: 0.0000e+00\n",
            "Epoch 1254/3000\n",
            "348/348 [==============================] - 0s 244us/sample - loss: 3730.8435 - acc: 0.0000e+00 - val_loss: 3931.3233 - val_acc: 0.0000e+00\n",
            "Epoch 1255/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3730.1933 - acc: 0.0000e+00 - val_loss: 3923.9551 - val_acc: 0.0000e+00\n",
            "Epoch 1256/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3730.3430 - acc: 0.0000e+00 - val_loss: 3915.3195 - val_acc: 0.0000e+00\n",
            "Epoch 1257/3000\n",
            "348/348 [==============================] - 0s 227us/sample - loss: 3730.0995 - acc: 0.0000e+00 - val_loss: 3917.5089 - val_acc: 0.0000e+00\n",
            "Epoch 1258/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3729.9733 - acc: 0.0000e+00 - val_loss: 3922.5420 - val_acc: 0.0000e+00\n",
            "Epoch 1259/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3729.7539 - acc: 0.0000e+00 - val_loss: 3930.8915 - val_acc: 0.0000e+00\n",
            "Epoch 1260/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3730.2096 - acc: 0.0000e+00 - val_loss: 3929.1980 - val_acc: 0.0000e+00\n",
            "Epoch 1261/3000\n",
            "348/348 [==============================] - 0s 223us/sample - loss: 3729.1980 - acc: 0.0000e+00 - val_loss: 3941.8422 - val_acc: 0.0000e+00\n",
            "Epoch 1262/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3730.1348 - acc: 0.0000e+00 - val_loss: 3926.2996 - val_acc: 0.0000e+00\n",
            "Epoch 1263/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3728.9156 - acc: 0.0000e+00 - val_loss: 3929.9428 - val_acc: 0.0000e+00\n",
            "Epoch 1264/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3729.7048 - acc: 0.0000e+00 - val_loss: 3918.5568 - val_acc: 0.0000e+00\n",
            "Epoch 1265/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3728.7796 - acc: 0.0000e+00 - val_loss: 3921.5935 - val_acc: 0.0000e+00\n",
            "Epoch 1266/3000\n",
            "348/348 [==============================] - 0s 190us/sample - loss: 3729.4783 - acc: 0.0000e+00 - val_loss: 3918.4251 - val_acc: 0.0000e+00\n",
            "Epoch 1267/3000\n",
            "348/348 [==============================] - 0s 218us/sample - loss: 3728.5073 - acc: 0.0000e+00 - val_loss: 3929.2420 - val_acc: 0.0000e+00\n",
            "Epoch 1268/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3729.2203 - acc: 0.0000e+00 - val_loss: 3926.5920 - val_acc: 0.0000e+00\n",
            "Epoch 1269/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3728.3473 - acc: 0.0000e+00 - val_loss: 3929.9042 - val_acc: 0.0000e+00\n",
            "Epoch 1270/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3728.8369 - acc: 0.0000e+00 - val_loss: 3921.7313 - val_acc: 0.0000e+00\n",
            "Epoch 1271/3000\n",
            "348/348 [==============================] - 0s 200us/sample - loss: 3728.1856 - acc: 0.0000e+00 - val_loss: 3929.1103 - val_acc: 0.0000e+00\n",
            "Epoch 1272/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3728.5078 - acc: 0.0000e+00 - val_loss: 3932.1634 - val_acc: 0.0000e+00\n",
            "Epoch 1273/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3727.9856 - acc: 0.0000e+00 - val_loss: 3943.1740 - val_acc: 0.0000e+00\n",
            "Epoch 1274/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3728.8019 - acc: 0.0000e+00 - val_loss: 3926.9492 - val_acc: 0.0000e+00\n",
            "Epoch 1275/3000\n",
            "348/348 [==============================] - 0s 231us/sample - loss: 3727.3649 - acc: 0.0000e+00 - val_loss: 3933.8157 - val_acc: 0.0000e+00\n",
            "Epoch 1276/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3728.6675 - acc: 0.0000e+00 - val_loss: 3915.2555 - val_acc: 0.0000e+00\n",
            "Epoch 1277/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3727.1253 - acc: 0.0000e+00 - val_loss: 3921.1099 - val_acc: 0.0000e+00\n",
            "Epoch 1278/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3728.2749 - acc: 0.0000e+00 - val_loss: 3914.0048 - val_acc: 0.0000e+00\n",
            "Epoch 1279/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3727.2510 - acc: 0.0000e+00 - val_loss: 3923.2950 - val_acc: 0.0000e+00\n",
            "Epoch 1280/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3727.4352 - acc: 0.0000e+00 - val_loss: 3927.5348 - val_acc: 0.0000e+00\n",
            "Epoch 1281/3000\n",
            "348/348 [==============================] - 0s 225us/sample - loss: 3727.1290 - acc: 0.0000e+00 - val_loss: 3937.3068 - val_acc: 0.0000e+00\n",
            "Epoch 1282/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3727.6237 - acc: 0.0000e+00 - val_loss: 3935.0060 - val_acc: 0.0000e+00\n",
            "Epoch 1283/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3726.7635 - acc: 0.0000e+00 - val_loss: 3936.2277 - val_acc: 0.0000e+00\n",
            "Epoch 1284/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3727.3147 - acc: 0.0000e+00 - val_loss: 3917.6842 - val_acc: 0.0000e+00\n",
            "Epoch 1285/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3726.5724 - acc: 0.0000e+00 - val_loss: 3918.4991 - val_acc: 0.0000e+00\n",
            "Epoch 1286/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3727.0488 - acc: 0.0000e+00 - val_loss: 3915.3329 - val_acc: 0.0000e+00\n",
            "Epoch 1287/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3726.3315 - acc: 0.0000e+00 - val_loss: 3925.2619 - val_acc: 0.0000e+00\n",
            "Epoch 1288/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3726.8477 - acc: 0.0000e+00 - val_loss: 3923.0509 - val_acc: 0.0000e+00\n",
            "Epoch 1289/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3726.1600 - acc: 0.0000e+00 - val_loss: 3934.0129 - val_acc: 0.0000e+00\n",
            "Epoch 1290/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 3726.7692 - acc: 0.0000e+00 - val_loss: 3919.4476 - val_acc: 0.0000e+00\n",
            "Epoch 1291/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3725.8257 - acc: 0.0000e+00 - val_loss: 3922.6296 - val_acc: 0.0000e+00\n",
            "Epoch 1292/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3726.3900 - acc: 0.0000e+00 - val_loss: 3919.2465 - val_acc: 0.0000e+00\n",
            "Epoch 1293/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3725.6496 - acc: 0.0000e+00 - val_loss: 3929.8845 - val_acc: 0.0000e+00\n",
            "Epoch 1294/3000\n",
            "348/348 [==============================] - 0s 192us/sample - loss: 3726.3932 - acc: 0.0000e+00 - val_loss: 3915.8331 - val_acc: 0.0000e+00\n",
            "Epoch 1295/3000\n",
            "348/348 [==============================] - 0s 214us/sample - loss: 3725.3469 - acc: 0.0000e+00 - val_loss: 3919.7517 - val_acc: 0.0000e+00\n",
            "Epoch 1296/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3726.0780 - acc: 0.0000e+00 - val_loss: 3914.2147 - val_acc: 0.0000e+00\n",
            "Epoch 1297/3000\n",
            "348/348 [==============================] - 0s 189us/sample - loss: 3725.1308 - acc: 0.0000e+00 - val_loss: 3925.4689 - val_acc: 0.0000e+00\n",
            "Epoch 1298/3000\n",
            "348/348 [==============================] - 0s 206us/sample - loss: 3725.8959 - acc: 0.0000e+00 - val_loss: 3912.0871 - val_acc: 0.0000e+00\n",
            "Epoch 1299/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3724.8654 - acc: 0.0000e+00 - val_loss: 3915.5861 - val_acc: 0.0000e+00\n",
            "Epoch 1300/3000\n",
            "348/348 [==============================] - 0s 202us/sample - loss: 3725.6735 - acc: 0.0000e+00 - val_loss: 3909.1928 - val_acc: 0.0000e+00\n",
            "Epoch 1301/3000\n",
            "348/348 [==============================] - 0s 195us/sample - loss: 3724.7079 - acc: 0.0000e+00 - val_loss: 3918.0390 - val_acc: 0.0000e+00\n",
            "Epoch 1302/3000\n",
            "348/348 [==============================] - 0s 186us/sample - loss: 3725.1847 - acc: 0.0000e+00 - val_loss: 3918.3871 - val_acc: 0.0000e+00\n",
            "Epoch 1303/3000\n",
            "348/348 [==============================] - 0s 212us/sample - loss: 3724.6162 - acc: 0.0000e+00 - val_loss: 3927.8627 - val_acc: 0.0000e+00\n",
            "Epoch 1304/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3725.2418 - acc: 0.0000e+00 - val_loss: 3922.2984 - val_acc: 0.0000e+00\n",
            "Epoch 1305/3000\n",
            "348/348 [==============================] - 0s 199us/sample - loss: 3724.3219 - acc: 0.0000e+00 - val_loss: 3932.6431 - val_acc: 0.0000e+00\n",
            "Epoch 1306/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3724.8904 - acc: 0.0000e+00 - val_loss: 3928.1906 - val_acc: 0.0000e+00\n",
            "Epoch 1307/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3724.0246 - acc: 0.0000e+00 - val_loss: 3921.7514 - val_acc: 0.0000e+00\n",
            "Epoch 1308/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3724.8393 - acc: 0.0000e+00 - val_loss: 3905.2314 - val_acc: 0.0000e+00\n",
            "Epoch 1309/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 3723.9668 - acc: 0.0000e+00 - val_loss: 3910.7715 - val_acc: 0.0000e+00\n",
            "Epoch 1310/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3724.5942 - acc: 0.0000e+00 - val_loss: 3908.0131 - val_acc: 0.0000e+00\n",
            "Epoch 1311/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3723.7151 - acc: 0.0000e+00 - val_loss: 3912.9782 - val_acc: 0.0000e+00\n",
            "Epoch 1312/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3724.3822 - acc: 0.0000e+00 - val_loss: 3907.3723 - val_acc: 0.0000e+00\n",
            "Epoch 1313/3000\n",
            "348/348 [==============================] - 0s 207us/sample - loss: 3723.4603 - acc: 0.0000e+00 - val_loss: 3916.4696 - val_acc: 0.0000e+00\n",
            "Epoch 1314/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3724.1395 - acc: 0.0000e+00 - val_loss: 3912.8387 - val_acc: 0.0000e+00\n",
            "Epoch 1315/3000\n",
            "348/348 [==============================] - 0s 198us/sample - loss: 3723.2068 - acc: 0.0000e+00 - val_loss: 3925.3186 - val_acc: 0.0000e+00\n",
            "Epoch 1316/3000\n",
            "348/348 [==============================] - 0s 266us/sample - loss: 3723.8910 - acc: 0.0000e+00 - val_loss: 3921.5641 - val_acc: 0.0000e+00\n",
            "Epoch 1317/3000\n",
            "348/348 [==============================] - 0s 186us/sample - loss: 3722.9785 - acc: 0.0000e+00 - val_loss: 3924.8750 - val_acc: 0.0000e+00\n",
            "Epoch 1318/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3723.6491 - acc: 0.0000e+00 - val_loss: 3909.1858 - val_acc: 0.0000e+00\n",
            "Epoch 1319/3000\n",
            "348/348 [==============================] - 0s 235us/sample - loss: 3722.8688 - acc: 0.0000e+00 - val_loss: 3912.7383 - val_acc: 0.0000e+00\n",
            "Epoch 1320/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3723.4488 - acc: 0.0000e+00 - val_loss: 3909.1629 - val_acc: 0.0000e+00\n",
            "Epoch 1321/3000\n",
            "348/348 [==============================] - 0s 217us/sample - loss: 3722.6645 - acc: 0.0000e+00 - val_loss: 3920.2502 - val_acc: 0.0000e+00\n",
            "Epoch 1322/3000\n",
            "348/348 [==============================] - 0s 232us/sample - loss: 3723.2230 - acc: 0.0000e+00 - val_loss: 3917.5443 - val_acc: 0.0000e+00\n",
            "Epoch 1323/3000\n",
            "348/348 [==============================] - 0s 258us/sample - loss: 3722.4643 - acc: 0.0000e+00 - val_loss: 3920.1555 - val_acc: 0.0000e+00\n",
            "Epoch 1324/3000\n",
            "348/348 [==============================] - 0s 263us/sample - loss: 3723.0155 - acc: 0.0000e+00 - val_loss: 3906.1815 - val_acc: 0.0000e+00\n",
            "Epoch 1325/3000\n",
            "348/348 [==============================] - 0s 286us/sample - loss: 3722.2005 - acc: 0.0000e+00 - val_loss: 3910.4547 - val_acc: 0.0000e+00\n",
            "Epoch 1326/3000\n",
            "348/348 [==============================] - 0s 246us/sample - loss: 3722.8483 - acc: 0.0000e+00 - val_loss: 3907.3097 - val_acc: 0.0000e+00\n",
            "Epoch 1327/3000\n",
            "348/348 [==============================] - 0s 219us/sample - loss: 3722.1507 - acc: 0.0000e+00 - val_loss: 3907.6950 - val_acc: 0.0000e+00\n",
            "Epoch 1328/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3722.5255 - acc: 0.0000e+00 - val_loss: 3901.5479 - val_acc: 0.0000e+00\n",
            "Epoch 1329/3000\n",
            "348/348 [==============================] - 0s 201us/sample - loss: 3721.7315 - acc: 0.0000e+00 - val_loss: 3909.6963 - val_acc: 0.0000e+00\n",
            "Epoch 1330/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3722.4752 - acc: 0.0000e+00 - val_loss: 3906.3536 - val_acc: 0.0000e+00\n",
            "Epoch 1331/3000\n",
            "348/348 [==============================] - 0s 243us/sample - loss: 3721.5584 - acc: 0.0000e+00 - val_loss: 3913.0965 - val_acc: 0.0000e+00\n",
            "Epoch 1332/3000\n",
            "348/348 [==============================] - 0s 233us/sample - loss: 3722.0663 - acc: 0.0000e+00 - val_loss: 3908.8319 - val_acc: 0.0000e+00\n",
            "Epoch 1333/3000\n",
            "348/348 [==============================] - 0s 194us/sample - loss: 3721.4992 - acc: 0.0000e+00 - val_loss: 3917.0902 - val_acc: 0.0000e+00\n",
            "Epoch 1334/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3721.9196 - acc: 0.0000e+00 - val_loss: 3915.3181 - val_acc: 0.0000e+00\n",
            "Epoch 1335/3000\n",
            "348/348 [==============================] - 0s 236us/sample - loss: 3721.1221 - acc: 0.0000e+00 - val_loss: 3929.0627 - val_acc: 0.0000e+00\n",
            "Epoch 1336/3000\n",
            "348/348 [==============================] - 0s 210us/sample - loss: 3721.8769 - acc: 0.0000e+00 - val_loss: 3917.9521 - val_acc: 0.0000e+00\n",
            "Epoch 1337/3000\n",
            "348/348 [==============================] - 0s 196us/sample - loss: 3720.8969 - acc: 0.0000e+00 - val_loss: 3916.3789 - val_acc: 0.0000e+00\n",
            "Epoch 1338/3000\n",
            "348/348 [==============================] - 0s 209us/sample - loss: 3721.6067 - acc: 0.0000e+00 - val_loss: 3904.6775 - val_acc: 0.0000e+00\n",
            "Epoch 1339/3000\n",
            "348/348 [==============================] - 0s 188us/sample - loss: 3720.7503 - acc: 0.0000e+00 - val_loss: 3905.3443 - val_acc: 0.0000e+00\n",
            "Epoch 1340/3000\n",
            "348/348 [==============================] - 0s 185us/sample - loss: 3721.3735 - acc: 0.0000e+00 - val_loss: 3898.2131 - val_acc: 0.0000e+00\n",
            "Epoch 1341/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3720.5181 - acc: 0.0000e+00 - val_loss: 3905.4958 - val_acc: 0.0000e+00\n",
            "Epoch 1342/3000\n",
            "348/348 [==============================] - 0s 208us/sample - loss: 3721.1658 - acc: 0.0000e+00 - val_loss: 3900.2389 - val_acc: 0.0000e+00\n",
            "Epoch 1343/3000\n",
            "348/348 [==============================] - 0s 213us/sample - loss: 3720.3048 - acc: 0.0000e+00 - val_loss: 3905.9541 - val_acc: 0.0000e+00\n",
            "Epoch 1344/3000\n",
            "348/348 [==============================] - 0s 228us/sample - loss: 3720.7708 - acc: 0.0000e+00 - val_loss: 3906.5833 - val_acc: 0.0000e+00\n",
            "Epoch 1345/3000\n",
            "348/348 [==============================] - 0s 204us/sample - loss: 3720.3200 - acc: 0.0000e+00 - val_loss: 3914.5215 - val_acc: 0.0000e+00\n",
            "Epoch 1346/3000\n",
            "348/348 [==============================] - 0s 229us/sample - loss: 3720.5703 - acc: 0.0000e+00 - val_loss: 3915.2509 - val_acc: 0.0000e+00\n",
            "Epoch 1347/3000\n",
            "348/348 [==============================] - 0s 203us/sample - loss: 3720.0007 - acc: 0.0000e+00 - val_loss: 3911.4143 - val_acc: 0.0000e+00\n",
            "Epoch 1348/3000\n",
            "348/348 [==============================] - 0s 205us/sample - loss: 3720.5485 - acc: 0.0000e+00 - val_loss: 3899.8501 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHRmUqWZwrIX",
        "colab_type": "code",
        "outputId": "7253caa0-af78-4d05-df80-3d052922adc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "arrx = []\n",
        "arry = []\n",
        "arryyr = []\n",
        "\n",
        "#Generate graph of the model\n",
        "for i in range(0,800):\n",
        "    arrx.append(i)\n",
        "    prediction = model.predict([i]) \n",
        "    arry.append(prediction[0][0])\n",
        "    arryyr.append(prediction[0][0] * 12)\n",
        "\n",
        "#graph plot\n",
        "plt.scatter(arrx, arry)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ffad1b0f208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvoQ0CVO_xnu",
        "colab_type": "code",
        "outputId": "d0a40b5a-d1da-4e09-ded5-2ef430a77049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "#graph model with data overlain to visually see how well the model fits\n",
        "plt.scatter(arrx, arry)\n",
        "plt.scatter(arrMonth, arrWeight)\n",
        "plt.show()\n",
        "\n",
        "#Seems to fit the data pretty well!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmi1947GghwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37HYuWJTPfB_",
        "colab_type": "code",
        "outputId": "b64c5e13-1ca7-4e9a-bf3c-ad5723b5ec18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "arrYM = []\n",
        "for i in arrYear:\n",
        "  arrYM.append(i*12)\n",
        "  \n",
        "plt.plot(arrx, arryyr)\n",
        "plt.scatter(arrYM, arrWeightyr)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkkD+Pw2a4JoOIHlfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_fUTSFm__Jd",
        "colab_type": "code",
        "outputId": "eb0a9dfd-24e3-47b4-b42f-9d7cb22b81cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "#Allow user to enter a year to predict:\n",
        "\n",
        "def ytm(x):\n",
        "    return((x-1960) * 12)\n",
        "\n",
        "print(\"Enter a year to predict production:\")\n",
        "inval = int(input())\n",
        "sum = 0  \n",
        "for i in range(ytm(inval), ytm(inval) + 12):\n",
        "  pred = model.predict([i])\n",
        "  sum += pred[0][0]\n",
        "\n",
        "print(\"Predicted Average Monthly Production Prediction (In millions of pounds): \", sum / 12)\n",
        "print(\"Predicted Annual Production (In millions of pounds): \", sum)\n",
        "\n",
        "try:\n",
        "  yp = arrWeightyr[inval - 1960]\n",
        "  print(\"The actual annual production weight: \", yp)\n",
        "  print(\"The percent error on the prediction was: \", 100*(abs(yp - sum)/yp), \"%\")\n",
        "except:\n",
        "  print(\"No Production Data was found for this Date\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a year to predict production:\n",
            "2006\n",
            "Predicted Average Monthly Production Prediction (In millions of pounds):  465.5751317342122\n",
            "Predicted Annual Production (In millions of pounds):  5586.901580810547\n",
            "The actual annual production weight:  5252.5\n",
            "The percent error on the prediction was:  6.366522242942349 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUEJ77AFAbGl",
        "colab_type": "code",
        "outputId": "11502357-b7c9-4b95-d088-2d10cdfbe3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#export trained model for future use\n",
        "\n",
        "model.save(\"turkey.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "#files.download('turkey.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTVeD0A7SsfP",
        "colab_type": "code",
        "outputId": "3926b53a-6c9b-476e-c2bc-e58c065c9edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#-------Convert to tensorflow.js model-----------#\n",
        "\n",
        "\n",
        "#Create 2 separate directories to save js models to\n",
        "!mkdir tfjs_files_graph\n",
        "!mkdir tfks_files_layer\n",
        "\n",
        "#convert to tfjs graph model\n",
        "!tensorflowjs_converter --input_format keras --output_format tfjs_graph_model turkey.h5 tfjs_files_graph/\n",
        "#convert to tfjs layer model\n",
        "!tensorflowjs_converter --input_format keras --output_format tfjs_graph_model turkey.h5 tfjs_files_layer/\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-12-12 11:53:37.663176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-12 11:53:37.667520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.668035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 11:53:37.668277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:37.669942: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 11:53:37.671518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 11:53:37.671830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 11:53:37.710037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 11:53:37.721237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 11:53:37.744522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 11:53:37.744640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.745232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.745706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 11:53:37.746021: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2019-12-12 11:53:37.750583: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000120000 Hz\n",
            "2019-12-12 11:53:37.750758: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24a9100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 11:53:37.750785: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-12 11:53:37.838692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.839347: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24a92c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 11:53:37.839380: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-12-12 11:53:37.839520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.839991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 11:53:37.840025: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:37.840037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 11:53:37.840047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 11:53:37.840108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 11:53:37.840124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 11:53:37.840135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 11:53:37.840148: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 11:53:37.840203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.840701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.841184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 11:53:37.841240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:37.842125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 11:53:37.842150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 11:53:37.842160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 11:53:37.842244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.842733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:37.843215: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-12 11:53:37.843252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13958 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-12-12 11:53:39.477752: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2019-12-12 11:53:40.031298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:40.031831: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
            "2019-12-12 11:53:40.031917: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2019-12-12 11:53:40.032523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:40.033036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 11:53:40.033107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:40.033130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 11:53:40.033150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 11:53:40.033169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 11:53:40.033190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 11:53:40.033208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 11:53:40.033227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 11:53:40.033279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:40.033782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:40.034260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 11:53:40.034302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 11:53:40.034316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 11:53:40.034328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 11:53:40.034398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:40.034886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:40.035374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13958 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-12-12 11:53:40.037340: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
            "2019-12-12 11:53:40.037369: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 53 nodes (40), 95 edges (82), time = 0.883ms.\n",
            "2019-12-12 11:53:40.037379: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.018ms.\n",
            "2019-12-12 11:53:40.076563: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
            "2019-12-12 11:53:40.076596: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   debug_stripper: debug_stripper did nothing. time = 0.005ms.\n",
            "2019-12-12 11:53:40.076610: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 52 nodes (-1), 94 edges (-1), time = 0.2ms.\n",
            "2019-12-12 11:53:40.076621: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 38 nodes (-14), 66 edges (-28), time = 1.908ms.\n",
            "2019-12-12 11:53:40.076632: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 38 nodes (0), 66 edges (0), time = 0.48ms.\n",
            "2019-12-12 11:53:40.076643: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (-18), 27 edges (-39), time = 0.252ms.\n",
            "2019-12-12 11:53:40.076654: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 20 nodes (0), 27 edges (0), time = 0.071ms.\n",
            "2019-12-12 11:53:40.076665: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 20 nodes (0), 27 edges (0), time = 0.42ms.\n",
            "2019-12-12 11:53:40.076676: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.272ms.\n",
            "2019-12-12 11:53:40.076687: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.104ms.\n",
            "2019-12-12 11:53:40.076698: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   debug_stripper: debug_stripper did nothing. time = 0.007ms.\n",
            "2019-12-12 11:53:40.076709: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 20 nodes (0), 27 edges (0), time = 0.05ms.\n",
            "2019-12-12 11:53:40.076720: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 20 nodes (0), 27 edges (0), time = 0.257ms.\n",
            "2019-12-12 11:53:40.076730: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.287ms.\n",
            "2019-12-12 11:53:40.076741: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.096ms.\n",
            "2019-12-12 11:53:40.076752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 20 nodes (0), 27 edges (0), time = 0.054ms.\n",
            "2019-12-12 11:53:40.076762: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 20 nodes (0), 27 edges (0), time = 0.394ms.\n",
            "2019-12-12 11:53:40.076773: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.244ms.\n",
            "2019-12-12 11:53:40.076784: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.108ms.\n",
            "2019-12-12 11:53:40.080750: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
            "2019-12-12 11:53:40.080780: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   remapper: Graph size after: 15 nodes (-5), 22 edges (-5), time = 0.106ms.\n",
            "2019-12-12 11:53:40.080793: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 15 nodes (0), 22 edges (0), time = 0.321ms.\n",
            "2019-12-12 11:53:40.080805: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.228ms.\n",
            "2019-12-12 11:53:40.080815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.153ms.\n",
            "2019-12-12 11:53:40.080826: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   remapper: Graph size after: 15 nodes (0), 22 edges (0), time = 0.37ms.\n",
            "2019-12-12 11:53:40.080837: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 15 nodes (0), 22 edges (0), time = 0.345ms.\n",
            "2019-12-12 11:53:40.080848: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.504ms.\n",
            "2019-12-12 11:53:40.080858: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.112ms.\n",
            "Writing weight file tfjs_files_graph/model.json...\n",
            "2019-12-12 11:53:43.540219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-12-12 11:53:43.544722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.545284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 11:53:43.545540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:43.547212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 11:53:43.548706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 11:53:43.549038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 11:53:43.550727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 11:53:43.551799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 11:53:43.555314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 11:53:43.555420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.555958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.556450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 11:53:43.556688: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2019-12-12 11:53:43.561638: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000120000 Hz\n",
            "2019-12-12 11:53:43.561818: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3225100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 11:53:43.561847: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-12-12 11:53:43.651968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.652691: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32252c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-12-12 11:53:43.652727: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-12-12 11:53:43.652873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.653383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 11:53:43.653429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:43.653448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 11:53:43.653464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 11:53:43.653481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 11:53:43.653497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 11:53:43.653512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 11:53:43.653527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 11:53:43.653578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.654080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.654563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 11:53:43.654621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:43.655536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 11:53:43.655558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 11:53:43.655571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 11:53:43.655660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.656194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:43.656688: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-12-12 11:53:43.656725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13958 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-12-12 11:53:45.291978: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2019-12-12 11:53:45.857469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:45.858012: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
            "2019-12-12 11:53:45.858114: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2019-12-12 11:53:45.858657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:45.859198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-12-12 11:53:45.859250: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-12-12 11:53:45.859267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-12-12 11:53:45.859279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-12-12 11:53:45.859293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-12-12 11:53:45.859304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-12-12 11:53:45.859315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-12-12 11:53:45.859327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-12-12 11:53:45.859372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:45.859855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:45.860341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-12-12 11:53:45.860375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-12-12 11:53:45.860385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-12-12 11:53:45.860393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-12-12 11:53:45.860458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:45.860939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-12-12 11:53:45.861421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13958 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-12-12 11:53:45.864638: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
            "2019-12-12 11:53:45.864668: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: Graph size after: 53 nodes (40), 95 edges (82), time = 2.134ms.\n",
            "2019-12-12 11:53:45.864686: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.021ms.\n",
            "2019-12-12 11:53:45.909848: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
            "2019-12-12 11:53:45.909894: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   debug_stripper: debug_stripper did nothing. time = 0.007ms.\n",
            "2019-12-12 11:53:45.909909: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 52 nodes (-1), 94 edges (-1), time = 0.209ms.\n",
            "2019-12-12 11:53:45.909927: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 38 nodes (-14), 66 edges (-28), time = 2.071ms.\n",
            "2019-12-12 11:53:45.909943: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 38 nodes (0), 66 edges (0), time = 0.755ms.\n",
            "2019-12-12 11:53:45.909960: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (-18), 27 edges (-39), time = 0.332ms.\n",
            "2019-12-12 11:53:45.909977: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 20 nodes (0), 27 edges (0), time = 0.118ms.\n",
            "2019-12-12 11:53:45.909994: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 20 nodes (0), 27 edges (0), time = 0.577ms.\n",
            "2019-12-12 11:53:45.910010: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.384ms.\n",
            "2019-12-12 11:53:45.910027: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.116ms.\n",
            "2019-12-12 11:53:45.910044: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   debug_stripper: debug_stripper did nothing. time = 0.024ms.\n",
            "2019-12-12 11:53:45.910078: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 20 nodes (0), 27 edges (0), time = 0.067ms.\n",
            "2019-12-12 11:53:45.910096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 20 nodes (0), 27 edges (0), time = 0.371ms.\n",
            "2019-12-12 11:53:45.910112: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.383ms.\n",
            "2019-12-12 11:53:45.910129: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.118ms.\n",
            "2019-12-12 11:53:45.910146: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   model_pruner: Graph size after: 20 nodes (0), 27 edges (0), time = 0.068ms.\n",
            "2019-12-12 11:53:45.910168: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 20 nodes (0), 27 edges (0), time = 0.367ms.\n",
            "2019-12-12 11:53:45.910185: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.373ms.\n",
            "2019-12-12 11:53:45.910202: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 20 nodes (0), 27 edges (0), time = 0.14ms.\n",
            "2019-12-12 11:53:45.914729: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
            "2019-12-12 11:53:45.914764: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   remapper: Graph size after: 15 nodes (-5), 22 edges (-5), time = 0.193ms.\n",
            "2019-12-12 11:53:45.914782: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 15 nodes (0), 22 edges (0), time = 0.568ms.\n",
            "2019-12-12 11:53:45.914803: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.374ms.\n",
            "2019-12-12 11:53:45.914820: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.107ms.\n",
            "2019-12-12 11:53:45.914836: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   remapper: Graph size after: 15 nodes (0), 22 edges (0), time = 0.064ms.\n",
            "2019-12-12 11:53:45.914852: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 15 nodes (0), 22 edges (0), time = 0.291ms.\n",
            "2019-12-12 11:53:45.914869: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   arithmetic_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.392ms.\n",
            "2019-12-12 11:53:45.914885: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   dependency_optimizer: Graph size after: 15 nodes (0), 22 edges (0), time = 0.113ms.\n",
            "Writing weight file tfjs_files_layer/model.json...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzsDFQdKWkAH",
        "colab_type": "text"
      },
      "source": [
        "TFjs Graph models are fixed and once loaded into java script return a **tf.Frozen model**. In constrast, the tfjs layer model returns a **tf.Model** which is able to be trained after being loaded into javascript.\n",
        "\n",
        "tfjs graph models are also generally faster due to certain optimizations (notably quantization for prediction speed) which are found with the Grappler API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdqTLImwk0ei",
        "colab_type": "code",
        "outputId": "b0a1b3c2-22a5-414e-9eee-23075f4f121e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  tfjs_files_graph  tfjs_files_layer  tfks_files_layer  turkey.h5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}